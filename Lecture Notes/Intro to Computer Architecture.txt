Supervision 3
    %LOCAL_FILE%jf-UuofY17s1TjPGvIF8ZS9eJEGHmrfT6Dtk0fNmdAlw730xbsBUpcv3s28ChgFS2TM2cTESgdTonLNfsyKo82RUWXrUehVixXxz1GrdTSRgtEiPUrhlTh0PBrIs_iYQ.png
        Interrupts are caused be factors external to the programmers, and cause a context switch in which the kernel takes over and performs certain commands - this could be scheduling different operations to take place, in which case the operations stack space and registers are saved and a different operations starts work. These occur occasionally and we have no control over them, and cannot predict when they will occur.
        Environment calls are made by the programmer, and are used when the program needs to make use of an OS feature - such as allocating more memory, changing access level (generally decrease) or make use of IO. The kernel will take over, execute the relevant command and then return control back to the user space process. We can predict when these will occur and have control over them.
        Exceptions are also caused by factors instigated by the instruction stream, usually errors like division by zero. These cause the same sequence where we jump to some privileged piece of code & in this case we detect whether this can be recovered from or we simply have to core dump and abandon the program.
    2. 2008 Paper 6 Question 2 a, c
        %LOCAL_FILE%rSCemIkz1ebYPPCOr7SiSWifHvnyMGhn6yrZ9to0JpDF1YhzqazpR0aBclMH_lj0DloczVqRw5a47AgXdpLRkVsQ2IYDWR1NA60tCiDN30tXtTWFlVRF_z3ZGBMGkceV.png
            RISC stands for Reduced Instruction set Computer while CISC stands for Complex Instruction Set Computer. In practise RISC commands are fixed length, while CISC commands are variable length (1 to 15 bytes) this means RISC commands are faster to decode as we don't have to wait for all 15 bytes to arrive - additionally, when CISC commands are decoded they are broken down into many short RISC like commands that go into the pipeline. However, given the greater length of instructions, you can often write complicated instructions faster (by hand) in CISC than RISC.
            RISC tends towards making the common case fast (based on Amhdahl's law of diminishing returns of speedup) while CISC often pushes for specific domains of commands being made as fast as possible - e.g. adding a specific integer division command.
            RISC has less pre-packaged instructions than CISC, but more can be emulated using subroutines that are attached to many of the Instruction types - CISC attempts to handle a wide range of instructions out of the box while RISC prefers the architecture to be built upon for specific usages.
            CISC attempts to minimize the number of instructions per program, the logic being fetching instructions takes time - however the result is long instructions that take time to decode. RISC attempts instead to minimize the number of instructions available by default and thus the cycles per instruction (while still offering a good amount of functionality). 
        %LOCAL_FILE%WZL6Sadbb6pgxZVdsiw95Y6XfhAd9nu9lmRaLHS618gTWt-VuLb_R7g1HiKWz7yOgUknIgOPRjJLlg7Vwxiy8TUcGgV774gb4Jrk8dEUdGp5aOCrhIEphZe3_cRJUzTQ.png
            For an accumulator to run an instruction on two operands, it needs one to already be in it's accumulator and the other needs to be fetched from memory - so the operation is run on the value in the accumulator & the value fetched from memory, finally the result is placed on the accumulator. For stack operations we need both operations on the stack, and then the two are popped from the stack the operation is applied and the result is pushed back onto the stack. 
            In stack based we need to fetch the two values (if they're not already on the stack) and then apply the operation (which is a dense command, causing two pops, some arithmetic and a store) while for an accumulator we would need to fetch one value, apply the operation (which is less dense just one fetch, an operation and a store). Thus the stack is narrowly more dense.
    3. 2006 Paper 6 Question 2- part b:
        %LOCAL_FILE%ae32BTrZ-dV7nLJGOy84Vjb38Zcr5CHMvCizurI6cEumjHdLdYWRWpseLfZoW_MaM56457SlgWeI7KxpeVYNir2vq5LEvUElvflyby7X80EKYYOCUe7UrmzeTn-tZTHt.png
            One reason is that while transistor speed & density continues to increase, the speed of electrons in a wire cannot be increased through engineering effort - as CPU speed increases, the clock cycles taken for electrons to reach different cache levels continues to increase to our detriment (no useful work done in that time). 
            Another reason is the problems caused due to ever increasing parallelism, an increasing number of cores that need to share data between them requires the creation of shared caches - we then have the overhead of ensuring the values stored by different caches is coherent and we also have the increased time of accessing caches further away from us than the L1 cache. 
            We also continue to live in a world governed by the Von Neumann bottleneck where Instructions and Data flow along the same buses, and although we have separate L1 caches for Instructions and data this separation causes a slow-down when fetching from caches higher up the hierarchy. 
        %LOCAL_FILE%N2z2HfJ2xfd2fMm_8q2AY3g1spuGcmNGdYAqgGKe2nVAmUHSlooSzcnxnw8TonaSGIieAKfswunhEfudtuSIKlINozST-DoJR5sPNSWjVzu-MSWvPEqJNf2C2qgNQD2h.png
            Spatial locality: 
            We adapt our caches to take advantage of Spatial locality through architecture to improve cache hit rate, we do this by reading items around our desired value into a cache line in the event of a compulsory miss. This decrease in hit rate makes memory accesses (appear to be ) faster as we can often fetch the data we need from cache. Similarly, for temporal locality in that we make the choice to evict the Least Recently Used item from the cache - even employing a victim buffer to stop pathological misses. 
            We also employ a hierarchy of caches, where if one cache further down the hierarchy misses another is likely to hit - this decrease in misses results in faster reads. These SRAM caches are faster than DRAM, and are positioned closer to the core (L1 cache often being part of the core) resulting in FAST read speeds.
    %LOCAL_FILE%p03-uxf6e0ERGAz1XawaYCw7btJszLgaAdOl_9LioV5SIYV9M_mSybKikb1jwSu-tf5rClW2ZW2npscjSUTC6xHOmS0OB6fD3L2aYHFFG7HSDSV_X7KwBxpPJT9EcRJJ.png
        The TLB improves performance as it can decrease the number of RAM accesses made - TLB is close to the CPU and can typically be accessed faster than level 1 cache (because we need to determine the page to fetch very quickly, before destination register flushed), if we do get a TLB 'hit' then we don't need to consult the page table in main memory, instead the TLB serves up the address of the PTE directly which can then be fetched. 
    %LOCAL_FILE%DWS_DbhGbc1gfIswk9CSqZ45WLZrf9UPdQHZRIsWfvwQNEa4LUS-7LN3ZTONqD4l32WulBKQ6vEeh61WDP8bjVaW9FwQW7_R6km3bdSFypp-552Dfv-HHSbZvoiZpiAf.png
        When a program attempts to access memory outside of it's virtual address space, this will be detected as the PTE will not be in the application specific page table (or on disk) and a page fault will be thrown. Thus, an application cannot access memory managed by different applications.
        Additionally, virtual memory provides permissions on who can read, write & execute a file - whether or not a process is allowed to access a file is checked dynamically and if the process does not have the required permissions it is denied access. This is possible because the actual address is hidden from the process, and hardware can decide whether or not to supply the real address.
    %LOCAL_FILE%TsQ1TzHmnIZh5ASUJrgWmNdalT2Zw2lQ1gChLh3aBI1DCJ2z37OJ3-onlMT0sr1ZQdr9V4wufI_vCt25EgM4-6292xa0NpHZpbPp83Gp_7CcxJF5Um7CO1zm-6IVqKap.png  
        Processors, memories and interconnects.
    %LOCAL_FILE%F_LCdAJw_NvkBinFS1Gf3VgNHiTkHy8SR9hyIqFeWs7ZDgDljRLP2Yr9aHash8P-S1J4EgtjKeVXKMQdI-h1IghV1NsyUmNzs08g_0n9Cydxxupmz8HK-7obw5I7Mz0G.png
        If all run in parallel, the bottleneck is the slowest member - and as D is now doing 60% of the work in the time it takes to do 20% of the work, the speedup is 3x.
        %LOCAL_FILE%7iZx4ZweGdvOvzu5z_l70StyotqciorBogTQ6HIlyEXejW8oGySnXQQfZCUZhw2tU6rTH-01veBC5M7vS31eFkBzdRBLQ3ICvBuolW6SbLwQDIkIruoq87qCMzXkPvy5.png
            \frac{1}{0.15 + 0.2 * 0.5 + 0.05 + 0.6 * 0.\overline{3}} = 2 times speedup 
        %LOCAL_FILE%cY6QT7bsC9lo6AwrvC6bshytYsneI0Hw01nqLGFF74XMVHNoFR1aiMWmRvU9zDkm96sQ1QKHnzZ7l_0qY-sGa6ZgT3K9iWmc_KtliJetZvTqQNpWLw4OKchJdbPsoa7m.png
        Sequentially: If B & D are instant we have \frac{1}{0.15+0.05} = 5 times speedup
        Parallel: If B&D are instant, the bottleneck becomes A so 1/0.15 = 6.\overline{666} times speedup
    %LOCAL_FILE%KwpkuAQjVok9cAmJfeDWPSi6oD4n6EUgpULx4L_MeBp9qelxjRMHG8cqi8mnKxJhZpR7xXflxIOMJxLE9p6JdL5UdFgHo2D2rxvpU7_rh3ekNroZr1WgKZ1jJ9D46sr7.png
        %LOCAL_FILE%GROvBCPnzUMS58cVrT3Cj0AK5Lml2PHMrLGzq7qHqdvCFxTqpSpzlcU9qywrd8F_wkJMNTLhCzEn1T8NNIcKIouGy01kJyUJoGh3GEL2su4bnhr8puZW-1W_Dl_AQNr9.png Drawn by hand.
        The DRAM cell is a method of storing data in a mediumly fast, volatile and cheap manner. There are two methods of operation
            Reading From Dram - In which the Word line is set to high, and the Bit line is set to some 'floating' value between low & high (say 1.5v) then the capacitor either outputs to the bitline if it was holding charge (a logical one) causing a voltage of 1.5V + \delta V  to be read at the sensor below OR if the capacitor contained no charge (a logical zero) and gets charged by the voltage causing a voltage of 1.5V - \delta V to be read at the sensor.
            Writing to DRAM - In which the word line is set to high for 1 and low for zero depending on what we wish to input, if zero then the capacitor will expunge it's value to the BL - if 1 then the capacitor will either charge up or remain the same. 
        The capacitor ambiently leaks the charge stored within it over time, thus we need to periodically (every ~64 ms) read the values held in the DRAM cells and write those same values back to them - recharging the capacitors who held 1s.
    %LOCAL_FILE%fq2nVqfEASe2rnCEpIIB2dL4wn9JaIZo1qx0FAHCetvWKlRNXoDp8gYk2ux0o77RDN4Ro3VigSt_yze77Jh-2UXANVxx-u99AV8ctH75RxR8mAR8fAQ7hfMj9-E7VnsX.png
        Because a bank that's just been accessed may be in the middle of refreshing the values stored in the arrays (since reading from the arrays is a destructive process), we can mitigate the time taken waiting for the refresh to complete by accessing a different bank first.
        %LOCAL_FILE%Mmrjr3lJSwf2HGb3SEYjiUKgGoC_g_F0kimsPV6k_2xFRkZ0A6igeyMEd_tqeBSKrmR3DI-gBBzMiFEN-YNgHIqIGHyhLEn7QBHcYdOAJUdv3cPHOrQxvhGYKxH_Tc3Y.png
            If we're in a Open-Page scheme, wherein the sense amplifiers are not reset by automatically after reading a bank, we can simply read the data stored in the amplifiers without having to fetch the data again - this saves time, and thus is beneficial.
    %LOCAL_FILE%dS9yzQZbCPSxlszPvk9vMGJBUhD26m5AgQr5_qloo3ztSp3LGRDaNwWZmAu5rWuUwGxgHB6Wz2wVfmoQ_klivUNkYoj3yVhVS2ntTuQ56YwIIXr_d_NN5nYWoa2Ygde1.png
        The row access enables the transistor, allowing the transistor to emit or charge and the column access then allows us to read the value detected by the voltage sensor.
    %LOCAL_FILE%hFCKZctPGzZ4iaV3m296g8bI9xVITUwrowz8mkSRk3vgg9FevcflRk0umaT_oB4dNRsSCktLVAOykLSWqm8VGBeRn6WOfTASMY-cjZIW_GH3-s6e_sg6EzAJNStZnoZk.png
        Single Instruction Single Data: A simple single core, sequential processor who reads instructions one at a time to process data serially.
        Multiple Instruction Single Data: used in systems designed to cater for robustness, i.e. when we have multiple CPUs voting over results from data inputs such as rockets. This voting process makes correctness more likely.
        Single Instruction Multiple Data: Vector operations where a single operation is applied to a host of datapoints, this is efficient and low energy.
        Multiple Instruction Multiple Data: The commonly used multi-core parallel processors of today, wherein multiple cores operate different instructions on different data at once.
        A multicore process with a short-vector instruction set would fit within MIMD, this highlights the breakdown of Flynn's taxonomy for use today - as almost all use cases are multi-core.
    %LOCAL_FILE%bkqqQkjhlzBDEmrYDKbLncwWBjl4nfafP6NZ5sIS9AZ11TZe8Uff_d-RhB6y5uHGDwUSadY6o3Q8BnforMTm7i-C8f93mPgyHG0tr96jZocEoD4J4RuIboucDnA7H84A.png 
        Amdahl's laws concerns computation of fixed instruction size, evaluating the speedup as number of cores increases. Namely: \text{speedup}(n) = \frac{1}{B+\frac{1-B}{n}}  Where B is the serial proportion of the code, and n is the number of cores. It makes clear that for a large speedup, one requires a significant process of the code to be parallel.  As we increase the number of cores, the speedup becomes more significant.
        Gustafson's law concerns computation of a fixed time, wherein the volume of the problem varies. For such a problem: \text{speedup}(n)= n - (n-1)B. This law explains that even when problem volume is scaled up, the parallel part still gains increasing speedup but the relationship becomes linear rather than sigmoidal.
    %LOCAL_FILE%PCW_aIZZjTdu7vAWNNYXs1SCIspDtj-FJfICVMl0rMpOguJqmOjQAEti0fEDQYBLgNVutmTfoEN_9h16fi3c04prxuHmgEvO2WYtsVu_GYawovf9i1GV5UnOeXZPrjF6.png  
        https://remnote-user-data.s3.amazonaws.com/vvC6eIv17JNOfYPFCIjZjixAGSJU0u7t0rYbRD6Rjln0rhC2Oil3z1gmJl4Jhe0BVV6yG4UwTq14ZcXWFePWclF7RrTBlfNYZlYz7ynWsW1exuY4nJPZ2JKwbvmzQqvw.png 
        DRAM is organised in a hierarchical structure, Devices are combined into ranks who act together (their results are concatenated) where ranks are independent. In devices within ranks, the data is gathered from a bank (only one at a time), the bank gets its output from individual DRAM cells concatenated into arrays. In this case we have 2 devices where each of their banks has 12 arrays,  resulting in a possible 24 bits.
        I don't understand the below excerpt from a slide, if we have 512 columns why only 16 bits? 
        %LOCAL_FILE%Ha_YYFQAifCByvjCuocyUgozmMrufyZe2HmoWyQEwRCgg7NHMoS_ALKGvZj-DJDxn1TX0BwBVqXxTaHSps4j4sMu91wuKIfxHaKz4Qls6sDc6V3l_zkfO5d2bMIAM2NV.png 
    %LOCAL_FILE%s5cRytkd_TEPs8P-ywDIKKBH2Wgremx2bFpd2YLXnDvfbbtZqoanRSfNgVaHxKxPnr0tTxE7gLemr26vugi0EUihzKgW1ZIFwSYsSZL6tQQGsPcLNK3bPuZbtziODcBQ.png
        Open Page: Results stored in the Voltage-Sensors are not flushed by default when results from a bank is read. This benefits processes which are likely to access the same locality repeatedly, as we don't need to re-fetch the values.
        Closed Page: Stored results are flushed after reading. This benefits processes which are likely to access differing localities, as we flush the buffers before the new locality fetch is required.
    %LOCAL_FILE%QGN4oxkVDnrIU52JdgYHePNfoYhX_cMCGGZx5x4ocIjBwWGWPZczL9Hu_fx7unVcBnOZKJ2ZaVHfMWoaES-nb_R6rNO5qRRfngwCWRdkyE-hr_G3tCaw6zkzv9Y7aKXM.png
        Shared memory is a useful concept as it gives an easy to conceptualize method for threads to act in unison. It's disadvantage is in the effort that must be done to ensure the threads are reading the same values, in ensuring this uniformity of values we have to implement some serial sections - e.g. the MSI Protocol which forces cores to wait for each other in order to communicate updated results. If this necessary aspect of data synchronisation is not upheld we can have data overwritten, incorrect calculations (based on incorrect input) and general undefined behaviour.
    %LOCAL_FILE%bFXtw7UfA25j4KUr2DJJUcsjAxscfdX-35LYuYejGdzHRyY4kdhrOC5d503-rnVmjiQ4u-YYuIXP2PyuvFRetOJZyw3b5HXa-7V6b-dvPmqK7tbhGDfigV1Ik-pNNSgu.png
        %LOCAL_FILE%3JlfPD9eb-Go6IkvwXCQJQwV-gJPBgRjyHUFwSqajFROt9wz7C5lEwuI9s6PhgKxsJN2TbmSp8UmX3Q2ipWLea8CoO8j_MBJioz6Qxi5oLFuz6gVNfD3fNCw9HV1TrDA.png %LOCAL_FILE%TS1Nt5YEigxRgtW21hBSS8kqzIL47DstMDt2jtVksbpi4U7ScTIEqG8YB900s-M9LoSntSCNOiL76knagGsjSa7g6yiDvxz6CXDHKJwVDWsiSS2aE5s8uyS26E6bBvvy.png %LOCAL_FILE%OEtTI8My8XSioVO0Fju6T0ioO0N3yT_0PgAM0j28ogNebOiJNjON2n_zZxQ_TKlC_LeRDwFwuPpFVxvaaZcdUOLvq_kQ_i8hdoKStO-ZXyuie8nEygfTeaPu5w8JrpNQ.png
        Method 1 has the benefit of quick communication of values between all cores in the fast L2 cache, each core has it's own fast private cache and not a huge amount of data is repeated as only one cache holds repeated data. However, the central bus could have a lot of traffic slowing down processing and a lot of effort would have to be done to ensure consistency between caches.
        Method 2 has the benefit of reducing traffic on individual buses by spreading it across two levels, additionally every pair of cores has a fast method of communicating. It has the downside that for the two pairs to exchange values they have to use very slow memory. Additionally this method makes bus sniffing more complex as the L2 cache would have to communicate the sniffed results to the L1 caches.
        Method 3 has the benefit of expanded private use space for the cores, reducing the chance of having to resort to memory. Additionally, it is more straight forward for cores to communicate via bus sniffing than in method 2 as the private caches & the public cache are clearly separated. However, this results in more repeated values being stored needlessly (in L2 cache) and a lot of processing has to be done before it's decided a value is missing (search L1 cache, search L2 cache, send a message to search L3 at which point we have to wait for the other caches to run a check after sniffing the bus then finally we check L3 and at last we have to resort to memory).
    %LOCAL_FILE%ALKU8hSSW9mdiBjiPYuDGLsx0Yl4N1tAyPEOjyemUzf0V5Qnp09h7t0pF8FsR4kWqM3fsbMiT7BPGslH8kAKeBK4w4aQG8BduX9nUZ36TEHQYmUwr466QGue4LicVYun.png  
        Inclusive: All values stored in smaller caches, will be stored in larger caches.
        Exclusive: Values will only be stored in a single cache.
        NINE: Values in smaller caches may be stored in larger caches, though there is no guarantee. 
        By smaller caches I mean caches lower in the hierarchy & larger means the opposite.
    %LOCAL_FILE%gnGeaQmtcoNy5ZhjROBvVzAwXMfdjSG3C5kxJSC1R9dsEAKRSGo7La2Kh__bK0IqcThPgZFWL6EVgJ40BaDlG0R15A5Z3ygZm4ecv5AXzMAgqGhykj_kpllr7QCOI1-i.png
        Under the MSI cache coherence protocol, caches can be in 3 separate modes:
            \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {I}}} mode, where the cache-line is marked as invalid - i.e. we act as if we don't have it stored (but don't actually take the action of removing it, it will be replaced). If we wish to read the data we must issue a BusWrite call, read the value from memory and upgrade to \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {S}}}.  If we wish to read & write values we issue a readWriteX call, read the value from memory and upgrade to \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {W}}}. 
            \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {S}}} mode, where we store the value as a reader NOT as a writer. If we sniff a BusWriteX call, we must become invalid - If we decide to want to write to the value, we issue a BusWriteX call and update our value from memory before upgrading to \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {W}}} mode.
            \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {W}}} mode, where we are the exclusive holder of the value and can read or write to it. If we detect a BusRead call, we must write our value to memory and downgrade to \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {S}}}, because a cache in \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {W}}} mode must be the only holder of the value. If we detect a BusReadX call, we do the same but the down \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {I}}}.
        This ensures all caches hold the same values, as only one cache can hold a value that's being written to - after which any reads to that value are synchronized from memory.
    %LOCAL_FILE%zvEuBYjnX_eiFcabqGcgI7WQeE3cwisMx6wIcGbV_ZNC8yuXiSQ_QbKt9YoH9AGmPE3nAK5Zxw984u6aeLkgmviAX7qWDeEpq_fkZf5I3060QNzC3RfrUYed19J4R8Vn.png
        You first write your data to memory (Wasn't really explained how the precedence works here, how does the other cache know to wait? And for how long?) then transition to S state.
        %LOCAL_FILE%FLBiBIzm4AwpDjF5bcxLv5v2J-QpDrRQiIZ3-U4ywuFOuLlp5wtq5620H_U4i83aERdqZOCaCjqbyp3O9n6rhEtsONWsDM8eFJF06RDYT_sUUjj-IBRcLj9OkqSubU13.png
            You do nothing. One could extend MSI to send your value along the bus as that would be faster than fetching from memory.
        %LOCAL_FILE%esM97s0emvnAHg5JhST0ezKtEWoXS0lquJCV3hWl-OV8mfsa04RPXrXaPXm8stTTTnWe9l71sxFURzzUkrG8nCc1oFmH-FUIcBCmdgm5S33WwNah0wEdFDIEViBeOWb7.png
            You do nothing.
    21. 2018 Paper 5 Question 3, part b.  
        %LOCAL_FILE%7bgda-H3OLiUSJ063JyXJ8hE_s3S8ODxBRThEtdRTcAvpZBsXTcvLesI5jMCnQHKsNqaZMBGHWC-sO4GO7NUTejpenn2Fduf-I0DaX3OMVlRSIlDYf-x0G3Pi95ccwxd.png
            Cache coherence ensures values written in one cache are seen by others, while memory consistency attempts to ensure the ordering between reads & writes doesn't cause improper behaviours.
        %LOCAL_FILE%5sIceteoEqGQoCKvfF8ey7aWgLHquJ87-RSGWsI1jBVqxzH51ZjWAVVN0bXAsIYoA_Ag7Br1dFR-YusLhlkFbHuah8oaUtQEJCJiubLqCynCqx00PrnKEH8ooq3Xj1nr.png
            https://remnote-user-data.s3.amazonaws.com/_ZQbCuIDZz0b7M3HkNsf6nTPaV3HqYI82Jpr810QCnTppzPYXgokn6juAkBg-3vUXrYgXirk_6iXKppd_Mhb9RyCETtt1mvUpix-lqgAyYxW7b7yi1XLPBs6BlwQ5ojh.png 
        %LOCAL_FILE%PDJPOQqzMhDtFwmkSxYgzwPaNArVL2To06Gd2Vj1BVTOFgrKjfF4Gu5z-ubiAH-T6NP3JpuUd5xvQMyEbTBpmWlK51W_u4WPtiEYpMMvuG31e8qJCvjsgykieZJspy2l.png
            %LOCAL_FILE%8a68vLHMj-2N5qVbfqbGSlhp4lOGjEmuoQ2L7vprEQu9SHKgVq2P3EcR6RU2WcKkQ0eLJ6EFSaQ03JFc7mEAqSXjaDufcacKCBFWAH9hQBPCX0PmB-Uwqi3unoKivYn1.png 
        %LOCAL_FILE%ZjGTjLTqjWMKO282PNdtrnKWSu7g-SzttsWD3jlULbBncXDZd2j-ap8qG2JDUwTpU4FeoSyCizFVhpO1ex3ayr4ZgjgLSTh7c66SzzkqfzNG0UNBqRSgxTp89T4bS22J.png  
            Passing from the cache to other caches is faster than fetching the values from memory.
            If we're in the M state and another thread issues a read, we transition to the O state - after which if we wish to transition back to M we no longer have to process an extra memory transaction - we know we have the most up to date version.
    22. Summarize the main message from Lecture 9 in 1-3 sentences?  
        There exist lots of methods of hardware support for operating systems. Different processors have different implementations. 
    24. Summarize the main message from Lecture 10 in 1-3 sentences? 
        CHERI protects from improper memory accesses by explicitly linking bounds measurements to processors. Stack based ISAs can have very compact instructions.
    25. Summarize the main message from Lecture 11 in 1-3 sentences? 
        SoCs come in all shapes and sizes, designed for specific domains (whatever that may be). Lots of opportunities for customization within an SoC.
    26. Summarize the main message from Lecture 12 in 1-3 sentences? 
        Multicore processors provide MIMD parallelism. Using shared memory paradigm intuitive for programmers, but propagating writes to data can be hard.
    
Supervision 2
    %LOCAL_FILE%ujcL6v4TPBhwrAcC9U-DaheF_SZq6DZo4qsSSIJ_y5ZWZw2da0KoPhAc-IC7-96muSFQa3iDtOcZFGvVILKMMqKF8thbd7P8VPDM5HDH-kPnflEeHP0Iu8ue1pvtHVLD.png 
        An immediate is a value that does not need to be fetched from a register or memory, that is it represents a value directly rather than an address of that value. The only computation that needs to be computed is extracting the immediate, involving sign extending.
        %LOCAL_FILE%Bp04bTw83jaJ9i3BiMwlJkcbV2zGungrBbTZkiONosl6IIUNJIY44FN8t0FxPPXLej7ilSAiBcFsEAvBObuzn9CT5dsZGLDwxryh2ouy-blZW25weMm4z1xKCbkLAbPx.png 
        AUIPC rd -1000; // rf[rd] = PC - 1000
BEQI x5 x4 100; // if (x5==x4) PC = PC+100;
ADDI x5, x0, -5; // This stores -5 in x5.
        %LOCAL_FILE%TeoYGLDUjRa7z5q-5Sn9A1w67YUAcm-CNL2WnXCz09VzCz1Y1eAmoGrncZiSqTrcIBx5hleeoQ8KLbe3CDOo3AHBFzl_korJ4nzpcqvhA0TUp8CJZVWmEYJoi97vraU9.png 
            Because immediates take up space that can otherwise be devoted to funct3 & funct7 fields, which extend the opcode space and allow for thousands of more instructions. R-type instructions contain no immediate and thus have a funct3 & funct7 field, while I-type uses an immediate and thus only have a funct3 field - 6 bits less opcode control results in 2^6 times less possible instructions you can perform. The reason in general is that register-to-register instructions require only 5-bit register addresses, while immediates often need to be longer than 5 bits.
    %LOCAL_FILE%TYJ8_gFTsVYCGehDl-wZLT7Wk-FpYNQne44DtqUNd_0Py1Jv5aJylbRJ4IaeaoUsJeyOuexaycxFPcKAOd--yodGGGFhyijXf2dy-_R2Ffu7kqihZ0lBTMsOzn79_bnA.png 
        ISA level simulations are programs that interpret machine code and run said machine code in a higher level language, they are often "bare metal" (they don't run an OS) and also ignore factors like varying clock speed & interrupts from other programs. An example would be SPIKE which is the ISA simulator for RISC-V.
    %LOCAL_FILE%J7L7np4Dby_9cLgtwDrxVP_rCQvCdpja652qu3tqA_9_zf00zFxmlz5Y1Y91Q67pxNMadtl_1T3b4jNQSVQnK436sCLkhjdhDGUyd30sLe-CNjCeLVA7cAOgRoHtkRP5.png
        The data path is the actual flow of information (be that immediates, addresses or register values) while the control path is the signal that controls the flow of information. They tend to flow together because certain control decision can only be made once the instruction has been decoded, and the data values can be examined - for example, the values of the registers in a BEQ command control the control-flag the ALU emits. 
    
        %LOCAL_FILE%hUXO8mnviRXtruya41BN9kijy81S75f1t4yhCe2D_F-K5Xcfy00i2Whwvtj99jatVA46g22xuuDOv_P9KBet5IdEGrYBfqtoWorSMisBVdFzpaJ7X8lrnVRKxaSQIBiq.png 
            Performance can mean many things, in terms of CPU we have Clock speed, Cycles per instruction, Cache fetch speed, Number of cores, Individual core speed etc. A benchmark could be developed to measure any one of those metrics, but there is no overall metric of performance.
            Performance varies as outside conditions vary, a benchmark could be run when the CPU or other components are already hot or when the room is abnormally cold, which can lead to incorrect predictions when the system operates outside those conditions.
            Performance can change due to conditions inside the system. for a CPU we could have lots of OS interrupts during the benchmark or lots of other processes being used that hog resources - making the benchmark innacurate for normal operation.
                Performance can vary for the use-case, for example a CPU can favour high core frequency vs more parallel cores - where the higher core frequency system could do better in a non-parallel use-case the system with more cores would do better on tasks like video-decode that are suited for parallelism.
        %LOCAL_FILE%DHrxwCXTlVqaXyaqr4YJu0IbL7nGyZFwnlKjLGPDeZMf46R9iyOaHoTqqmMn5aIdtOCMtTu4BuB1UhY87LdrVnJM-IsjP_GRWLZyIzpINdk3GP7_nPBvnq869JBRisYq.png
            Instructions per second varies wildly depending on the program used, some instructions take more cycles than others - thus, a computer running software with more lengthy instructions would inherently be considered slower by this metric.
            Two computers using identical hardware but with different ISAs would result in different measurements for performance, consider CISC vs RISC - a given CISC instruction could encode multiple RISC instructions into 1, and even for the computations would thus compute less instructions. 
            Again, this measurement would vary with the conditions of the two computers - a higher temperature computer would likely throttle and result in a lower clock speed, leading to less computations being computed per second.
            Under Von-Neumann architecture, much of the time spent is data fetching & setting - and while we wait we can simply perform polling instructions. Thus, the most critical factor for performance is not captured by this metric as we can compute a load and then repeatedly poll to see if the data has arrived - this will result in a high instructions per second although nothing useful is calculated.
        %LOCAL_FILE%oJoZsvB6MsmJwvU8JTK0mCpOZwXsO74aGqk67D3Kj0BtNaS0ZZ3ub3GzMCUEzROzFokhWOllo9mU1Ht2Rdzvp0omRBRQ756XZfOpMBsc7tz53YwTOfD_naTefMAxWsxm.png
            Peak performance is only useful as an upper bound on performance, can't tell you anything about the common case.
            Common case can vary wildy from the peak performance.
            The difference between peak performance and common case performance will vary wildly from program to program.
            A computer with much higher peak performance may complete a task slower than a computer with a higher average case performance as the peak performance cannot be continuously upheld (temperature throttling, data starvation etc.). 
        %LOCAL_FILE%jazhr05nvPg_iVt0LtIBeOZTQuMCyOcdNOviCvlVf8W-KmER0h5cIj1Gy4QVqy27RU2Mf6u2qwK0glP-aOVH4hOEofmgDjSUeOpeV9vqStLxou_cxkEC8iLi0bDwCb7X.png 
            Some applications are orthogonal, e.g. best performance and lowest price, meaning you can't have both. 
            General purpose CPUs are usually slower than FPGAs computing the same algorithm, due to the allowance for greater parallelism. Thus, serving multiple purposes results in slower speeds. This similarly goes against making the common case fast.
            The design is always a trade of between performance, energy use and area - these trade offs cannot be thrown aside, as some applications want small area & energy use with a lower requirement for performance (phones) while some require high energy & performance resulting in larger areas (compute servers). So no ideal processor can be developed for all applications. 
        %LOCAL_FILE%Dpic-8nsa3ExxGcMZkvAmhuAwT4wbYgoy6GScYpcugSf2Dc3x02taF2oTvhMwuRzlhJIHprRr0jXl8fTvSyvfLNlzpcchdPBLJhtQxoF8eFn_67EheC2FgQxVtZv5h56.png 
            While longer pipelines can allow for higher clock speeds (less work per stage, allows for higher clock speed), there are associated trade offs. 
            Diminishing returns from performance increases, Amdahl's law - if an operation time is barely used, adding a pipeline stage can decrease performance as time needs to be spent waiting for that stage to complete while a fast operation may be waiting.
            Additionally, resolution of control and data hazards increases time taken (we need to fetch the correct instructions and flush state causing bubbles in the pipeline). Mis predicted paths have a greater effect on longer pipelines - Got this from notes, but don't really understand why it's the case wouldn't it be the same? 
    %LOCAL_FILE%HrcWNFR_E2HESaJNxYOc7XrcKLX7vDuEmKx5F6q0Ap4tlz7gLbVz46xUZcZFdvFK8tcDtriByoZ1znxvUBVKmXSTWuBOSG3FdtkBC2qTP4jjZxoijgI4vGt2mfKPaIf8.png 
        No, more complex pipelines can have more stages - the 5 stages is what RISC-V uses and can be modelled as an abstraction for more complex pipelines. For example, CISC pipelines can have a second decode stage where longer instructions need to be further examined but this would be put under the umbrella of DI.
    
        %LOCAL_FILE%3Uy5_FV4LoCFD69iUHoN_NLwWYbbpIANN4-7dl7wqsadCZTdSr4xD-zcEHF0qNv23_lY9jodR7L4rdJ2zgKOFGKC_LU5288COxmEU7NaLzdlxbKOIQJjtXzlziqGeyJz.png 
            From a birds eye view, data & control hazards are a necessary feature of parallelism - if we are to run sequential programs concurrently we are going to have instructions which rely on the results of a currently executing instruction. More specifically, data hazards are when an instruction needs one or more of the items that are to be "written-back" by an executing instruction as it's operand/s. This hazard is caused by operations wanting to use transformed data. Control hazards are caused when conditional branch instructions need to wait for there condition to be evaluated before we can update the program counter and fetch the next instruction. This can be mitigated by trying to predict the outcome of the branch, and rolling back if the guess was incorrect.
        %LOCAL_FILE%p7VHTCavrC_gDoAR3T0SQE-x2lZ_lO0XFya-WFeB8v6O2zvBO6s58odYJIEVdVIHLepDOIV3hatj8h8JZdGnpm9wZjd81eZD7Yeh9vuEx_fffZQ2FUQWL9_RZI45vFYL.png 
            Can we go through this question - don't know what it's asking 
        %LOCAL_FILE%fzxrPBHAxm4NkUXqKIIy3xlM1J658qmhBqCZWn75WRdHYpWMNfBPKWKdz_GVO_imF2vt0XpRlyQLUNFiCsbEkHOa2w2Cam9dfqed3WxBz8dZ6CMFjiYVFaTwaSLC4WdA.png 
            Pipeline A:
                Data hazards cannot happen in pipeline A, because once we've decoded the instruction the previous instruction has concluded and the register has been updated so when we fetch the register the results will be correct.
            Pipeline B:
                We can introduce forwarding from end of the 3rd stage of the 1st instruction to the beginning of the 3rd stage of the 2nd instruction:
                    %LOCAL_FILE%I_OZbM7nhza2JNMJh9i2jbqw_hqkjESg5n2cqMqBOB5aHntnRGYE-eWc_xiuCoxd86If-izdy5APWDfFTaUwprErMbypg7j-UDXOSqc8QYz1J78Ht_rHpij5uhmKKGSh.png 
                This forwarding would overwrite the values in the registers if they were changed, for example:  ADD x1, x5, x6;
 SUB x2, x1, x6;
                We would have to feed the updated value of x1 from the 1st instruction to the second.
            Pipeline C:
                This can be resolved using standard forwarding if this is an R-type instruction, where the result of the execution of the first instruction is fed into the operands for the second execution stage:
                %LOCAL_FILE%2X9HLoPP_olxS3qMxdKywBN3qrUQgBv4Kf1gqfUKzKNf0TriTV0CpKUE2BvwaQrnlAVIwM7SmYywV8nKXZauyItrAKNbnAdUzyMExe8igbYJHystbNYxCkDFFvGOvzwU.png 
                HOWEVER, if our first instruction is a load that the second instruction relies upon we will have to have a bubble and use forwarding from Memory to execution.
        %LOCAL_FILE%1iYR9GJRtRBen6sgG7ruEOkFMcgFH1yGk9-keOL-hwq0rO_NbQwSkqMITqdFblngsP9H1TSsdhYrqlPyp7_pYQ3nO1B8moL3fvPWS0_tSq2TYDbf0Sl4P_D2up8CNA64.png 
            Exceptions do introduce control hazards, as they result in the pipeline being flushed. For a precise exception, the exception is recoverable and this will simply result in bubbles in the pipeline according to the number of clock cycles the exception took to resolve. However, for an imprecise exception we cannot recover and must simply exit.
        %LOCAL_FILE%rj3My9YwE1oxjg_TS7tLeqXizhP0fVSeBZvGTt2ve4bmbMWPQx7skKXtO1nEQyuIwAIIMTOB6YrXpcvSAXWF2iVxhwW8_c-mpR3DUg3SjHZGkZiofmio1q8VIZBtSeVB.png 
            Interrupts do not introduce a control hazard, as the entire pipeline would be pushed onto the stack in the event of an interrupt and resumed after the interrupt was handled. We never add incorrect instructions to the pipeline while the interrupt is running.
    
        %LOCAL_FILE%E7o0ebzVnw8np-AE1bfqtq4SmA7zbpTWTm-KLVnaK_u4T80CWJvAXgZpbBOgPh4J56uQ4qBtn1GzFJSpo6w6QmNmH-NJvJhJvQJ09P40jKqdpBMgVJjx2UR11VtZah4J.png
            Branch prediction is when the processor does not wait for the arithmetic operation section of a conditional branch to be executed, instead we make a prediction about what leaf of the branch we'll follow and roll back if we are found to be incorrect. This can avoid control hazards (if it predicts correctly more often than incorrectly) as we don't begin fetching items while we wait for the branch to execute, instead we instantly make our prediction and update the PC resulting in less instructions needing to be flushed.
    
        %LOCAL_FILE%E3_cpU7FhIMFMosx0KRu0eo_OaU7N78GMgNKn6Z4mK05y9ZFcbgrbb_xNb5vFRQhmDFCxO9uEGx9vqsbmy8p0InfYnVHSLIG032U0pjFECdo8YqrRwhM5RB2uJCC_tJB.png 
            This is contents of lecture 9 which has not occurred yet.
    
        %LOCAL_FILE%XqoBtTygBlIN6uHJnGQFh8I3qcG5IFvD87VmbkDnkztk5tUxtfQOmQ40ltDXkkhsoRbKPujZEQQmJ1r7D0-xwc8AfWKfENyGLKqnF8dNeNZPx3Hzb5rcMjgZsBNdVUPU.png 
            Memory latency is the amount of time it takes for memory to arrive after being fetched, while memory bandwidth is the amount of memory that can be fetched within a certain amount of time (generally a second but sometimes 1 clock cycle).
        %LOCAL_FILE%4MbwB78Gg0rz54Ex-HTTS495wkzxAFFrId7boZPkYTW-NPvTw84Qy-53SoNYxxl69wJs1_1j2r8vQFsyMWfBPaiv3mkT0CgYqPKrqdf5aOTBKXc-hPD3KD_pC3UhqCiU.png
            Exceptions are caused by processes in the CPU, whether that be a division by zero error or a syscall, while interrupts are caused by an outside I/O controller like the OS. Interrupts require the registers to be put on the stack until the handler is completed, while exceptions can cause irrecoverable flushes of register files.
        %LOCAL_FILE%64IDOmAG3--v5FSz06UBv19TM9Ten3RMit5FpFJrCZQedy_Pgck5FP-1510pdgGPPtl5qPy-9vTc09QwuPVdvnZrrzZiJBbbQO3Ao__BU6FBh4ye4wZ_zhLPMo43xFM7.png 
            The programming model of a flat linear address space is the most intuitive and easiest to reason with model, we assume the programmer doesn't need to know the fine details and abstract them away. The CPU is then tasked with making this intuitive process fast. A hierarchy of memories is used because the speed of light is slow (and because very fast memory is expensive) , calls to fetch data from DRAM can take hundreds of cycles so we position fast caches nearby the CPU that can be polled very quickly. Only if we miss our poll do we need to step up the hierarchy and fetch data from DRAM or potentially even HDDs. This is also a result of the Turing tax, as most of the time is spent fetching and manipulating data rather than performing computation.   
        %LOCAL_FILE%CjzrJJe0a3jCVrKSznbuZBlZa6b775Anpio3P0U3NAmDXWuekow4eGuAqN7z0olkCfSAxcq0RR6HeaMfRf00bPhCXCBf0EgKikgvbJN88Rgdaj0FmQZeFT_L1NBYBygS.png 
            A direct mapped cache consists of many cache lines that contains data that was spatially-bunched on memory when it was fetched - each line is referred to by a unique hash which is usually some combination of it's index, it's tag and the word to be fetched. A set associative cache is a large collection of direct caches, where all direct caches are polled simultaneously to find data. In direct-mapped we have no choice of which cache-line is replaced, as it is built into the memory address. In Set-associative we have certain options, such as least-recently used, not last used and random. 
    
        %LOCAL_FILE%Ayul6TLMRCoxjIc4cAeVdla03F32FFZBHs2WOkxOm2JoUTWhDDDPHb8vr82sLaKnJBgFSvWweZFPG13haPBzu_i3HkOIMibSi3Dl2xp5QEaUlmmq0CIc6xsAEWPCUWJZ.png 
            In control-flow machines, values need to be accessed and consumed quickly to allow for the progression of the program - as later instructions will require the results garnered from the consumption of the aforementioned value. Thus, if the memory access latency is high the program cannot proceed - this lack of progress while waiting explains the sensitivity.
        %LOCAL_FILE%6XbZtfmhU9YoHSaiiAAH83hNSuVF-1TFe30ZdN-KZp1KnKiYdasyjpow9irpDz__J0zagsIWnV0Khhi2GcNVi-DMhcImhRCB06PwBVLuNLgsTvXFz3zIms49CxjvSPPJ.png
            Temporal and spatial properties of data access patterns are exploited. Spatial: if a certain piece of data is accessed, it is likely that nearby data is going to be accessed soon. Thus, we cache requested data and data spatially close to it (in the linear memory) on a miss. Temporal: If a piece of data has recently been accessed, it is likely it will be accessed again soon. Thus, don't just flush data after it's been delivered to the CPU - cache it for reuse.  
        %LOCAL_FILE%BubVqInmQH73h3cTr6eb56u3QrOMDUy5kFkJzxcyS4I_rHKgLZAQl5X7WiVh5fbXXb0d-BcUAOC62OoAj4WKHekVCLk6dpp5CjkD76PMcil9bcVpM3nVLQdjz19D5dBH.png  
            Direct-mapped caches have no choice as it is governed by the address of the item. However, set associative can use least recently used (hard to manage for large sets), not most recently used (easier to manage) or random (easy to implement and around same performance as LRU). They might also implement a victim buffer, to avoid "pathological misses" 
        %LOCAL_FILE%x42S6Ceps_2HqEL8DCX2MNjn47hCcBIv9BnoibcAplrSNuL_RqflsWNePHiY5N5eqrUFDV2Wm8SpJfcq6qwtGCjYEhto6W3BFUAt7d3m4rdWi9Mim0FhrzSw8TvhPmWe.png
            Write back & Write through. Write through is where you keep a write buffer, when a data-write hits update the item in the cache and add it to the write buffer. Only stalls if the write buffer is full, in which case memory needs to be synced. Write back is where we mark updated items as dirty on data-write hits, then when we attempt to remove the item and it's dirty then we must sync memory. Both of these are methods of keeping memory and caches synchronised while keeping the CPU occupied with computation rather than memory management (as much as possible).  
    %LOCAL_FILE%tSOQjmd979Wel0vDmCZMaa4dj7gHcSLmRnHZGNq-uA1Gb4DFVOomMrCrDVhBVXvV9Cxt50j0SrgvZUOg3hsJqdi3K1aOOmmrVFslMiHiQztfSseQ-kh8ZOlfQIHFQgdf.png
        Cache miss rate is the rate at which we poll the cache for a certain value, and it's not there - meaning we have to fetch the data from higher up in the memory hierarchy, if that's DRAM then hundreds of clock cycles can be spent waiting for memory to arrive rather than the 1 or 2 for the data to come from cache. If our cache miss rate is very high then we are constantly accessing from DRAM, resulting in huge slowdown of our code as we wait for memory to be shipped over to operate on.
    %LOCAL_FILE%neVpREEPM9QEQh0eE3YLO6iHU2MOTJ8UiE2T7JCPv6ylofruCHwLnLNZxJUANvvPIw5kajdAq7yl4xxj7T7OJgd_Pjuc4SmLGKzDApXwbEKWsi1JFjIsiPUVq-HFBqUE.png
        Single sections of the pipeline can take less time than others:
        %LOCAL_FILE%QgDYiw-84BUNbSgRI6NeWKTsYHXqJqKIWtTbVWCEnfVMsvlGWNfH5W5BeOhrrU5w0y9YksdAxhGCdVecxuyuDUb4iJjyRvl6EzU7QIohyaNRMn-pQ631fgHoWeTquEbn.png
        Thus, if a given instruction wasn't waiting for the proceeding instruction to finish it could complete the execute stage quickly and advance to the memory stage - however in the pipeline model, every stage takes as long as the slowest stage - resulting in: 
        %LOCAL_FILE%Mu_iWQF_bZD1YvTf0XGvMi35rlo8rUysvGO3fuqEI3NB9t-5Gn3tTOVaLJEIMSSn0JHGqE_6up5Rok71mWSeQJahF-fzbRXKp5m9o7AZUGIw25izDnqQ1xJ4OkzYnv1n.png 
        While individual stages can be slower, we can compute up to 5 stages simultaneously using the pipeline model, resulting in the overall execution being faster.
    %LOCAL_FILE%xBcTxIj0lgc1qrOs4Tz9MmVw0eVOqdOgEF_xlwQ3nmMJXJNAb2DWCYVTLvC9tinnMJxqkmtRZYWBUOLadV3iTFoi4cPNaDDMkYTXclrfMWae96U6qY0svUk7ndFljEBZ.png 
        Compulsory misses occur on the first access to the block and will always occur.
        Capacity misses occur because the cache is a finite size.
        Collision misses occur (only) in non-fully associative caches due to competition for entries in a set.  
    Discuss the main message from lecture 5 in 1-3 sentences:  
        The ISA provides the interface between hardware and software, however that interface can be misunderstood as its often explained in English prose rather than a formal language. We desire a formal language of an ISA to allow for easy ISA simulation of a processor. We can then compare the models of the processor using tandem verification.
    Discuss the main message from lecture 7 in 1-3 sentences:  
        ISA influences the design of the data & control paths. Multiple issue and dynamic scheduling can be used to reduce hazards and increase parallelism.
    Discuss the main message from lecture 8 in 1-3 sentences:  
        Because physical constraints prevent the creation of a huge low-latency memory, we must rely on a memory hierarchy. Caches (widely set-associative caches) take advantage of the temporal and spatial statistical features of memory access to store data that is likely to be used close to the processor.
    
    
    Discuss the main message from lecture 6 in 1-3 sentences:  
        Pipelining improves performance through parallelisation, each individual instruction has the same  (or higher) latency but the overall computation time is decreased. However, it is subject to hazards that must be handled with care. Instruction set design affects the complexity and feasibility of a pipeline.
Supervision 1
    
    %LOCAL_FILE%2UQdpnQ1FkPkFUL4c5t1kWXX8y3CADrCFDn91MYJHDc14YPQZRJNraIwCbbIp89K4lKp_c4rP_kIcGIvHYGgzPMxlPie8s-mfFv3OwXYC4kPYjfGX5OrK1qSldMXblyW.png 
        Moore's law states that the density of transistors on integrated circuits, doubles (about) once every two years. This continues to apply for the time being, however we are approaching the limit in size of transistors - given that the smallest transistor we could produce lithographically would consist of a single atom in size, it stands to reason that Moore's law cannot continue forever (although we cannot predict future discoveries). However, for the time being the doubling of transistor density every two years is still being upheld. 
        %LOCAL_FILE%xYXHCdBxVEZ6SpLlP-ayz2XFjwCy7rNKi_33h8Ft17-nd85USY418aMCx8u1LRpktHyZtvLU8yMgvokFrZkmRMFDkL0vk1EJmqRIId_2caDaXUokQUVpOl0smC6Vz0Ut.png 
            While the density of transistors continues to rise, the computing power you can leverage from said transistors is not increasing at the rate it used to. This is due to the breakdown of Dennard Scaling, Dennard scaling is the idea that the decreasing transistor size results in higher clock frequencies at the same power. However, this has broken down as transistors require more static power to run and the working voltage of the system cannot be scaled down at the rate Dennard scaling requires - this means relying more on specialised chips (accelerators) and improving processor architecture rather than continuing to miniaturize it.
        %LOCAL_FILE%Q4ZZGTuw-9knew4SIejsy_O4NY5MIENMPj9WtraKgVUfPcSKFU4XeS2GoYszaS6keQ62ydV4udorTU2XdJ9BwbPsD6FykSnC-dIDlRpqDT5no4GOWHQdqm53hiHH5EYz.png 
            The doubling in density of transistors every two years would result in a pattern of Dennard Scaling, namely
                Chip area halves every two years
                Frequency can be increased (by about 40%) as the length of the critical path would have decreased dramatically
                Voltage must be decreased to maintain the same magnetic field over the chip
                Power & energy decrease in sympathy with the voltage
                Thus, frequency has increased 40% but power draw is unchanged
            Before 2006 this relationship of doubling transistor density resulting in increasing frequency at  the same power draw was feasible, however after 2006 this relationship broke down for a few reasons:
                Transistors leaked more power, static power use rose and transistors required relatively high voltage even when not switching
                Semiconductor technology no longer could improve at Moore's law like rate
                Wires don't scale with same chip area - Writing this from notes, but don't really understand what it means - would be helpful to go through this.
        %LOCAL_FILE%Fw_FBajQEF-Q1GHz6GAARODE3VXBhH3R0LRSkqdRZBXYmxqRgG7vn8gy-EfmFUvKbVD4PG8R6E4mtm2ATTTyn4E3hlZyrRlAn8ZcsIpWvWI84T60DpyNFqGCSomlzvtp.png 
            The breakdown in Dennard scaling requires a changing in approach in the design of high performance general purpose chips, the 1.4x frequency increase every generation can no longer be relied upon to bring hardware improvements. Alternative strategies to improving performance must be employed including:
                Domain specific FPGA chips known as accelerators
                Limiting the number of transistors active at any one time - in that vein is race-to-dark, turning of cores when not in use
                Improvements to chip architecture & ISA
                Employment of different chips for different workflows, e.g. a more powerful CPU (or a GPU) being employed when doing rendering work
        %LOCAL_FILE%rEOnDKh22cKdx73dq-PcN_vOV8Cxn89SGxibA9BzJ7xQI8S8uQLf6rULEmAM-eaeTDB4Ug_Uer2MECtP2NkXhQ72OTzE_UTYrbJTWJwoOjKvDbK3WuAUaRsHuiK6WyxQ.png 
            Dark silicon is the amount of chip area in a integrated circuit that cannot be powered on while maintaining suitable working temperatures. As described above, the efficiency of transistors has not increased as Dennard scaling would have it - so if all transistors were supplied with power in increasingly dense chips, the resulting increase in temperature could damage the chip or cause a fire.  
    
        %LOCAL_FILE%IT2dql__fLGqf4GVNEhdRVjR6-BnrpjXNiugp6PTQvw_I9j5k69Tv1Ks--Ykd6G55c_eFnSdr0NXF0Bnfq2S51OWqpezgQTjtNyZgIA6LYsNC6ur2O8y0tNYxfYTKSUa.png 
            %LOCAL_FILE%p6oneUwYrLCGTrHJs9vTxCUgjN_tN_NPdlHLiLcZ2R31Mlpdau-G6wEKpokuFnXG7JYy8TBo0PPh_tqb-dvSq4DmiVNA-T1lLxeiegE0vgor63iFn6gkmX20JkzBeyQZ.png 
            module seven_bit_nor(
    input [6:0] a,
  
    output [0:0] b);
  
  assign b = !((a[0])||(a[1])||(a[2])||(a[3])||(a[4])||(a[5])||(a[6]));
endmodule 
        %LOCAL_FILE%hvLJ8_w-jNwTuRhid60ORFMQI6QKJZXkC8LH76ZnP4iZ2IE4w0g0eVxf3OW6r5RkQ5ioCwFNi-he9qQbhhPmCi0M9x7sdqXKWkrrkduxhT2DJdtzGg8MmwysRT1TuKr3.png 
            module four_bit_adder(
  input [3:0] d,
  input [3:0] e,
  
  output [0:0] carry,
  output [3:0] result);
  
  wire [4:0] temp = d+e;
  assign carry = temp[4];
  assign result = temp[3:0];
endmodule 
        %LOCAL_FILE%cIXgZzNJHK2WR6WlPb5Xa5O2WJKh45GL9qCrPopl3SFPcaCG-s-u1w9WGvjhgCbyIFpB6r2kDBj2h2Wab-OzxI5hjcdWEk1dr9jAQXL6Hcg3EMaZL8Whr8K88mR2V78B.png 
            module multiplexer(
	input [3:0] a,
	input [3:0] b,
	input load,
	output result);

 	assign result = (load == 1'b0) ? a : b;
endmodule


module thing(
	
	input clk,
	input load,
	input r,
	input [3:0] g,

	output h);

	reg [3:0] ff_out;
	wire [3:0] adder_out;
	wire [3:0] mux_out;
    wire who_cares;
    four_bit_adder adder(4'd1, ff_out, who_cares, adder_out);
	multiplexer mux(adder_out, g, load, mux_out);

	always_ff @(posedge clk or posedge r)
		if(r)
			ff_out <= 0;
		else
			ff_out <= mux_out;
		
	assign h = ff_out;

endmodule 
        %LOCAL_FILE%6K7crCiuD-NaTWO5HDkgVM6FkAe0Of2_sqJQvWVP3upsUajztlz2m-KGH70D7eVV_cRR_H2gri3BqPGzELahOpgmJNKlqGC3AkLtPehk75w1wo19H5XEy5BjYc71zyOa.png 
            %LOCAL_FILE%BUjnDkUsxRvCapdfbOZPBKiFsYzgeUIhACSbI38k3SkiAJHDSdGse8ICaWWLaDhY1QPEzayhsBQazHEWL8mEiPFuKe6xKYh2ACD0Ay86olZl4U2_cY3jZ_Q8vlc3HAPC.png 
        %LOCAL_FILE%rOhjWwznStNn4uT7WgYJKTEYQY4qRLbaELNgyfAg_NM3wpopMfnPgCTJQjnNMs_sWeE2SDwzHA8sfu83f4BRt2tp23FdJE-wgyPDlw-hZDFfbhuxHmg4VccfcEJn3ofm.png 
            Whenever Ain is 0 & Bin is non-zero, 
    %LOCAL_FILE%62jYcXcvp5-K-KT9jGUFLq0G8ymM7zsMMkrns5IvjLMBDOFIj_hYmDl0Uu-FPw3unnSyFYHrkKInj3tYN7P7WcJIENoO7_UWgxJx9CieJwrKeJ72aOLVsCwi4GZLAmri.png 
        The mystery module behaves like a stack. 'head' is the index of the head of the stack and mem is the equivalent of an array (that head points into) that stores that items in the stack. The opIn command puts the value in dataIn onto the top of the stack and opOut simply moves the head of the stack back one (allowing that value to be overwritten in the future). When the module is full, if we try and add an item error will be set to 1, meaning we will not be able to add another item. Similarly, when we are empty and we try and take an item out error will be set to true and we will be stopped from doing so. When we are empty we have the additional side effect of outputting -1 in dataOut. op decides whether we are inputting data or outputting data or neither.
        %LOCAL_FILE%hUkjVm7UBOSGw-ABIHcSOtftvD9mLbfGtF8VIgib2ETzsgFn_3vwsXGxUFPyBesxk70KJMVN-DGsja5FjlBTelQFspG3eQMSnM_LpRYyv_H9nyR77FhX4LWjKMsxofik.png 
            Production tests are used when the chip comes off the production line, it tests whether the correctness of the board is maintained throughout the manufacture process. Functional tests are done before production, and ensure the correctness of your HDL code.
        %LOCAL_FILE%ufaK7X_Tg3kA8QzEtneISxNOefoDhJE0V6Bwh8EMdc0lNl24lQDjdz45KY_jSQnZ3BoMOUZaiGkFQh7PHbsB_cERPZb1pGLlXTkJ7_H-wzYumQeY3U19dJK9j7kMQG_s.png
            The mystery module is parameterized by the width & depth variables, thus there are a huge amount of permutations of those values that would need to be tested. The opIn, opOut & opNeutral values would all need to be tested - and varying ordering of the commands would  to be tested, resulting in thousands of possible permutations. Additionally the correctness of the outputs of the stack would need to be tested according to varying inputs.
        %LOCAL_FILE%61sohsSD65S5uRDO7M5mmOgV9Cl-ttxt3bT7agOqJCBQKBffyOlE7wriUFv-K7DgJEA2Ypjvj8YJOmEqUlqVRJrugPn9eJvIEfwbDq9tse1AiMDeUsoDqHjZyftziDRh.png 
            Similarly, the correctness of opIn, opOut & opNeutral and permutations thereof would have to be examined as well as the effects of varying width and depth - however less permutations need to be tested, as we have concluded the logic is correct and are now testing the physical board where any problems would appear with less nuance than a small logic error. However, this process is made more difficult as we are using a physical board - it becomes more difficult for testing in parallel, and physical factors like temperature and humidity need to be considered. So individual tests may take longer, and we must ensure the board is kept at the conditions it was designed to be used in by users.
        %LOCAL_FILE%L9w2S79jAzPJe-B5MEgJrZ03BGSBkt6ryzB0yvzSWy6YJnEalCGKflbVuEEnCT8EnxOGZGmphXuvmJHHIgbQbo5e1g2J3nlZ3sSZhNL8s3thrK64Ai1YlI9irxlIGCQq.png 
            SystemVerilog is a hardware description & verification language used to build logical models of hardware that can then be simulated & tested before finally being implemented in silicon.
        %LOCAL_FILE%s_dGIjzu1_bYbflvHG1GqIQhDr3wBoOryD0wRVWVCyiN6pAc-21l-DmB-dHReBb1K4qy7jBN09IW3LXdZ_Er7L0PsQNeynZaSeNrZK5awAt4wP1JLQnjySt3ubgF1bHM.png
            Meta-stability is when the output of the flip-flop takes a value with voltage between that of a high & low signal. It occurs when the input to the flip flop changes during it's setup or hold time, and the flip flop captures the value of the input as it's changing.
        %LOCAL_FILE%rljtTxKhNSKzEPi1Q5loesdd1fpx_6hXt-vg42OuntEKsxFmYR8YCUiuM_cGTjeV4PMnY7az-5OR-AB6gUor50g7dvs4gHpRrUzmsOADWhjf35daaVoLOl9YYuVEt_5U.png
            FPGAs comprise a sea of interconnected logic blocks & IO units. These logic blocks can contain complex features like flip flops and lookup tables, and can be programmed to implement a variety of functions. They benefit from being directly programmable by the user, that is one needn't invest in a chip negative for millions and produce your design in silicon - this means programming one is much cheaper than the alternative. Because of the vast array of logic blocks, FPGAs can take advantage of massive parallelism resulting in quick computations. However, FPGAs are much slower per gate so this parallelism is necessary to reap a performance benefit.  
    %LOCAL_FILE%9JfvfQNG88PPatL-LGthmAf1ekA6VqJBTBChY92IumRxKw24x0ntjkLvUzM3ZV_UObXKGqhdW8oZCKDv9HvetY8ptV4R6SvHMBd1ReiQGfLE3VqcKcrEsKWXtP0KCHIi.png 
        In C's case first pre-processing is done where the plaintext elements of the code is changed. Then the compiler compiles the C code to assembly code, this is ISA dependent as different instructions will be deployed in different architectures. The compiler will employ tricks to improve performance while keeping the semantics of the code the same. The assembler then converts this assembly code to object code and finally the linker combines various .obj files into a single executable file containing machine code.
    %LOCAL_FILE%Sdh8cPzeyLI-XzDa6wMfkiZYUB6H_Zki2pKhO_YN3Ns3c_Cd1JHwxAtAhDZ1pu1Ami8Tl67xRH55V6FFll45UCaBIp_jWUQ7AO4j4wtkLEoRCqk4knF7CI_GcEe-lMhw.png
        Clock Speed - The number of times per second the CPU clock flips, equal to: f \leq \frac{1}{\text{Time taken to traverse critical path}} . Operations can be segmented based on the number of clock cycles they take to complete, and all are sped up by increasing the clock speed.
        Number of cores - more cores allows for greater parallelism and thus a potential speedup, this does require software to be written to take advantage of parallelism.
        Cache size - as fetching from memory is such a slow task, having a cache large enough to minimize those expensive calls is very important when trying to improve CPU performance.
    %LOCAL_FILE%g0pdH_L0Ima0pUXs9OKScuB2D6KMgpQBOK8bRQXVNuGGd4YUOCHTjIuTOuDn4mbe9YjsJu96F3uCt8T5d5g8-3koJVb3dsY5T-P1_ZtOeXm0cm2F_khcU2PV50yInyY_.png
        An ISA is the set of primitive instructions the CPU can perform, ISA matters for a number of reasons:
            Ease of programming - One can design ISA instructions that couple multiple commonly combined instructions together to make the life of assembly programmers easier. In RISC such instructions could be left as subroutines.
            Making the general case fast - Including certain operations into the ISA can result in speed ups in specific operations that will rarely be used (e.g. supporting integer operations) due to pipelining. Thus choosing which instructions to leave out, or how to use non-special-purpose components for a given instruction requires careful thought.
            Some parts of the modern software stack is ISA dependent, meaning porting those sections to modern systems would be incredibly expensive. 
    
        %LOCAL_FILE%hSiQu5PlT8ACwIkSvrP7QR59BevxaMb9YVZOVrd72gcmD7ONRE87Qq0XY_iUHb-sZke0qV9Mo3060_YtFMjL3kjOJu2x8zQoyUG6we5DDRM8kYSGECzgwqA61IXsyqee.png
            Moore's law makes predictions about the density of transistors on the CMOS chip (namely that it will double once every ~2 years) while Dennard scaling makes the prediction that clock frequency will increase by 40% every 2 years while keeping power the same. While Dennard scaling is based on the assumption that Moore's law is true, it goes further and assumes increasing transistor efficiency. Dennard scaling broke down in 2006 as transistors required too much energy even when not switching, however Moore's law still holds today.
        %LOCAL_FILE%UY0yfJpN6phy-eB9hvgQ8zb-XyGz1uy9gljT2bTMYvVfdOY_EehHgKuHVUtdm-aK4iLXdRS_gJN8kk9kNvtfh5QBu_Ks21NuUw_QuEqCXalCgUT-Su4MqvY2lq6OPKZp.png 
            The critical path is the longest path that data can travel in the circuit - an exhaustive search could be carried out by testing every input for every state & measuring the time taken to finish computation. Alternatively, we could model the circuit as a type of graph and compute a breadth first search - recording the largest number of steps in the search. The critical path directly impacts the maximum clock frequency, in that the clock frequency is:
            f \leq \frac{1}{\text{Time taken to traverse critical path}}
            Thus, decreasing the length of the critical path would allow you to increase the clock frequency.
        %LOCAL_FILE%sZU5STJY-guUdE3BQKXz05R38ufvi96_JuO8__Gyvy-s_Q1IZ9jQ2FMe83wU57ZB1AyOmj6Q6vhwYopP9mIrqF_gwdK52wpZVyQFbII2GpcxT6E7JRL7sJiwee9hg3wq.png 
            A calling convention is a low level scheme for how functions receive parameters from their caller and where they do with results, for example MV r0 r1 moves the value stored in r0 into r1. It can impact the design in a big way, calling conventions imply certain big decisions such as:
                Max instruction length, decides the total number of instructions possible
                Max number of immediate parameters to a function, e.g. parameters that don't need to be fetched
                Where return addresses are saved
            %LOCAL_FILE%BlFcoIXChRxj-7sJ7kp-w1qtgRgmJVcwHgNY9niWjD3TvdTtUvetXCboQ-cPtmM99T17rV38pCr_s6_zcGGjV1gQjSxxBDzzPhZrcLgC062aMOSSTZ4l6eEFqpLRvye4.png 
                Section A: This section checks if we need to do another round of the function or if we're finished. If a1  0 then jump to .L7 otherwise jump to ra (this will either be to the initial function caller or to another instance of gcd)
                Section B: This section allocates space in the stack, and saves registers so we can make our way back up the stack frame to the initial caller. Allocate 4 spaces on the stack, store the current return address in the first segment of the stack.
                Section C: This section computes the modulus and recalls the function with these new values. Set temp = n2, n2 = n1 % temp and finally n1 = temp. Then jump and link to gcd (storing the current instruction location into ra).
                Section D: We'll reach here if we've previously completed section C and section A finds a1 = 0. Here we retrieve a previously store return address from the stack, free up the stack and jump to the return address. This starts the process of unravelling the stack frame.
    %LOCAL_FILE%sBPoSMEcGwfeclHCQG5umDhvhCzuEURc3ZFKjKlKxZWrAqVUb05SDX2eaWMJOC8QpkfIjBRGoNxhcRdC4XDdaIiJtDvfcRC-5S02rkInpNg2NzzkCnvWZBbX9yoCyJl0.png 
        Von Neumann style processors are not the be all and end all of general purpose processing, the incurred Turing tax results in slowdown & massive power use. As the golden age of silicon is over, we need to consider accelerators & other methods of improving performance that our outside the Von Neumann box. 
    %LOCAL_FILE%xzHKmOBmsp5VTUoRYJqnaeoGLIMjg6wfYAlFayAT7WDYyE3UH1rDsBABqCNr8M6_ZfUR7SyD0-tUrPiXdfiXuO3t1uuT3vqOiMDug6FC-Z0X_z9Z_q2JxwWc_W0nGfvb.png
        Clocked digital designs are used everywhere, propagating the clock signal is very hard and requires careful thought & design. Designers need to ensure that their system meets timing requirements and handles asynchronous inputs safely.
    %LOCAL_FILE%KnuI9ow0ir_IgodEqmPCY7IHvLAA5uGz_W4OqIwyGOIrbGVgtUaub1gRxD2OMrQg2JLYTmNUH5OKJLuUMoJLFzyHknqz358TnMcq-rMQVSx21tmKPfKUcYzwonBnDHlT.png
        There are many possible elements to optimize in processor design, and while one optimization could help a given workflow it may hinder another - thus there's no perfect CPU architecture only compromises. The Von Neumann model doesn't help with improvements, most transistors are used moving & storing data rather than performing computation. 
    %LOCAL_FILE%M6Mcdr232g4ok7gXv5Jnt0aBIXo-zv-LY1wfcfH3O-o8qTWJ_IbgZSIJJN-ma2PeMKIeiC8JlpD4XwHqS3srsVvlCk8lnU8OBJjeMph6pjP8HqEP4LLLL_GZ1rw1InkS.png
        RISC processors are often simpler and lower power, but CISC still dominates the PC industry (ostensibly due to backwards compatibility issues). RISC is used in the phone & microprocessor space and also in the research & teaching space as it is open-source and (relatively) easy to understand. 
Lecture 1:
    Different Processors are created to fulfill certain requirements, what are the 3 main requirements?Power, Area and speed
    Give two factors that constrain the production of CPUs with higher speedInstruction set parallelism and Power
    What is the main benefit of devices having general purpose CPUs when a preprogrammed gate array would work?General purpose CPUs allow for firmware upgrades in the field
    Von Neumann Architecture
        Almost all processors use this architecture 
        What are the two main features of Von Neumann architecture
            A linear memory storing data and commands
            Control flow processors that execute sequentially 
        https://www.cs.mcgill.ca/~cs573/fall2002/notes/lec273/lecture8/vnb.jpg 
    Memory Hierarchy
        Various different memory storage types with increasing size and decreasing speed. 
        Processor Core, L1 Cache, L2 Cache, DRAM
        What feature of processors poses a problem for fast memory fetchingSequential execution, means the processor cannot asynchronously fetch data
    Manufacturing cost
        What is the most crucial factor for determining a chip production profit margin, and what is that item proportional to?Yield, which is proportional to Chip size
        One of chip production cost is going up, and semiconductor fabrication foundries are getting  more expensive exponentially
    Wire Scaling 
        %LOCAL_FILE%1T9JycMAAFENrnfZUO0ti3ABwW9gPPx7tlIvpaxhOCk1D53dJNi65eBCRM6mtfzcDtHO6cPcfvmz_juAgfvUAO6oZ3_swOANtSJ3F89qkLy0QdS604_CmJIUBd_MtVwp.png 
        R \propto \frac{L}{W+H}
        C \propto w \cdot L
        T \propto RC \propto \frac{L^2}{H} 
        Where T is the "charge time", e.g. the time between applying a voltage at one end of a wire and reading it at the other. So performance doesn't increase with the same chip area
        Halving L makes the wire go 4 times faster, so can add buffers
    Dennard Scaling
        Sequence of Dennard Scaling
            Transistor dimensions reduce by 70% each year
            Area decreases by 50%
            Frequency increases due to more densely packed chip
            Voltage must be decreased to keep electric field the same
            Voltage decreases 70% reducing energy by 65% and power by 50%
            So every generation, transistor density doubles, frequency gets 40% faster and power stays the same
        Causes of the 2006+ Breakdown of Dennard Scaling
            Cannot further decrease supply voltage as transistors leak too much, static power demand increases vs dynamic power (when transistors actually switching)
            Improvement of semiconductor technology slows - no longer delivers on Moore's law like performance
            Wires don't scale for the same chip area
        Is Moore's law dead?Moore's law is still (just about) delivering on the doubling of transistor density every 2 years, however due to this leakage and static power issue we need to constrain the number of transistors active at a time. Move to accelerators.
    Age of Dark Silicon
        What are three methods to improve processor power efficiency in the age of dark silicon?
            Use domain specific processors (accelerators) which are more efficient and often faster
            Race to dark - Keep transistors depowered when not needed
            Use a range of processors for different work loads
    Voltage and frequency trade-off
        Active Power = C \cdot V^2 \cdot f \cdot A 
        Where A is the proportion of transistors actively switching (Activity Factor)
        For a limited range of V, f is proportional to V - so why not just crank V?Heat could pose a fire hazard, and CMOS circuits get slower as they heat up
        What is DVFS?Dynamic Voltage Frequency Switching, adjust voltage with frequency to trade off performance and power use. Similar to overclocking
    Low power at idle
        Would like power use to be proportional to load, however the i7 uses 258W at 100% and 121W at 10%
        Static power makes this difficult, and having a lower static power results in slower transistors
    Moore's law for storage
        Moore's law for density applies to DRAM and flash memory, because they are made of transistors
        What is Kryder's law?Kryder's law is the equivelant of Moore's law of increasing density for magnetic HD. However, the size of HD increased much faster than Moore's law
    Design and verification gaps
        As computers get larger and more complex we get problems with managing designing them
        Solutions to the design and verification gaps caused by large and complex systems
            Hierarchical Decomposition
            Replication of parts
            Libraries
            Abstract interfaces
            Higher level hardware description languages
    General Purpose Chips
        Not power efficient and slower than the equivalent FPGA which have 15x slower gates - this is due to parallelism being exploited. 

