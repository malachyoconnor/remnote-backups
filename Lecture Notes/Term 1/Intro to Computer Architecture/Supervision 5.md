1. Discuss an example of a problem that we can solve efficiently by using CUDA.
    - CUDA works only for Nvidia GPUs, so the problem needs to be parallelisable (problems like video decode, rendering, deep learning etc.) and preferably with enough data to take advantage of the large scale compute GPUs offer. Due to CUDAs relative succinct syntax, the solution can be developed quicker than with OpenCL and will run faster than OpenCL code **on nvidia GPUs. **I.e. while OpenCL code can be run on NVidia GPUs, CUDA exploits methods to make it's code run faster.
2. Discuss an example of a problem that we can solve efficiently by using OpenCL.   
    - OpenCL can be used for problems where the final device the code will run on varies. Because the code is compiled at runtime, OpenCL can cater to many different device types (CPUs, GPUs, FPGAs...) as the ISA is optimized for at runtime. Thus, if you were planning on implementing your code on a microchip without space for a GPU, and on an FPGA OpenCL would be a good option. However, the hardware you're planning on running your OpenCL code on must have support from the vendors as otherwise the API will not work with the device. OpenCL can provide many of the same parallel features that CUDA does, however on some of it's application domains those features will not be applicable (For example CPUs with modest numbers of cores).
3. How is it possible that so many different devices, with completely different internal  architecture are compatible with OpenCL and enable acceleration of execution?
    - This is possible due to two reasons
        1. OpenCL code is compiled at runtime, allowing the system to implement system specific optimizations when the physical characteristics of the device can be determined. The alternative method would be to have separate binaries for every device.   
        2. OpenCL gives the programmers set ways of abstracting away from the metal. Memory is viewed abstractly, threads and thread grouping is viewed abstractly and the platform model abstractly models the system with one host and multiple devices. 
4. In CUDA, what’s the difference between a thread block and a warp?
    - In CUDA thread blocks contain multiple warps - while thread blocks describes a set of warps all executing the same kernel, a warp describes a set of threads all executing the same kernel. Warps within a thread block can communicate. Thread blocks are executed on a single streaming multi-processor, with individual threads being executed concurrently. 
5. Why does OpenCL require an application to discover the available platforms and devices?   
    - Because we don't know before runtime what platforms and devices are available, OpenCL code can be run on a wide range of different platforms and devices. This also forces the programmer to consider that fact.
6. Compare and contrast the OpenCL programming model with CUDA.   
    1. CUDA built for NVIDIA GPUs only, OpenCL heterogenous. Meaning OpenCL requires more general programming approach that will work for many devices.
    2. CUDA specifies host and device, OpenCL specifies host and devices. Both have sections run on the host CPU and the devices. OpenCL uses the kernel keyboard while CUDA uses device.
    3. OpenCL much more verbose than CUDA, more code required as less guarantees about the devices and their capabilities.
    4. OpenCL has work groups and NDRanges - while CUDA has warps, thread blocks **and grids. **NDRange comes from a use of mostly 1, 2 or 3d problems in most target applications of parallel processing, CUDA gives you more freedom to structure your solution how you'd like.
    5. Compilation at runtime with OpenCL, meaning you can't know the exact machine code your program will generate - with NVidia you have more guarantees.
7. What types of application best suit GPUs and how can you program to take advantage of the  various forms of parallelism available? Try to describe examples.
    - The best applications are so called 'embarrassingly' parallel applications. These applications require the same operations to be applied on a wide array of data items, with (crucially) individual operations being independent. This allows all the operations to occur at once, with minimal consideration on whether another operation has completed or not. However, one must program in such a way that the parallel operations are truly done in parallel - for example in graphics the different types of the Phong light model are independent, so rather than doing them one by one in - load them all into the immensely capable GPU and let the cores work away. Another example would be machine learning, while layers depend on each other for back-propagation, we can pipe in partial results from one layer to start working on the next - partly parallelising a linear sequence. 
8. 
    - ![](local://C:/Users/malac/remnote/Malachy_O'Connor/files/4ouCT39UXVo_2Bj7qJhThMxfe_vyMST5gsBkVM-nYLDRVEczCQPWTeC682qd3XYi7ev-Yxp_eS8B81hHJxaArA1PcHIuWtWGLg33HbqWjk-UfwxRh92RVUusa5o9yQEf.png) 
        - Platform Model - Describes how one host interacts with multiple devices. Using a rich API to facilitate communications to disparate devices.
        - Execution Model - Describes how the host interacts with devices, an abstract environment is prepared to manage memory objects and interact with devices.
        - Memory Model - Describes how memory is logically (not necessarily physically arranged) where work groups have their own shared memory, work units have private memories and there are device global memories - one read only and one writable.
        - Kernel execution model - Defines how concurrency is mapped. ^^This wasn't really discussed in the lectures, would be helpful to go through this.^^ 
    - ![](local://C:/Users/malac/remnote/Malachy_O'Connor/files/H3ilrOTcONHsImYWvjkSJBeFEo9leITtzTe7jiTnlRn4YtakmfeQyrbb_Qkmf5FxDwUSwoP6vq3qjhEol_xbZoZRgvNVYRZVQ2Oixupxv0-lRigrvS1dtv3qu5D415Gs.png) 
        - Individual work items have their own private memory containing their stack and registers for computation, this is very fast to access with no congestion. Then work groups are given a shared memory to facilitate communication and allowing for a single fetch for all work items rather than multiple different fetches, this is slower to access but is still quick. Multiple work groups can be running the same kernel, and each of those can access a global memory, which is part constant read-only memory this is slow to access - but large chunks can be loaded to work group local memory at a time. 
    - ![](local://C:/Users/malac/remnote/Malachy_O'Connor/files/hIyewZoz0zMszjLOw_8Fhc9vO2cWNX9KLO1Y8Fj1AQo2SEjqXrRCp5jE8pFEUQBWnDyj57QuFpxZPgHD039DzVEJ-UqOX1K3dYrptvYfgwdv34jySrYMs-rQnMdx_Y1I.png) 
        - In CUDA calls are scheduled based on if they are blocked, the scheduler orders commands for maximum efficiency - the programmer must specify the grid size and the number of warps in each thread block. The function must be specified as running on the device, and is then launched form the host computer. 
        - In OpenCL command dependencies replace the scheduler, with guards used to ensure dependencies are not violated. Each command is given an event and a wait list, where other threads are dependent on a given threads event. NDRanges containg work groups much like warps.
9. Why is energy efficiency the “new fundamental limiter of processor performance”, as Borkar  and Chien say?  
    - The problem is no longer the number of transistors that can be fit on the board, we now have the issue that we cannot control the heat produced when a large proportion of those transistors are switching. With the breakdown of Dennard scaling (due to transistors requiring more static power), transistors are used actively sparingly to not overheat the board. A similar effect describes why frequency cannot be easily increased as it had in the past, the heat cannot be easily dissipated and hotter transistors perform slower.  
10. Describe Arm’s big.LITTLE system.
    - Arm's big. LITTLE system pairs a smaller more efficient CPU with a larger more powerful (and more power hungry) CPU into a single SoC, such that the system appears to be a single multicore processor to users and the OS. This allows for real-time selection of the appropriate processor for the task, depending on the required computation intensity and the power budget. The vast majority of the time, the smaller low power CPU will be in operation saving battery power - however when the user undertakes demanding tasks like gaming, video decode and certain web browsing the larger more powerful CPU steps up to take the load.
11. When might it make sense to implement functionality in a specialised accelerator rather than  within a general-purpose core?
    - If energy efficiency and speed are a important, and the task is specific and unchanging then a specialised accelerator is a good option. Accelerators are faster and more energy efficient than a general purpose core, this is because they can make much better sue of parallelism and take advantage of the fact that not everything needs to be routed through a CPU. However, they are custom designed for a singular purpose - so if a better algorithm is found then the accelerator would need to be replaced to be upgraded, which can be expensive. 
12. **Discuss a general multicore CPU vs ASIC vs DSP**  
    - A general multicore CPU does implement some parallelism, but is designed to carry out any problem one can dream up to throw at it (albeit not hugely efficiently for all) - DSP and ASIC on the other hand are designed to solve specific problems efficiently and quickly. DSPs are for signal processing problems generally concerned with mathematical operations, they are SIMD and allow for lots of parallelism. ASIC are designed for a very specific operation, customized for high performance and efficient power usage. ASICs are more general than DSPs in that more problems than digital signal processing are solved with them, but only one problem is implemented at once on an ASIC. ASICs have a much higher one time cost than CPUs or DSPs as a specific solution must be painstakingly designed and implemented on silicon, while CPUs are very general and can be bought off the shelf and DSPs are cheaper to specialize to a given signal processing problem. 
13. **Discuss Approximate Computing:**
    - Approximate computing is computing in which (generally small) errors in results are accepted, allowing for far less error checking and the use of algorithms that sacrifice correctness for speed. Approximate computing is deployed in areas where correctness is not critical such as videos, gaming, graphics, audio, etc. The thought process that a missing pixel here or there in a single frame of a > million pixel image will not go amiss.
14. 
    - ![](local://C:/Users/malac/remnote/Malachy_O'Connor/files/HJi56y0nwzE0cilTtsd0vFKyRasLtGjNsw1DJmHZ2k_Z5AZbF26xUzydqNZ8R4cDZGqlx-uz2Mr-vqVET3nPtWYHUTRVkzh9IpfC04h0gM2H0H8no3diZPWaPBipsAAb.png) 
        - Computational sprinting would be perfect for workloads where the general case is simple, manageable computation - but rarely a heavy serial amount of execution needs to be undertaken, for which a speedup of clock speed will reduce the time taken to compute. In the case of serial execution, accelerators usually take advantage of parallelism to make gains in speed and energy efficiency, thus accelerators would fall behind. Additionally, if the general case the CPU can manage the load. This system would also be cheaper than a specialised accelerator and allow for on the fly patching.
        - ![](local://C:/Users/malac/remnote/Malachy_O'Connor/files/IYi4VtVDDSvT6SSNeJaOQg40uKCtASXOwlFUH6X8fD715TIU5JbCDXzquNypf5adAkXxMJorq5-LTdmZf3hapXwMQqZpgvTuQcEaPVjeMhAJhLpTMtxS9URgZlW0hExL.png) 
            - Advantages:
                - Can speedup tasks that the CPU regularly uses, as well as reduce energy usage.
                - Can change the function the FPGA (acting as an accelerator) computes, depending on requirements, changing of the algorithm or depending on user habits (e.g. a user that does more graphical processing could convert the FPGA to do rendering).
            - Disadvantages: 
                - Added cost of buying an FPGA. Additionally, FPGA need a lot of power supplier for their constituent controllers - need to have access to lots of those.
                - Need to keep the FPGA close to the CPU, so time isn't lost transferring the data (energy will be spent during transfer).  
                - FPGA slower than an ASIC and less energy efficient, so if we end up not upgrading the system we've lost out. Additionally, while ASICs have a high up-front cost they are cheaper per-unit - thus if a large number of these boards are to be produced use ASICS.
                - Reprogramming the FPGA can be arduous as you need to use a hardware design language like verilog, which requires a lot of expertise and low level knowledge (far more than a language like c), and a very long compilation process (hours long) needs to be undertaken (place and route very slow). 
15. **Discuss the Spectre bug** 
    - Spectre takes advantage of speculative execution to extract kernel information the program should not have access to. This is done using speculative execution, where commands within branches are computed under a probabilistic assumption that the branch is likely to be computed - while the CPU does roll the changes to registers back if unwanted computation was performed, it cannot roll back the changes made to the cache. The cache can then be probed by repeatedly getting the CPU to read data and timing the speed it takes, if the data is read quickly (you start this process by flushing the cache so you know you aren't reading an unrelated value) then it's in the cache and must have been what was read from the kernel. 
    - This is an interesting bug, because it was prevalent under many different CPU manufacturers and was caused by a design issue within **hardware **not software - meaning it couldn't simply be patched on the fly. This resulted in millions being spent patching the bug, CPUs having to be redesigned to address the issue. 
16. **Create a diagram with all discussed elements of a computer system (e.g., cache, RAM). Discuss  common units of time for operations that these elements perform (e.g., time to fetch memory  from cache and RAM, time to execute an instruction).  ** 
    - ![](local://C:/Users/malac/remnote/Malachy_O'Connor/files/f-uF3MGr2QyRTAJRFcKgoGMmxdh6wtTBzNR1fJGCFn25DufzT_OEp61iAggmyk0HybwuVUX0zMdRTKlP5KijPkXPlLmrsYAdEzukkCjPYvOAPdKvLCq4mWxzAKZv3Kdt.png) 
17. 
    - Write a 1-page essay (no longer) describing how topics discussed in different lectures are related, and how in your opinion they relate to software engineering. Focus on identifying conceptual solutions discussed in different lectures and explain how these abstract concepts are applied for different concrete problems, on different abstraction levels.
    - Discussion of GPUs, FPGAs and parallelism improvements -> Moore’s law and breakdown of Dennard scaling -> Discussion of increasing complexity of computer architecture -> System Verilog and HDL -> Abstraction to and away from the metal -> Using different cache levels and relation to C programming -> relation to security with spectre and meltdown
    - Three areas that had direct relations between lectures were that of GPUs, FPGAs and concurrency in general. All of these topics move away from serial ­computation, and towards parallelism allowing multiple computations to be performed at once (with individual instructions often taking longer to compute than in a serial CPU). GPUs user a huge array of threads which are split into ‘warps’ which operate serially in lock step, FPGAs design varies and makes use of concurrency as sections of the program can be computed in parallel and then combined. Abstractly concurrency is used in both, but on the metal, they employ the technique in different ways.
    - Concurrency is something programmers are increasingly having to take account of if they wish to gain large speedups in code, this is because of (and relates to) the decline of Moore’s law and the breakdown of Dennard scaling.  Dennard scaling relates to the increasing amount of static power transistors use, resulting in power requirements not shrinking with the size of transistors – this is part of the reason for the slowdown of Moore’s law, as a large number of transistors cannot be made use of due to temperature constraints. The solution to the decline of Moore’s law is twofold, one is increased parallelism of architecture and software and another is cleverer architecture making further use of techniques like speculative execution, accelerators (which make use of concurrency) and pipelining (also uses concurrency, different stages of execution computed at once).
    - These techniques relate to the discussions of increasing complexity of computer architecture. As systems become larger, more complex and with more people working on them the design of chips has long surpassed what can be contained in the brain of a single person. Computer architecture is now developed using the cognition of hundreds of engineers over tens of thousands of man hours. While systems like System Verilog and other Hardware Design languages be a solution to reduce complexity, computer architects still need to be incredibly focused and detail oriented to make progress (the required focus is rewarded with a big fat salary). Another tool for conceptualizing massively complex systems is abstraction, allowing for thinking at the level of individual transistors up to the level of communication between different components on the board. Similarly, programmers should have some conception of these abstractions when writing performant and accurate code – for example, a thorough understanding of the working of a cache can help write more performant code using techniques like preloading, loop-blocking and struct-of-arrays.
    - However, when a large system of huge complexity is deployed, unseen problems can occur (despite extensive testing through fuzzing and other methods) as they did in the use of caches. The problems in discussion being Spectre and Meltdown, these two disparate functions of the CPU, that of speculative execution and caching, to leak kernel information to people who should not have been able to access it. These bugs cost (and is still costing) companies across the globe millions as they rushed to fix the problem, also resulting in a palpable slowdown in the computing devices of all large vendors across the glove as systems were needed to ensure the exploit isn’t employed. Similarly, software developers need to be cognisant of the security of the code they write – and ensure exploits are detected or ideally caught before they make their way into the codebase.
18. Summarize the main message from “Lecture 15: Cuda, OpenCL” in 1-3 sentences?
    - New programming languages can help build better code in specific areas. CUDA is Nvidia's approach while OpenCL is more heterogeneous. Modern GPUs are multithreaded multiprocessors, with multithreading hiding latency from stalls.
19. Summarize the main message from Lecture “16: Future directions. Energy efficiency.  Performance. Reliability. Security” in 1-3 sentences?  
    - Computer Architecture is changing rapidly, with the rise of multicores and accelerators. There are lots of good solutions already, with more sure to arise. We are in a new golden age for computer architecture.
