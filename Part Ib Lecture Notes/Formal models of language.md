- **Lecture 1** (Regular grammars)
    - Natural languages 
        - A natural language is a mutually understandable communication, used in a population. Speakers tacitly agree on what strings are allows (grammatical). 
            - Dialects and social media langue are natural languages in their own right.
        - Natural languages have **high ambiguity**. As opposed to a programming language.
e.g. What does "I made her duck" mean? 
        - Types of **ambiguity**:
            - Morphological ⇒ her can be a dative pronoun (I cooked duck for her) or possessive pronoun (I created her duck out of clay).
            - Syntactic ⇒ Make can behave transitively or ditransitively (takes 1 or takes 2 objects respectively). I made **her duck** vs I made **her **duck. 
            - Semantic ⇒ Make can mean many things (create, cook, cause etc.)
    - Formal languages 
        - A formal language is a set of strings over a finite alphabet.
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/oL-mPUR-xQv2wUbTgvxIR6dZhpfRZZ9bqaopoNCFE8ScHgMaG8vlVbJ21Saef5VlLX_zPobKW6Q6xcoNFyqvM-CmAbGMoxlJsuNSsgjlLdzbeCT7SOhS2h_zD74xN1nB.png) 
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/KSTciC4AT1plKHmhONryRr2vBWB-bV9d-kI2iP53e_DoOtxotuG0Mx7Dg2d1YWECJcc_CLOleb9QNhLoLGxjuntzHIc2zVDBi6jhb1DDSsHIlmVHClI9iSfSJT__J7n0.png) 
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/ozxrdAafqcXNQT-tF6c-CzVwyy9tfQbqiSTjcIFtVw-BWA2RnLEnCHSIfEwWJ1ztYjv6mWblZabs_Cc03yo3oRDN4z4VBjmQvy2wNGkFoiAyzTC3UtlSdEZyn46Y6jDq.png) 
        - When is a language regular?―If it is equal to the set of strings accepted by some deterministic finite-state automata. 
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/MYgVPEcl_CAdi-hdhwk0-effFlQGZKXR4rMpYETW-QQrkFs02klsNXiA5ZUroCjZdV6m7ngqQfZH1CAYnXNL_7R3bAhKnQOaiKVUJDyORhH0_WT7AcD9UBVauUZf1RBN.png) 
    - Regular grammars 
        - For every finite automaton, if we write the production rules we find another way to generate our language - i.e. we find a regular grammar:
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/kPx38aheWfPaTT_JDsq4BpFAuLrXYrD73eJ74FC8glqgTfp2gXLqTrjF_HZ2beM_A2nZeLTyBEr24Iy1KRUBWTPX2I2K8e2I7_KvcKMhELE8tpFlZh-PpP76BNoJ_rCf.png)
 ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/i2JbPWNnxpvPN7pummQsukkxykSDP3RQZ3mCFneavc6gmfvNqUU68KMFn9WeyUhW1RqY5N2CYW5RaIDOPjIYDI9nxYNunfQxTPUNc6xqzbaTCSp07BAFaVsqxAZzw8ox.png) 
        - $$S \rightarrow bA$$
This production rule states - if I start at state S, I can output a "b" and progress to state A. 
        - More formally, the language of strings accepted by a given DFA can be generated by a regular grammar.
        - Describe what each of the following symbols represent:$$G_{reg} = (\mathcal{N}, \Sigma, S, \mathcal{P})$$ ↓ 
            - $\mathcal{N} = \{\mathcal{Q}\}$  the **non-terminals **are the states of M. 
            - $\Sigma$ = the **terminals **are the set of transition symbols of M 
            - $S = s$ the **starting symbols **of the machine M. 
            - $\mathcal{P} = q_i \rightarrow aq_j$ when the machine would transition from state q_i to q_j when it encountered the string a. 
        - Within a regular grammar, a derivation consists of repeatedly expanding the non-terminals until we cannot do so any further. 
        - Every regular language has a **left and right-linear **grammar: 
            - The rules of a regular grammar can only be expressed in the form:
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/YsFDgy5fcrHEsBQXXUU8EU_5xLcWIOMik3X7ogyPiv57LQzkXLZtuYlj5R7Q0lbzsc9t3WF5NCZJdKZUgPKcQ08qVEZTTxEs8e4xnMNjrEhPIjq2rAUHDlA68xebY_CK.png) 
or in the form:
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/yLhMYgrFPgJrNZl-XRvq89p3G7g2AL9SNuSdYGiRCzLnR8uh6lVE4WpWFdcdK9cWW6RH5E3cUOEPIFBb4xa9CMilhMJQ3xGwh9rY31yALT32jCf4TEBqJnC_M12Tmmum.png) 
Are these two forms equivalent?―The two grammars are **weakly **equivalent since they generate the same set of strings. 
However, they are not **strongly-equivalent **because they do not generate the same structure to strings (I.e. the derivation trees are different).
    - Phrase structure grammar 
        - More general than a regular grammar, simply require that the left side of a rule has at {{least one non-terminal}} - while on the right side we have {{any combination of terminals and non-terminals.}} 
        -  _Phrase structure grammar derivation_ :
            - If $u_1, u_2$ are some string of terminals and non terminals - we can convert from $w = u_1 \alpha u_2$ to $v = u_1 \beta u_2$ **if **$\alpha \rightarrow \beta \in \mathcal{P}$.
            - We then say ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/llIrrS3qeM3eTvoplfovEZCK3qfBdIr771yQuSOH3hHKEMc3Z1GkHk0_RWziKJuwZn-wpVbuUEmEoVpHx6F1nnSIX9tbo2-n3aOBgq_Y_hWuSVgjup8ukFC8iMgflfRe.png) , w can transition to v under this grammar.
            - The set of strings in a grammar is then the set of all strings derivable in a **finite number of derivation steps **from the starting symbol S. 
        - Chomsky suggested that phrase structure grammars can be grouped together by the properties of their production rules:
            - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/SF3CwGkdDC_3aBxaMxEvqaECU2OY4GFz-FrpZJj5s7oegFp95OntGg2t7zscFuY1srRfRoHonK4F-XY2RTjRjnL7DuJwG6Xw7Fcvlz3fNHccFBxPOJwgfpz68nAmCKcu.png) 
            - Context free ⇒ Still have a single terminal on the left, but can have a combination of terminals and non-terminals on the right.
            - Context sensitive ⇒ Can have more than one terminal on the left (there is a **context**). 
            - Recursively enumerable ⇒ anything goes.
        - How does a **class **of languages relate to grammars (e.g. regular languages)―A class of languages are the  languages that can be generated by a particular type of grammar.
        - The term **power **then means―The number of **subsets** of $\Sigma^*$ that a particular language can generate. 
        - Are Chomsky language classes **closed under union**?―Yes, if I take the union of two regular languages I will always get another regular language.
        - Are Chomsky language classes **closed under intersection**?―They are closed under **intersection with a regular language.** e.g. a context-free language intersected with a regular language yields another context-free language. 
Consider that we won't necessarily get all the values in the regular language, and a regular language may not be able to generate that lack of strings.
        - The **complexity **of a language class is―defined in terms of the recognition, how long would it take to recognise a string from that language:
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/2CGxDC-5ghW-KwGQG1kJWH7a6jM6D-YZPkWaOSpsJD_l3VGQ5g6y6CItoruCAaj57zTEscPjHnHvsT2hk6rDO_6HMnn-FHuS70xE8IyaK32SRsECqAlmmJQYflKwokoH.png)
Where  _recognition occurs if we end up in an accepting state_ . 
    - Can regular grammar model natural language? 
        - Short answer, no (in general). Would be nice, as we could form linear algorithms to read text.
        - **Centre embedding**
            - A centre embedding is―an infinitely recursive structure described by the rule $A \rightarrow \alpha A \beta$, which generates language examples of the form $a^nb^n$. 
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/UpUbxsTMQDbnOYtuQZPJQp1yoHtl3Z6aMI8lqXmJGWOgNZ6C2b1zB863yb_Y1CayTPe0r8En9ZKKlalKLQjTrqrXxNTKBOgaYwHw7KrFd2XcvbeysI47ndIQhp3f2kHD.png)
Could continue:
The students that the police that the informant helped arrested complained. 
            - The **pumping lemma **was used to prove **not **regular. 
            - Does the existence of centre embeddings prove the English language is irregular?―**No, **the  _complexity of a sub language within a language is not necessarily the same as the overall language_ . E.g. $a^nb^n$ is a subset of $a^*b^*$ - but the latter is regular! 
        - Describe the Pumping lemma  ↓ 
            - The pumping lemma builds off the fact, that if I have a string longer than the number of states in my machine **I must have gone in a loop!** 
            - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/N317Vy-_VFuQMHoQ3BzaDvThavpBOjiVI9NJF2Rln2_kPTi8ReeHMmE8nJG70NJgwOkLvKInFHcjmT64g5RSzKMFDPH0eeHhn4SLBkHUx4GiT_XVBPgSwy3L9YXvXqtS.png)
l is the number of states, v is the loop, and u1 & u2 are the pieces before and after the loop respectively.
            - $|v| > 1$ - the loop exists
            - $|u_1 v| \le l$ 
            - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/SM1Khqzpxbg7qVI6fwUrgtBpfDzRxDICbud4KnqLdachZic3Gtx1aJSuW8VQAKaAaE7litTBb9arS_vX_v410AyA7zLQIfgieCMqgtzxQxoTaUN1PLwVHkEH18xQGmuq.png)
If I go around the loop once, I must be able to go around it **any number of times.** 
        - How do you prove English is not regular?  ↓ 
            - First consider that a regular language intersected with another regular language is regular.
            - Next consider that regular languages are **closed under homomorphism**, we can map all nouns to a and all verbs to b and remain regular. 
            - This mapping gives us the language $\text{the } a\ ( \text{that the }a)^* \ b^*$ which we know is regular.
            - If we intersect that with English, we get centre embedding $a^nb^n$ (which isn't regular by the pumping lemma) - ($\text{the } a\ ( \text{that the }a)^{n-1} \ b^n$ ⇒ **Therefore, **our assumption that English was regular must be incorrect.
        - But for finite n we can still model English using a DFA (consider that for n=3 the processing load for us is pretty high).
        - Why not use a regular language to model English?  ↓ 
            - Context free vs regular:
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/lHingeygt1UyLLS09lO26kciLYotXwf_aw25knxGop-1qOfEW6yMFnLkR3oj9Ze1erHWC5yi_F_vSAxRbkSA0NHhhxCShM9TpJUFTRcb7gjOL42ohyRwLSXlJAhUmbF7.png) 
            - English not regular, can't model in general. 
            - Redundancy - need many many rules
            - Non-useful internal structures - the structures generated by regular grammars are not very general or representative of the language as a whole. We need informative internal structure to build up good semantic representations.

Context free vs regular:
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/lHingeygt1UyLLS09lO26kciLYotXwf_aw25knxGop-1qOfEW6yMFnLkR3oj9Ze1erHWC5yi_F_vSAxRbkSA0NHhhxCShM9TpJUFTRcb7gjOL42ohyRwLSXlJAhUmbF7.png) 
- **Lecture 2 **(PDA, phrase-structure grammars) 
    - Phrase Structure 
        - **Context free grammars capture ** _**phrase structure**_  
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/GcZGOKcQangEeK8KXL_-1HFldGiO8HLetSLiO1x82KD2kT1O0I6-efJOHerFZh5bmEFlrwMAnPGG5L7ssD41iUxl1zLRpndgeAwpqrU1pJ5QWW7f2lREXPgy-hANdPmP.png) 
        - NP = Noun phrases
PP = Preposition phrase
V = Verb
Adjective
        - The most influential word in a phrase is called the head. For example, flamingos is the most influential word in the NP non-terminal - pink is more information.  
        - The head of the entire string for context free grammars is always―The main verb.
        - With a context free grammar, we can change around any branches within the tree with the same name: E.g. croquet plays Pink flamingos with Alice
    - Chomsky Normal Form 
        - Every production rule has the form $$A \rightarrow BC \ \ \ \ \ \text{or} \ \ \ \ \ \ A \rightarrow a$$ 
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/KvnfwciChcYFXcEQJjQqngBi-8NnibfmsVj3Wnvtq67UR-gG0tJNNcHJ6kh8tpa3zqlnNKg2FVeWDc-9rg9sN6bduc-AQf8zEz_UHHJYnTOgEpaO67Bqq4M2-zq2Kg9w.png) 
Here A ⇒ BCD becomes A ⇒ BX AND X⇒CD
        - Why is Chomsky Normal Form used?―So we can write algorithms where our data structures are always of the same form. Convenient and can allow for more efficient algos.
    - Push Down Automata 
        - A finite state automata with some memory
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/6QFVsAI8e5gF3_xXYT-XOIfpRgWlbD36BqI_iVxQy6ZeEKBxPeKGeXwuU5jAt_8Qx2ePNG8acqvFtlEP9KDHh2AGwwuk-CUm-aCipL5Vfv8LUgjnawubIZ-NIT5o5DOe.png) 
        - When we move from one state to the next we may push or pop.
        - Explain these two diagrams:
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/afN-4q6dRIsPS4DyvR67hB3CzPOE7_v_Wo_Fc-B1z7UQgrd7JinzpsSylxbtKc4VEkBs1c1q3SDDdk-_rUDyNk6GbbKap_4GJ7DosyPUWz2s-eeezD7Qj2dfTc25QePP.png) ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/Bu0aCExBkru9iT6fU62_gIvuJgovVyzUubyHGjbtDYKfqH8cR0d81VTE8SCVYUd0Tzsp5QBkROl4WgO4RIe1Ni7Kiq7tCtLPbmTSac2EdXYFWVsZdbvAbBcejPq2hHtQ.png)―These are both is part of a diagrams for pushdown automata. 
Diagram 1. Here, if our machine is in state $q_x$ and it encounters an a on the tape. It will pop an A from it's stack and push a B onto it's stack.
Diagram 2. Simply saying push A onto the stack in state $q_x$. 
    - Can context-free grammars model natural language? 
        - What are cross serial dependencies?―Strings where dependencies are interleaved within a sentence:
We have wanted to let the children help Hans paint the house.
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/oYTiANb13Y4emLT5EezgIp587bVpy2krn_eKtXwetxqWOfkcEXM6oND3pnRr6ZpxeuqH-i13IuMYAliifa8cUVWxkN4erHRK9nT-yLUnJ9Fzo0-wvZPCozWXxg-an-vr.png) 
        - We can then map this construction to the form$$wa^nb^mxc^nd^my$$ 
        - Pumping Lemma Proof:

        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/hHwUdZyhku0KR7CFT9stgi-K9dpSFtvBwR4KgXYjthWb4xuqEsIQJiuVtwMPyB2jVNbgnPIZ-xF-t-XamGUQNgojQyYsszJO9R1aqlV2g1PPZO-sPUFnuvGCBKH7ty1w.png) 
    - Mildly Context-sensitive grammars 
        - More expressive than context-free languages, less expressive than context-sensitive languages. Between the two in the Chomsky hierarchy.
        - Have the following properties 
            - Includes all the context-free languages
            - Members of the languages in this class must be recognisable in polynomial time
            - Must allow for all constructions in natural languages - i.e. can represent cross-serial dependencies. 
    - Tree adjoining Grammar 
        - Trees rewritten as other trees, rather than symbols rewritten as other symbols.
        - $\mathcal{I}$ - is the set of **Initial trees **(also known as $\alpha$-trees).  
        - $\mathcal{A}$ - is the set of **auxiliary trees **(also known as $\beta$-trees). One leaf of a $\beta$-tree is distinguished as the foot and will be the same non-terminal as the root. (Often indicated with an Asterisk). 
        - Describe substitution and ... in Tree adjoining grammars ↓ 
            - **Substitution: **Replace a non-terminal leaf with an $\alpha$-tree with A at it's root:
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/-dljmU_Ztvn6ppm3Hh37icRm6rJX4DxA86E0OWrAgTjS_MOu2Lm_nM8hAwoSf7wuS1pnyKq0WAsBBzNw79O_v7d7gNjnL1sbvnxWRLxZZcVFtcr0ujOJdaVlah-kuu6f.png) 
            - **Adjunction: **In 
    - 
- **Lecture 3** (Parsing)
    - Shift-reduce parsers 
        - This is in the context of **deterministic languages **where no string has more than one tree i.e. the grammar is **unambiguous**. 
        - Initially the input string is held in the buffer, and the stack is empty. Symbols are shifted from the buffer to the stack.
        - When the top items in the stack, match the RHS of a rule in our grammar. They can be reduced - e.g. replaced with the LHS of that rule.
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/-UIUX0aiu53AuyE33ZDUaQCbZaAz-5anOZABpyg2TYCv01cO6fkZAZBqGb76EjForqrqFCqyUH_FnKTFv1DB40SB7GPOs_hB0RPHDxEdw1iQDNICNp_rn0ZyV-87KIte.png) 
    - Deterministic context-free languages 
        - Proper subset of the context-free languages
        - Are accepted by deterministic push-down automata
        - Can be modelled by an unambiguous grammar
        - Can be parsed in linear time
        - Parser can automatically be generated from the grammar
        - Note that Natural languages generally don't work with shift-reduce parsers, as we can have multiple choices of decisions to make.
We could choose the path using a machine-learning classifier.
    - Ambiguous grammar results 
        - The number of binary trees is proportional to the Catalan number, 
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/sdbsPL_5oRAxIvi7_7fZfpaBVJlMrZObv_Of-9opP1tawl1OEVKc_d-Lu8un0PGDudxc8F76xHqYJsn38_QI2WpUNZn3kaH2IhRdZn2CUEcG1vkOagqWEDUwRujIP9YP.png)
 I.e. for a sentence with 12 words in - the number of possible trees to go down is 58786.
        - What are parse forests and chart parses?―A collection of parses is a parse forest. A chart forest is a pars method with hash table lookback.
    - The Earley parser 
        - This is a dynamic programming algorithm that records partial derivations in a CHART.
        - What is a dotted rule?―![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/00JiGkJoes5-WpA3Q5RFIbbdXiaozpvSi_g7kpttbTxLsbpjrCT8pU3G-wY4vH4mNgMnt8Uv7Y6D8dBedg8GIp8LKdUrQJLzK6PfA9e6vEOVMwsC8da0mJURKMWlQYLp.png) 
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/ncgfIkddYNdf889mTyqYWxiy-SmLTQD949v99KsYLn-6WGR6SecDKBZ_hr4IiTXMNtodWW6QR-5TnzSLKxVoHUUqn6cn1jeX_he9du0ApbK_Ai47GKM2ESKeok2Wb0BC.png) 
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/8X26iP4vERb91q2RUexqNYB-uENUbc_9swXNraIZrCVTbymR9xR0HWaTVX90w8mIgI2iI-2pwxOMUWOt9TXhv1U8igvbFKAuAvrI4dl-y-jutQOSYf8-BCRJl9v_1UMw.png) 
        - What are the 3 steps of the Earley parser?―Predict, scan and complete.
        - What are the privileged set of non-terminals―In the predict step, for something like $NP \rightarrow \bullet N$ - we would have to add **every noun **in the language to the chart during the predict step. We don't want to do this, for size & time.
So we privilege a set of non-terminals and perform a look-up of the next $a_j$ to see whether it will be consistent. 
        - Describe the predict step ↓ 
            - We look for the non terminals just to the right of the dot, and look at all the rules which generate those non terminals and add them to our chart
            - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/4iYyiix0VUGap_OWCrJRCZuOvx3axuohmaK8r3g7hvKvLEUBWgtUONszYrH2fpVlQW20qxfjIHp4icDW7Q_9SOM0oCFpMe1F_7v_claz3qPEhdfTuIKVCKvfG9YlLCe8.png) 
            - In rule induction notation:![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/oggF-KwcYsI8bHlROprsy-PlQYdKIHJyfJwz3LG5Sf7CzBymSbfEJyZUet09LhAah3eKpyX2GYhBnGnSmE_FzQyoLRwykVpvzXfsa_cBLTLQ2eOTNl88O-mTv4IyHNPs.png)
The lower half say's we're **starting at j**, and **waiting at j**. 
            - Note that here we're privileging $\{N,V,P\}$ 
        - Describe the scan step ↓ 
            - This step allows us to check if we have a node that is consistent with the input sentence. I.e. they is derived from one of our rules.
            - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/icZW7i9FxieJ4l1pDIBpHQQ_k0Bzy1HDD6SUW5G_PlC5-MD9nUd-C-8QZ1fujfp3mzjGSEpnToTExZoAq9KSd0Nnln2lMAybIDic5OsmHWhThr_YqF4H0YT0x3lel47a.png) 
            - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/l5rVp5O7T3A3tOzaPTwjrvhAeEpjNxeCr_SFYrpHknKPwRXNu1VMQ0NsfL6IPS_S20NVUHWDCoykoAq38-RqLCN-pVS327j7tX60YhIAQo439LUwsyqUsFmxeN9SR_hO.png)
In rule induction notation, we've seen $a_j == a$ - so we say we started at i and we're **waiting at j now **(we advanced where we were waiting at by one). 
        - Describe the complete step ↓ 
            - We take all the rules generated in the scan step (e.g. N⇒they) and test them against all our rules from the scan step, seeing if we can advance anything. Then if we complete any more derivations, we need to test **those **against words made in the scan step too.
Moves dots along, signifies exploring nodes.
            - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/YOzJi9vVF5qaHubirzPkB_qI27LzMzs58FFFYCxvAZXHMueyEmX1TIg4srmtpVVIDRC6QveyKODCYa1DBmjzK230iPili4RZlQy8FQo-oZo7Dxuz4UsYmW_7HnUn1HDK.png) 
            - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/yhiJ2_hSBwPqSjDwEV02GnHcumjzi9lrhwdJgI9sKC_y9LDL84MeUXGQB-pWZQBcRIbcHu_mdmABX_IP6--J8zhPcoI-W_RdRyK-LU4hIoDdH5gG1lGaf_J3rF1SryLM.png)
On the left, if we have alpha and are waiting for B. On the right, if we've found B.
Then we can move past B. 
        - Example:
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/nYN0UDYlqepYX46jUR-6m0rnNBndg4a1J9G8wQKRGCRihhJ7Vk-CgHuFcXlkY6oKqq_Ca8OWYYKamJK8UcsgZ7IBPM7ZaCRMMEvAsNtZCjowsha8rcBwEk1cMG8X6fp-.png) 
        - What is the time complexity of this algorithm, and why mate?―It's $O(n^3),$ because the complete step dominates with $O(n^2)$ runtime and you do that step $\propto n$ times.
    - Complexity for humans 
        - Complexity can refer to: 
            - The time and space requirements for your brain to process the sentence.
            - The information theoretic content of the sentence itself in  _isolation from the human processor_  
        - Traditional work in this area gave humans problems, and observed if the difficulty of parsing a given sentence correlated with a parsing algorithms difficulty.
        - There are two assumptions in this work
            1. The longer a sentence takes to process, the more complex it is
            2. A complicated sentence will not occur frequently in the spoken language.
        - The horse raced past the barn 
The horse raced past the barn fell
Sentence two, while still correct, takes much longer to parse.
    - The Human Parser 
        - What is surprisal?―Negative log conditional probability, the more probable the less surprising.$$\log \frac{1}{P(w_i | w_1..._{i-1})}$$ 
        - The suggestion is that we can model humans using a probabilistic context-free grammar. 
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/DO5NXwDGecVA0SnciDIBMBlxSIzw-af39PNpaJ8DrzusXVjvEgAFmmO6mwbtbqzvOKjXdjCfTQq-9QlsHJByRPD_Am8weahvqCVq9kpg2q1PNTAF_OX5RahB-64fQ_dK.png) 
        - This model differs from the Earley parser, where ALL PARSES are found. Instead we follow the **most likely **path, and then **double back **if we're found to be wrong.
        - What does this sentence mean:
The hypothesis is that the cognitive effort to parse a given prefix, is proportional to the total probability of all other analyses not compatible with that prefix.―Means that more surprising sentences take longer to parse, because we try loads of other parsing trees before finding the correct one.
        - The probability of a derivation is―the product of the probability of all rules used to make that derivation. 
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/3c_9GqzCaHqqQMRMOcWvSozo2wwQZPwt86jpzli1zyUsDh9BDWGKUpV19vIwwgItY1MIRvFC-wDsbAkIksxXepr6PsORR-IXM_cio2AwNslpoJlFrcfXrE0G-RvH2a8r.png) 
    - PDA as a model of sentence processing (Yngve)
        - **Hypothesis**: The larger the stack, the higher the working memory load.
        - **Prediction**: More complex ⇒ Difficult to process ⇒ Less probable. So sentences with larger stacks will be less frequent.
        - **Prediction**: When multiple parses are possible, we should prefer the one with the minimised stack.
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/GdNNnYwASAVVy0eqrSR6iH9oAthE5Plm78ChrwYTuK8KqAq55eLj_-pgJEBtPwvGP5YkJNBwvCkdJNx9qTFokSTOu9rU5SXCLsKZ11krXRba8o40W4VHSp4SNZm_E2yT.png) 
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/mOdkW0JqMCszthAGOzC6yIFgon9KMziFAf6ftdMfOuay-RD0GP1aSv3G64KLMEl6ompIAl60LiMHvgTuy9saXJpj1l7Lpr5MtBuxpHM4ua5IOXOMEq9qBO17yMnhXH-i.png)
The max stack size here is 2, meaning a sentence with a larger max stack is considered harder to parse. 
        - Great example 
            - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/a3csr3NwS-hJcUvsr5YDQvjUZdj7dFKTBMDYLdwzxzNv1un_5Z_cbDISHoI28-T3GtjBrET9iKWAsy2jmwCebSJ_HtCextJ2YZEwjlTSzaVUenx2KZh6MiUmupLojeoG.png)
Nouns and verbs resolved locally in sentence 2. 
    - 
- **Lecture 4 **(Dependency Trees)
    - Dependency Trees 
        - A dependency tree is a―directed graph representation of a tree - where every edge represents a grammatical relationship between the symbols:
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/fnOjSdkXDa9DMXTVIrZs2kxa1HnckgaywjI8mQRrqjJ-fA_wU8Ter2f0Dwz6IVRUcsJy5M_pNJPIeYf8s0_l6EERJEwwnuw2j3iyssYstOx6yIZZhRe8jX1xCNtZWr69.png) 
        - A dependency grammar derives dependency trees:
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/vJdyEly6JKk42TJRyO_frnEY4JJOMYbH_RT4YEW3_j8HYD-dMko0mXh8l8Fg2woDCuFTjqeonjfYzdCTs09UmoAVXAj-KLYWfd9pCEnQj6QVTjafbbRlAQkvsBg_wSZB.png) 
        - Using these rules, how would you test if the string bacdfe is in the language:![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/IVlhvDPmDEl7OW-CHPMGS1yDSJaELVRHpmLIKTFvdVqYK4tPcRZfJpiDtxELYS0PEy5x5-3c3B8poIcB_v1nIpN7JPlrqMMjxe4Yip9N-f24VZX_iUprVEuS-bxDLk60.png)―![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/ZbWPB-3DuqkykqzgC_oW4cpuZZbuxAQO_rggGjzW2f51e7dZ-xPlgGt6tLTf_1VILYGTxq2zRkVxIed6G8kZwyRDZq9FQP3nPW7cvlmreNAMtp_okX6FrtAR43w-HGbt.png) I.e. use the rules one by one until something doesn't work, or you're done.
        - Note that by the above grammar, we could also generate badfec. The fact that this grammar can handle differing word order means it could be useful for modelling natural language with flexible word order.
        - A valid derivation is one that is rooted in s and weakly connected. What does this mean?―Means it starts in the start state, and you can reach any node from any other node (if you ignore direction). 
It's a bloody tree, of course it's weakly connected...
        - Trees can be projective or non-projective, what do these terms mean?―Non-projective means there's no way to draw the arrows without at least one crossing another
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/L4tbDyALjccvtgVQ1-z2Fk3wodEPrU4bvm0oVxbgN87bjw5Wa9HTqacsSJwY9ruxaPdNm08IMhPpRTMRSQQH-poJvBDwFDUCLMclxOEgqSrbtcGtJFu-wh130BoYNzxB.png)
 This has implications for parsing complexity.
        - Labels 
            - You can also add labels to dependency edges
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/88dQ2eXiq3fq9RAiluZmr68D5mAes3lUXqWGVp0C9AHm2xPiU6W6_ZZWVAOOt9OA7gUTU92CLjQhuKsIZffClIc3pD9TVPG9F4cFBvDAnIBgCNyKiDTeh9VAos_XZexb.png) 
            - Where the labels are r in:
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/V--9y6L5vgEsdBojdwa6NQQFwCj52V2cbusaGFhrhiqPLZQ-_Yf4Hn5kX9OrMHdC9dvu1YYn4gg6OwYXCf6VaVaCslztbE6vySaLMtxquBAEeAm8Bb4u-aHmNzAQm7tu.png) 
            - In the sentence "Alice plays croquet with pink flamingos", give a **subject, object **and **modifier** ↓ 
                - Subject: Alice 
                - Object: Croquet / with
                - Modifier: Pink
    - Dependency Grammar equivalency 
        - **Projective **dependency grammars can be shown to be **weakly equivalent **to context-free grammars. 
        - We can convert context free grammar trees to a dependency grammar tree
            - Start with the CFG
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/1UO9Kkh2FyDnzt5NPzeTbmwT6kH697N6LezokJPqqPcQWSMgAnFm6Z8bW8CmENedfBWnPkzfGFHzgE1vWxP6F0M4ow3oK7RcYkmYq1R2vbmk4AD_8ZKk6Jhx750vgrpZ.png) 
            - Find all the lexical heads:
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/bjnLWm8nnG0rxodTRYb0AZ2ZDaA179WG8C0Mou8C_XR8bRwQ4TUIn0K6G1ls7fs4f3XYExbe5WHJsgj2cVKkchlbS-w8lbWg0AMkSCEXAMVwRJdMrj3q44Vc6mdCyoDo.png) 
            - Then note the **highest **position of any lexical head: 
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/KccGk17DvxzmZ-XxGOcZUZVBtUvdN5uPNozCo_Y7Vj_jbww6DENQhqLFso9jqI7ywt1R6uDyLCBhNOn-kJKlyHnQTwRpLLqgWX1ZAEssJS6nR3nPfSJQpINReSpA6u9W.png) 
            - Then remove all parts of the tree that don't link one of the boxes:
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/jr86z7mA4oTfuU2rTJ4_abB18lR_bwQfu3AxjTUFRF_BfOetFc77xH0VCrCkePh0MIa7GV2a3z41R_0ke8PQPhQLnpEXixSVXOvttDs9p7TGprvxgDqtynTCs-rcrHpV.png) 
            - Finally, merge together all nodes with the same lexical head:
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/6OuZx2NnwjllIu1Q4FzOICEbFy-tiyVNMFh4gPLC9P4sGbLZhYipEXGNtpywOYgnJUeWHiAga_pzG-JElrLl1Gqk-jUiY1KNojVQ8A4l6MsjzyiN-4rYX58fBt5EQR6l.png) 
            - Resulting in:
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/dgxFCWR5ABlSJzBwESM0iDDZAz-ERNhfBwCjx7hvaJd8uxhudqk2xmUcyOah0fnchdLjPY4B83ZM1QH5EFUU2PllhBinI5Un3DG488Rg0wOhnQeJFb28U56kWmYk46Ke.png)
Which is a dependency tree. 
    - Dependency Parsing 
        - Use a modified shift-reduce parser, instead of a reduce we have―a left arc and a right arc operator which reduce the top two stack symbols leaving the head on the stack.
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/bb9ysqDZy3uYwUw87ChWsN7BcSf-EqNgxFw_9TgBgJy-AF1vOhFfteBWVwjJ3TwLbFKeg3GZ65NdEF258_XaYxok7OUXM6Qp0MUKgmhFC5LWqHKoivR_U80r0X5DrAOJ.png) 
        - Parse bacdfe using a modified shift-reduce parser and the following rules: ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/EMFn6bTnGRiFshBJ11SJecbTWJ7X9jR2deeZCGZQl15O6GDemIKeD38bw1kSCgMEdJMaDBOY0wl_H2kKoFHRaatnckANG9yLbYqNmQwAhfB8KBO_R3CZlNPxsGimplY-.png)―![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/foIkUOVFVHEpnzaKOO0uuIi4_c7pYJSlPVhVgBFUbONCxRhXoEQS7MVQbiyEYgukQgXpcyfrM5zpELzCh4nF6UGIN3Ifn9yF_V1IXYmg02Q7q1W5CMHKgRbwl0fDYmRN.png)
Note that for a deterministic parse, a lookahead would be needed. Because we could use the d right rule in step 6, but that would lead to an incorrect parse. 
    - Data driven dependency parsing 
        - For natural language, defining P would be impossible - i.e. defining all dependencies between all words in the lang.
        - Creating a det grammar would be impossible (nat lang ambiguous)
        - Dependency parsing can be done deterministically by {{choosing the parsers actions}} using a {{machine learning classifier}}.
        - Training is performed on **dependency banks **that is―sentences that have been manually annotated with their correct dependencies. 
        - This parsing is grammarless, no grammar is designed ahead of training. 
        - The classifier can return a **probability **of an action. To avoid the problem of early incorrect resolution of an ambiguous parse―Multiple competing parses can be recorded, and a beam search (expand most promising one) used to keep track of the best alternative parses. 
    - Spoken Language
        - We can mark speech features as features for the dependency parser. Prosody, stress, intonation:
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/gqDlDXz6E1-QxRIjhpJXeqqlmA2UW086zkXM3puktIBVCq_OiKkbF9bE5rmOa_4HwSUSGsoIeyxcaliKKnjp_CHBjHj5mv-RAGFbj5LTfND8fBoxQ3ThPzNh0Syr6lNl.png) 
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/1y7vjOVls7Wfpo2zi2QSoIg4vtvtJtFZyfse4pi_rZPPSRoCXO_jhxMjaiBdP0rml53VNiqO4uXG1kczMBjVbT1_sTwdJ0rAsozEST4BGVUSlNfYzL739ttOY0iHwssv.png) 
        - A fundamental issue affecting syntactic parsing of spoken language, is the lack of the sentence unit. No full stops or capital letters. Instead these can be identified by pauses, intonation and change of speaker.
However:
Speaker-overlap, ellipsis, hesitation and false starts can wreck this.
        - Speech units often contain words and grammatical constructions that do not appear in written language.
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/QyK83YNOLGQqajyPHixoLJUXQMazBtO-PsZDzbnO4omsvOsgsqNWl8Jtv9UyqC6DiSFfLQKGkXyhqdoMzksI9OG7mzzLdd3FPCq0i3Z6kTKqZPbUw8gDxIVkEEuM-de_.png) 
        - What are disfluency, and what is added to a dependency parser to handle them?―Disfluencies are pauses in the natural flow of speech. To handle them we add an edit action, which discards all the connections to the current head and scrolls the stack back to the previous head:
In "his company went broke I-mean went bankrupt" - I-mean marks a place where we need to redo.
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/AGDkHjN8U-b8kNXF6XNZdvMyTi8ow4R9kqAR9kbC6Py96HVZcoHM6355u4YLGTSpakfazzGCel6Dsf8BlqHXGIRx0L1Za5lfE9hfvoROSUYPF_LEYsR4AWuDgFJdeG1m.png)
Here,  
    - 
- **Lecture 5 ** (Grammar induction)
    - Inferring Grammars 
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/D-ADiiOVKdNwDb4-Spfs9OqjsJ20r1uZRAiWEi3M2ZtlzpzWMGkG2YGuSdPdjt8ZZFE7gN_VT5JUL7HZODkClPZ1ReiZJXwJR1XvTadR9wG4K3Qb1l_6ogfiEtX6NFLw.png) 
        - Describe Byte-Pair encoding ↓ 
            - Count the most frequent pairs 
            - Convert those pairs into a non-terminal
            - Repeat until all pairs appear at most once
        - Byte pair encoding has benefits for encryptions and compression. However, it has multiple issues.
            - It's frequency driven, this is a problem because―it might not lead to appropriate constituency. E.g. for "the cat ate (a mouse)" we can replace a mouse. But (ate a) will be replaced, because a cat is eating **something **more often than it's eating a mouse. 
            - If two pairs have the same frequency, we make an arbitrary choice of which to reduce.
            - The data is assumed to be non-noisy.
            - The algorithm learns from strings alone, no extra-linguistic information.
            - Two improvements could be―ternary branching (HHH ⇒ K) or adding extra information to break ties.  
    - Learning paradigms  
        - Grammatical systems 
            - What is a grammatical system comprised of?―![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/1OIIf_Bd5qetlxuM1D2zSXUyp_UQvVmLWHajy9983kCOXq2MFhgESHfKiuHAM8639XTpIjDEZoJ2Jww7iezcjJu_GTLCX596E-74MIz4wdzySiT0YNY0tFgAPgHw1hUE.png) 
            - What is a learning function?―A function maps a subset of $\Omega$ to a member of $H$. E.g. makes a guess at the grammar given some strings. 
            - Learnability is a property of a language class when F is surjective.
        - What are learning paradigms?―Learning paradigms specify the type of input given to a learner.
        - Negative evidence is―the learner receives strings not in the target language, and they are flagged as such.
        - Exhaustive evidence is―The learner receives every relevant sample from the sample space.
        - Non-string evidence is―non-string samples. E.g. verbal cues.
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/bSBZxehFbaKDhioDagcj253oc0ZP-s4UhM0zaLtEgImXNC3Dj3ApvhWnqIaTIyyLg15JQoDG3VbO-FSNBExCD0iSNGgsq3luHzWtVHUH3rmsOPPbCSp8rTF_4t-MTT4q.png) 
        - Positive evidence is―the learner receives only valid examples from the sample space.
    - Gold's learning paradigms 
        - Describe Gold's learning paradigm ↓ 
            - Learning is an infinite process, where the learner receives an infinite stream of strings of the target language. 
            - Positive evidence is presented one sample at a time. 
            - After each sample, the learner produces a hypothesis. Outputs some grammar $G_n$ after seeing n samples. 
            - Every string in the language will be presented 
        - What is identification in the limit―Gold described that at some point the hypothesis language will not change. In the limit.
        - The class of **suprafinite **languages are {{**not **}}learnable. Those are languages that contain all {{finite languages}} and at least one {{infinite language}}. This means context free languages are {{not learnable}}.
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/7tRCV5ng3niE9ACtHVXefmUN0tkcoYGDTwX3oVU-VC2gK8HgbaL7I1vHB5xDZH3FascQvk7Dg1GEP3txN_4Z9JN72Mxn7Q5s2r_Qu2pTMVY7-45q_fVTZ9V6HC9WsRuD.png) 
    - Categorial grammars 
        - Describe classical categorial grammar―![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/8-cPmC_AEsqdcAmf8660Yp0qPyhsBTUlr9FYvI8SkQ8zoLSmdK1vt84-j5a4b0VaWrmhpbTZzDoslamHNEuzHuQlILU5XWLTWj07XGiXqrr8DvOVej6AIGgYWCR92PRq.png) 
        - What is a rigid grammar and what is a k-valued grammar ↓ 
            - A categorial grammar that associates one type to each symbol in the alphabet.
            - A k-valued grammar has that types are associated with at most k types.
        - In a classical categorial grammar when is a type $A$ a subtype of a type $B$  ↓ 
            - If $A==B$ 
            - Or, $B = B1 \backslash B2 \ \ \lor \ \ B=B1/B2 \ \ \text{ AND A is a subtype of B1 or B2}$ 
        - What are forward applications and backwards applications in Categorial grammars? ↓ 
            - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/3QE3ShdsaLRw0DLEgTWfnTkVF9VOCNIhpCVqqYYWQU5UNXPpWc5B7625r0PRHssKZYRA0CK7zBX9KF8xdJ-eQJClJWSzBYIW24s5W-JlrQYzUk6DTKNjL0JEdQAUGJNw.png)
I will be an A, once I have a B on my right hand side. FORWARD SLASH
            - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/tM7JMj7CW794RFsuCPKwHq7Jb4FvYEzT8q5qr9Ix7bGDgkvQJySLvjwU9bOYTpJy9te7EHwl_4jfLLwmjpoh9ObggqDzDlyCmLKSFuEr9vj09CtvID0N7gKqOIsDC9B9.png)
I will be an A, once I have a B on my left hand side. BACK SLASH 
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/jqufYc8Ql8qqA-gYUs5_2PyKvoBx8LzdHvsRGl-Uop1EnH629taN04PS7oXzoF49fheTJ0KxJBByLfq2A-33B--1Lg0zWQ7ljbbCEBYDdo_tQHzXk7wSJNwPM5Nm3I8n.png) 
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/MBYqWN0LZwhCdhykYtZVxEJ96jhpP2X0KJMVmD0khQTCC1iuOOi02lD3G1Bo_wXMaLqzBU3lZCnk4zC5mgTOLpu__hkWIV_0gw3LlR-paOd59B_4jpDzmKsY1AspKpRa.png) 
    - Constructing equivalence between CFG's and categorial grammars 
        - Note that for the tree formed from a categorial grammar e.g.:
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/x3MAcjOKGIsRXbRpp3qtQTg1kq_aE0ObfoY0jGjlt2np0NDBE8ktPsvCmt5u_HQEpW-bDOS3LrwMkwcJ52LHqAry4LKPT9gpRBS03jdZN0QbksNKz9vDyCHA34Dld2lU.png)
The left child will be the argument to the right in a backwards application, and vice versa for a forwards app. 
        - So to form an equivalent grammar to a CFG we create the following rules  ↓ 
            - $\{ A \rightarrow B \ \ A \backslash B   \ \ | \ \ A \backslash B \in T_p  \}$
A is formed by combining B and A\B 
            - $\{ A \rightarrow  A / B  \ \ B \ \ | \ \ A / B \in T_p  \}$ 
A is formed by combining A/B and B
            - $\{ A\rightarrow a \ \ | \ \ \mathcal{R }:a\rightarrow A \}$ 
The non terminals just go to a.
    - A categorial grammar learner within Gold's paradigm 
        - Given simply **functor-argument structures **e.g. we just have ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/fUjtZ20uP9dr6r2A5RGNOorORkc9FCjytFZzuwemrmx3sqOxirLxb9tF_HJTIiS8uRlRi5uZ7Yr7lTjfUUx3dVzVqC4rcxQbmsy8yrMnxzJFhy1dOk8C0yix20Lh8GVg.png) and all we know is that a {{forwards application}} took place - there is an algorithm that could learn the class of {{rigid grammars}} from an infinite stream of {{functor-argument structures}}. Satisfying Gold's paradigm.
        - How does learning the categorial grammar in Gold's paradigm work? Given an initial hypothesis ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/QCHldSmmvncPq0H5jTBKOvf-9ldCeoXXyDga4XUNyM2sJqFuRQnWPqNumo8GfCm4PabK5TJluJ6qsNeqUKInu1T1ufaBxblNPoXLLYUAa6UHSNsuara-z4DIjIcHA6tq.png) 
and a functor-argument stream like
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/qtU3n9gXq8awHSEjlNuQ4QKTofwL33CXKdNnJzb-BULF_SoFt2-l0YvwqA2Cwq_wOp3eA7L-VrYg-izHkXInDINvdMEDcfSwlQsp1Y3ANL4wJouNMVkOCbLft6jUrONB.png)  ↓ 
            - We can infer from the functor-argument stream that:
alice has type $x_2$ 
quickly has type $\langle s \backslash x_2  \rangle\backslash x_3$ 
grows has type $x_3$ 
            - We then unify that with our prior knowledge (rigid grammar, so has to unify)
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/hFjj9DabtstmhlWDclpJdrQuz25wFvtMyBhds_2mI9VW3SDUwp_iBuprEgYjCBXL8fJMHJytpW7V9WqHFrqXFGeG_NwgFi6v2GuhPxh3ADGGVdJ_m5VKeYW604f13rlE.png) 
            - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/vTRh-K-dDwPu5VU0TnmPR2Mijt04roV8qYyDZfw1TNnmc40WTlODPFd-0FVWidqFqTGZvsh6UeNzU-5hwgV_CJHadarN71MEqje1NUOfxqUJsAsjIn6cc-e5idhOYFbu.png) 
    - Golds paradigm not like human acquisition 
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/AYvd8FN1ElKG0dl2iBATQwjM7y_0J4tTjcmnixjWLi3Gjb82IXjTo2Lxuaa6bpBs0TQTdDaeb-U8jYNakMxaqz6HzdBUo9zrX4vstclgrpHK-JE4sUyWkFnlwyPLYU4d.png) 
        - Humans are not required to exactly identify the target language.
        - Gold's learner hypothesis a grammar at every step, but children only attend to selective evidence - the parts that are helpful to them (goldilocks effect).
Dunno WTF this means.
    - Extended Categorial grammars 
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/NnHSUZ-WaPVb-JFXeZUaYd5l47oIKbJRAvN_kNvCgerg6vWT42bLUqYOUxGyps3XacxPEmv2JjH-C-keSSCjxnSNDrZJvUF7cVroaU9PcRYIUtWIbdMsn2CztVxuRxYD.png) 
- **Lecture 6** (Entropy and surprisal)  
    - It's easier to guess the meaning of a sentence with vowels removed than consonants. 
    - It can be difficult to spot typos in sentences. Why is that? Would it be easier to spot other missing words
    - How can we tell if a made up word will seem real?
    - When do we insert **that**? E.g. Glad  __that__  it was over VS glad it was over. 
    - Entropy is a measure of information 
        - Information sources produce information as {{events }}or {{messages}}. We represent our information sources as X, producing discrete information over a dataset.
        - What is Entropy? Give a couple of ways of thinking about it  ↓ 
            - The average information produced by a source.
            - The average amount of information gained by a message.
            - The average amount of information we lack before receiving a message.
            - The average uncertainty of a random variable.
            - The average amount of uncertainty we have in a message we're about to receive
        - Entropy, $H$, is measured in **bits**. If X has M equally likely events, what is $H(X)$?―$$H(X) = log_2(M)$$
E.g. X can be represented in log_2(M) bits of information.
Note that this is a lower limit.
    - Surprisal  
        - Describe surprisal's formula―$$-\log_2 (p(x))$$
Where p(x) is the probability of a given string in our alphabet. 
The more probable the word, the less surprised we are to see it.
        - We can represent entropy produce by X, as the average of how surprising each word is: $$H(X) = -\sum_{x \in \chi} p(x)\log_2(p(x))$$ 
        - If all items are equally likely, we just get log2(M)
        - When examining corpuses, we find that vowels are less surprising than consonants. Meaning consonants encode more information, and thus removing them makes the sentence harder to read.
        - If English is efficiently encoded, then the shorter words should be less surprising (we use them more) while longer words should be more surprising. 
        - Does length of words correlate more with frequency (number of times it's used) or information content?―Information content.
        - What is **Joint Entropy**?―The amount of information needed to specify two discrete random variables 
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/b9UoPsw3x49pZb7kus3TxZY4el9ta4qpX1vlcIRVEp5bj4OB__mNT9nzddjogDnvBi_oAK53TEqWuWG92pqg6jZ7ToBLQ-04TMXYO8x1ufLfaKlXYf20U028ztnTs6_t.png) 
        - What is **Conditional Entropy**?―The amount of information needed to calculate x, given y is already known. 
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/6cXv5xo7LZ43d5_hnbuQA3rom1OP9-MRk-jUgPQDSqiLkMrp8kGNo5_wQlQ3GPn5ao-TF9-3WJPkPuTXciqZ9imVr4D6-1Y8G5l-nevj5SOds4LOy-yJ1bINlaCvQLoP.png) 
        - Chain rule:
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/-oYHGsjYK5kLGxmtRwetXVf7wXWp2uKYv4VJ5-AeSS-1_cRmpuWWnlJ_3eE3h9t5IZ3sL3CWY5BtvCOAu5F_P5WlI5KOipxGOmAINPk8KwLinlktDgeOjr-3HBr7RCNL.png) 
        - The information in the transitions of Bandersnatch is (e.g. Ba, an, nd etc.) much lower than the overall word. What does that tell us?―It explains why the word seems like plausible English - as even if the entire word is surprising, it's components aren't. 
    - Entropy Rate 
        - What is the **entropy rate **of a language?―If we model language as a stochastic process generating random words, then we can measure the entropy at the limit $$H_{rate}(L)= \lim_{n \rightarrow \infin} \frac{1}{n} H(X_1,...,X_n)$$
This gives the entropy rate - the limit of the entropy of a random sample of the language. 
        - The capacity of a channel is―the number of bits on average it can transmit
        - We assume language users want to {{maximize }}information transmission, while making it easy for the "receiver" of information.
They do this by keeping the information rate {{equal to }}the {{channel capacity}}. This explains the use of words like {{"that"}} which slow down the information rate in an otherwise information dense sentence.
    - Smooth signal redundancy Hypothesis 
        - It was found that there's an inverse correlation between **redundancy **and **length **of words. 
    - 
- **Lecture 7 **(Noisy channels)
    - In communication, Information needs to be transmitted 
        - This leads to a trade-off between compression (removing redundancy) and accuracy (allow information to be recovered) in the presence of a noisy channel.
    - Modelling transmission using a noisy channel 
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/tq1FLdbDpRXBSam3HjA_O8dJoakwUiWgXYaEVx36YpVCNfRmo--IWTw8H6AyXa8phzXoxEKT1gez6xsk4vTfqJu4AxuEErqVAKIgzdgpDf_HK5cDHVB5WIpqeEe-kZpr.png) 
        - Output of channel depends probabilistically on the input. Decoder finds most likely sentence given Y.
    - Mutual Information 
        - What is **Mutual Information**?―The measure in the reduction of uncertainty in one random variable, based on information gained about another. 
$$I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$$
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/vje52Ja56Y7TmsOOmkJk8wUF-he8dXmglDFAtJVYwr7AsLFZUevtwhhGuseoHUQZbll8xXWO_XmaVzLS3w5dHf9o-iZckFGt1npg4sMpt24ukknKSTInfL5a5tXxKdX7.png)
The information in Y that tells us about X. 
        - How does channel capacity relate to mutual information?―The channel capacity is the maximum of the mutual information of X and Y over all input distributions of the input $p(X)$: $$C = \max_{p(X)} I(X;Y)$$
I.e. the higher the mutual information, the better the channel. 
    - Ambiguity has communicative benefit 
        - Ambiguity is not a problem in normal language - Alice wanted to cry, Alice saw two rabbits etc. 
        - Only ambiguous out of context. In order to make best use of our channel, we overload the small words for efficient transmission. In general, **shorter words have more meanings**. 
    - Noisy channel can account for word order 
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/C9L1zWf2hLtZ9MaN_6abr0BUHbvqNLcR9kKPeHPr9AnaOu_i4Zc4826HWdpv1ombL2kyKi5MssEqz5eQGzy6pRrLC0gp8JeOxP7LtjJPVg3Kn5fg7z8UVStYbHzT0XMF.png) 
        - In an experiment where characters and objects were the same, but the objects were changed: 
They found that people act out Subject-Object-Verb for human on object interaction. 
On the other hand, people act out Subject-Verb-Object for human on human interaction. 
        - Explains SOV because the subject and the object were old information, while the verb was new information. 
        - Subject-Verb-Objects preserves information better, even if information is lost we know who is kicking and who is being kicked:
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/91JxKNqeLKUnfskHWgeM-D3Hui7ThF7BscBksn6xZlLxLtixCivrNUNx3NDdq0c2Za1XXzfwbld8kvF2Wb84F3_VUwV_U-NJRG7Q8egmcazgCPh0l9odYEwXZTssoGg1.png) 
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/vH_TSFxL_gvnwF_GN-fH75Q3vv0T3WyuxSomJjugV-GdUidvda8mo-O9FDTfvb7Zz_RXONga6Uvl9IMSCHV0IMcSGiP77x1jShgMN7cjWcotKTeAI1q0Z8mkbmwxWu6I.png)  
    - Noisy channel for NLP problem solving 
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/gd5oKbFgbLjvO_DtrtPMsPPsbFSxnwrwAw8wEWt1ankhz-p-f_mfgJv34qRWokw5wmOHwL4FTyUDdpBT42AhpXAsgl095Z924xS9m6jEIgSvEFgeS_sKkDiv5464be_j.png)
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/bRRm9FjBW1mULqnIflMC-R_pFxYvgxT3FYvjeP0Tg4HEeMPVTPvdEmWb7L8lVVYsEiQlHCT4QcK1B51586KZd-qPHmAfj2YdChYNDlil6qF1IeXl5W5EujQmicaV9_0_.png) 
What does this equation mean?―To find $I'$ we find the most likely input, given the output. E.g. the element with the highest probability. 
Note that $p(i)$ is the probability of the input.
And, $p(o|i)$ is the **channel probability**.
    - Translating from English to French 
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/utpSM_KNsO_uYwjGqMJFE_WNUy1gRp31pKHbwIvmxTbxyjIWvUwAoetRSPK7tMoDB2A4J9dpKDrHnm0YSqQ_oxl1V4K5KmxzQmCv6IY85nYX2HrgI1B8Dr3PeeqIeuWE.png)Let's pretend the original text comes through a noisy channel and comes out as English - (bonjour ⇒ hello). To recover the French we need to decode the English.
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/nV27rK1vGP9zq6e5-OIoNupob4mCLDGTeQGicSd7Au7fIaN_sLvBIMoUUbnGkKp7xsM5O5RYcAsiNwUSs80woWU2p-eMSmtJ9MHtWobv88-8iRjOCpBu6PkaXlozStgO.png) 
        - E.g. we use this idea of a noisy channel, to build a translation tool.
    - Spelling correction using noisy channel method 
        - Two types of spelling error:
            - Non-word error: fall ⇒ flal
            - Real-word error: Three ⇒ There
        - Detection of non-word errors is easy (not in dictionary). So we'll detect and correct.
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/qKlvjXSlAmDHcMJCGRWMD8_fmSW2wYSTdKcuN0r7Qktt9IgPGp20peSRbJuiegU36SMFJfHYm5zDBa54GilRhAkuJZuey95noaitVT5ANdzB1oDv49edB4i132Lc8Nir.png)
How can we model $p(word)$ and $p(error|word)$?  ↓ 
            - $\text{p(word)}$ can be obtained from a corpus. Could use some n-word lookback or similar. 
            - $\text{p(error|word)}$ can be obtained using minimum text edit distance or minimum pronunciation distance. 
        - Edit distance model 
            - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/IzVmhjREdxY0kDmH0O9KzGg3Lf7YddOYG9vMX_QDPCb8avT9X6CCaBpF4d1rKqIbKmqKZAnm10n04rLOOORS9p618bf0XQNGIN5RKXE06f7J-I1DfUd3jNCcazICOBGc.png) 
            - Uses insertions, deletions, substitutions and transpositions (ac ⇒ ca).
            - We form loads of candidate corrections based on different letters to apply to and different methods of fixing.
            - Note that 80% of words are within 1 edit difference.
            - We then choose the best edit based on confusion matrices:
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/GfMJZRRLR-4hUW5_U0cvKeaxacHkLIGuIJe117lS8ExCKSE05iLsuCi4bEL8iOf7SSIjiQebakRl0n0K3LSybRpbqj2x6clkZbmTvQoAbVKYXAz81Q5L9J0P8WmN9D7_.png) 
    - 
- **Lecture 8 **(
    - Words local context 
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/t7xpgzYJt4Fh5iv7bCsD00rbw9nf9xQaOea2MfWR2MfAXi52XsTXQUK_KbTziNbDPyZXP6y29TuLI-kfZuTiHmVEpidb2HOdzPJcRLxvsiji55stoZA1NxXDAKm9w2v_.png) 
        - We've been examining words as discrete symbols, but we need an indication of words similarity:
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/rFLS7E9vvV51vutshH_XC9tgpOfsPz1VKDnuc6hzco--gWcnW8lnhas0Vwi5Qb7MeAupuXE3fBBX3xiJ-12sA3hKuvFBdRVEWZuMscFlpzjiNwQVV1YhXpQdtFFFtLqh.png) 
        - Instead of symbols, to find similarities we can represent a word by―a collection of keywords from its context (as a proxy for meaning).
        - How do we weight words in our context window around a target word?―Based on their frequency. E.g. 
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/hOHI6lY2sdLRDF7O2ioVXahPDZJlIkn_4vLt4af-QICETifswpQCeAvmnRnwizjR5Hbdii-Q2B-_5jmXXH_xW8sCqO-cCaotnW6snoS1dUqPuFR_AQGwTEdLdkjO1Yew.png)
This becomes more informative if we then remove the function words: 
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/vNRXPeS9DNxBO1AySIf_ZyQF6zOP5sBRpOVo6X8FVLbwb2g8ob6bOKvf7T2NR64hCeWpuSz-DBDx6b6K8Eg3DcZ-7BOC9T5K33N4FfsJNTtp-D70k2t2PKxnFu9lS6zW.png) 
    - Replace symbols with vector representations 
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/R0CFzKKXtBX2g7s0G54HhDwViU0EdRGxr6mN0TQM53ZBZF9koy23XGWpmDrYPvJmHdio7WwFrXV7o_eCZuySYV6kVMFxZo89ntaKNFEdlGYK2wq1X65N2NkrznDehken.png) 
            - Give one problem with finding similarity using it's "context" vector.―If a word has multiple meanings, we can get the incorrect meaning of the word. E.g. Cheshire cat and domestic cat in Alice in wonderland.
        - We then measure two vectors **proximity in word space **and correlate that with **similarity **in meaning. We can measure that similarity with any distance measure - dot product, Euclidean distance etc. 
    - Devising context vectors 
        - These can be generated expertly, or by just using the surrounding words for each word. This will result in **large, sparse **context vectors. 
        - Is there some k-dimensional space that is sufficient to encode the word meanings of natural language? They could encode tense, count (singular vs plural), gender etc.
    - Reducing generated context vectors 
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/QpcAstaHpCJGRkEQFzZQCAWYOpMTc5rF3fVpxTZYIVNSUsaEAjqRDD7yJgftxoV-CtuiPpzafOxZf2QWt_VtWkqeFDWv9gFiE-4fbgoFS_Hij_9qpgkwI-9W6UYAcl4e.png) 
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/obV-lZJuvIZxKDSgmmE6EFZ6jAXqt-U-yB6ixXghjrX6NJwZHlaxlWJH-mn4KLGOdK--WdC4YDKvljaYUtHq_OhcSnu4Xqini_3U5IkU0AmaNeFKvFouRp-kW9WMzJTW.png) 
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/7KI1Fi934RTXPZhXsEb3swzMemuwD4PU9vb1USEhiY5U_OYK5spaPR0cHTVpjKp9MS1OYBvnvx8tjs2kFzkp8pu7c1309OOvEMuv_4ov9XNM0h7okNt51X6XRt5J7aUF.png) 
    - Predict models 
        - What is a predict model?―A model that learns iteratively to output the probability of a word given it's context. 
        - The parameters of the model are the word embeddings. The model is trained on a certain objective.
        - At every iteration we run our model, evaluate the errors, and then adjust the model parameters that caused the error.
        - Describe the Continuous Bag of Words technique―We predict the target's word embeddings, given the word embeddings for all the context words around it. 
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/z_lMt-FJf_FcLxJTYNgLb84GotA6AldwSdrS7nyZkHtzIjli6FU-12-fZcCNt2WoPABv1cW_mc-6Ps6gQ7EE07yWsnbgVioNl4xwE2_80kMldIXuFL1gDtIpQig3jQo2.png) 
        - Describe the Skip-gram technique―Given a target word embedding, predict the context word embeddings for words around it. After a prediction update based on the ground truth.
    - Distributional models 
        - Count models and predict models are distributional models, both are representative of how the word was used.
Why do distributional models make Multi-modal (including extra information like speech) experiments more straightforward?―We can easily add new components to the vectors describing information about what we're hearing, seeing etc.
        - These have much improved NLP - change in the state-of-the-art e.g. Google Translate.
        - These models are statistical, and thus need very large amounts of data - and thus need a way to handle unseen words.
    - Training word embeddings methods 
        - Many different methods for grammatical error detection:
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/u8qsprX6GA1iKf17F-nVjUY12mUYbzHfhtyMb26-aOSHUjfU5Ih98_F72tmh3_yZD1F2_tDlJQiVFVOSYL_SNpfygX2pPJK8YZWUX06C-DHrF9PvcW525mXm_aEsrnOC.png) 
        - These train on sub-word units, e.g. characters, so we can make predictions on unseen words based on character sequences.
    - Predict models vs count models 
        - Predict models can be more efficient, don't need to hold statistics on the whole dataset. 
        - Need to initialise the word embeddings.
        - The size of the embeddings is a chosen parameter and the best number isn't known. 
        - Predict models need no hand-crafting.
        - Dimensionality of the embeddings are assumed to capture meaningful generalisations, but **we don't actually know what each dimension means.** 
        - Predict models perform better on some tasks. 
    - Word embeddings correlated with human intuitions 
        - We can rank relatedness of words, based on human similarity judgements. 
        - We can then find a rank correlation between embeddings **and **human judgements. Where good embeddings would have a correlation of 0.8 or more. 
    - Reasoning may be possible based on word embeddings 
        - To solve the problem "Man is to woman, as King is to ..." (answer being Queen).
            - Find the difference vector between man and woman, and add that to Queen. Then search for the closest word to the result in vector space.
![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/1x5-9n5fzqludXtXU1OGPTMYslwK3xo6miT_jvExgruKocNrwd3T2inYu7KNTccsEcH4MMcciz3G3UatGSfzkE5u4awy4mhGE5Ay2iUgzRF9l1xpC-yQtvkS65Mq9riS.png) 
    - Relationships between embeddings and **brain activity?** 
        - We must have some mental representation of meaning that are mapped to language, but no direct access to the representations.
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/AMNsNhU-8wBWrKjVryJPVMI4e1SkukxzzdjFqEi5ekmv0Tblx14oqRSWX0EM3iF0d-RV21SRG_DpBRdl4qhIzjxncJzmMFW2yAw9lIjqK_yMof-xYUN3AJIs9bdXFsSf.png) 
        - 
- 
- **Supervision 1**
    - Natural Languages 
    - Question 1 
        - a.) She fed her cat food. 
The subject for fed is ambiguous, she could be feeding her cat some food - or she could be feeding her "cat food" somehow.
        - b.) She saw the man with one eye.
Can't tell which part of the sentence "with one eye" is associated with. She could be looking at the man with one eye closed, or the man she's looking at could only have 1 eye.
        - c.) She saw the queen in the garden with the telescope.
Again, you cannot tell which actor in the sentence "the telescope" is associated with.
It could be the girl looking, the queen or the garden which has the telescope.
        - He'll probs give as a note, but because binary - need to split a VP into a VP and something else (PP), so the stuff to the right then modifies the VP. Same for NP and PP. 
    - Question 2 
        - a.) I told the girl the rabbit knew the caterpillar would help her.  
Deciding which object goes with which subject is complex in this sentence. There are multiple parse trees which you have to consider, and this takes time. You need to remember things during the parse - "pop" things off and off your cognitive stack.
Additionally, there are multiple correct parses. Do you decide the rabbit knew the caterpillar would help the girl **or **does the rabbit know the girl and the "I" thinks the caterpillar will help her.
Craft sentences with various numbers of parse trees and record human parse time.
        - b.)The twins the rabbit the girl chased liked laughed.  
This is another example when an unusual amount of memory is required for a parse, we have a sentence in the general form $\text{noun}^n \text{verb}^n$ where we must remember which noun goes with which verb. This increased memory usage makes the parse more complicated. The main point is that the noun and the verbs are spread out, meaning we must remember things for longer.
You could test this by crafting various sentences where dependencies between words in the sentences are needed to be remembered for various times - then record the parse time for each sentence.
        - c.) She shook the bottle containing the potion which had made her grow very tall up.  
Another example of b, we need to remember "the bottle" until the very end were we note that she's shaking it up. Additionally, the up is surprising as we're more used to combining up with grow up (grow is close by) than with bottle.
Long range dependency.
    - 
    - Formal Languages 
    - Question 1 
        - Can do all of these using product construction with DFAs.
        - a.) Given machines M1 and M2 which recognise strings in L1 and L2 respectively. Simply form the DFA which non-deterministically decides between using L1 or L2 to parse the string.
        - b.) Simply concatenate machines M1 and M2, replacing the end state of M1 with the start state of M2.
        - c.) First note that if we have a machine M accepting language L, the complement of L (L') is accepted by the machine M' where all the accepting states become rejecting and vice versa. L' is regular because M' is regular and accepts L'.
Then we can use De Morgan's laws:
$\overline{L1} \ \cup \ \overline{L1}$    This is a regular language by a.) and complement.
If we apply the complement again (using De Morgans laws) we get:
$L1 \ \cap \ L2$. And as this is the complement of a regular language, this must be regular. 
    - Question 2 
        - a.) Prove if L1 regular, L2 context-free then $L1 \ \cap \ L2$ is context-free.

Assume $L1 \ \cap \ L2$ is regular.
Consider that a pushdown automata can be converted to accept the complement of it's context-free language by making the accepting states non accepting and vice versa. Therefore, the complement of a context free grammar is also a context free grammar - as it's accepted by a pushdown automata.
Using that fact and the method in c.) we can derive that $L1 \ \cup \ L2$ must also be regular. 
Then, by our assumption above, the following must also be regular:  $(L1 \ \cup \ L2) \ \cap L2$. 
However, this is just $L2$, so $L2$ must be regular. But this is a contradiction!
Therefore we must discard our assumption that $L1 \ \cap \ L2$ is regular. 

^^Not sure how to conclude from this that the language is context-free - i.e. how do I say, if its not Regular it must be context free because... 
If we have the fact that languages in the Chomsky hierarchy are closed under union and intersection then the proof is easier. Simply build ^^$L1 \cup L2$^^ then intersect with ^^$L2$^^ which is context-free. ^^

    - Pumping Lemma 
        - At the end of the PDF:
    - 
    - Top-down Parsing 
        - Question 1 
            - I'm getting 3 for a.) and b.). I think the right answer from using online parsers should be 4 and 9. Spent >3 hours on this already not going to troubleshoot anymore.
I've gained an understanding of the Earley parser.
            - ```python
S, NP, VP, PP, N, V, P = "S ", "NP ", "VP ", "PP ", "N ", "V ", "P "
dot = "• "

rules = {
    NP + VP: S,
    N: NP,
    N + PP: NP,
    P + NP: PP,
    V: VP,
    V + NP: VP,
    V + VP: VP,
    VP + PP: VP,
    "can": N,
    "fish": V,
    "rivers": N,
    "december": N,
    "they": N,
    "in": P,
    "can": V,
}
chart = []

print(rules.items())


class Step:
    predict = []
    scan = []
    complete = []


class Rule:
    def __init__(self, name, body, i, j, derived_from):
        self.name = name
        self.body = body
        self.i = i
        self.j = j
        self.derived_from = derived_from

    def __repr__(self):
        return self.name + "-> " + self.body

    def __eq__(self, other):
        return (
            self.name == other.name
            and self.body == other.body
            and self.i == other.i
            and self.j == other.j
        )

    def advance_dot(self):
        symbol_list = self.body.split(" ")
        for index, symbol in enumerate(symbol_list):
            if symbol + " " == dot:
                symbol_list[index], symbol_list[index + 1] = (
                    symbol_list[index + 1],
                    symbol_list[index],
                )
                return Rule(self.name, " ".join(symbol_list), self.i, self.i + 1, None)


def get_rules(rule, non_terminal):
    results = []
    for key, value in rules.items():
        if value == non_terminal:
            new_rule = Rule(value, dot + key, rule.i, rule.j + 1, None)
            results.append(new_rule)
    return results


def get_symbol_after_dot(rule):
    symbol_list = rule.body.split(" ")
    for index, symbol in enumerate(symbol_list):
        if symbol + " " == dot:
            return symbol_list[index + 1]


def predict(prev_step):
    results = []
    added = {}
    for rule in prev_step.complete:
        new_symbol = get_symbol_after_dot(rule)
        if new_symbol == "":
            continue
        for new_rule in get_rules(rule, new_symbol + " "):
            if new_rule.name + new_rule.body not in added:
                added[new_rule.name + new_rule.body] = True
                results.append(new_rule)
    return results


def scan(current_step, current_word):
    i, j = current_step.predict[0].i, current_step.predict[0].j

    # BIG time bodge, didn't think about having a word both a verb and a noun
    if current_word == "fish":
        rule_1 = Rule(V, current_word + dot, i, j + 1, None)
        rule_2 = Rule(N, current_word + dot, i, j + 1, None)
        current_step.scan = [rule_1, rule_2]
        return current_step

    rule_name = rules[current_word]
    new_rule = Rule(rule_name, current_word + dot, i, j + 1, None)
    current_step.scan = [new_rule]
    return current_step


derived = []


def complete(previous_step, current_step):
    to_advance = current_step.scan[:]
    for finished_rule in to_advance:
        non_terminal = finished_rule.name
        for rule in current_step.predict + previous_step.complete:
            next_symbol = get_symbol_after_dot(rule) + " "
            if next_symbol == non_terminal:
                new_rule = rule.advance_dot()
                if new_rule.body[-2] == dot[0]:
                    if not (new_rule in to_advance):
                        to_advance.append(new_rule)
                    derived.append(new_rule)
                current_step.complete.append(new_rule)
    return current_step


# test = Rule(P, N + dot, 1, 2, None)
# print(list(get_symbol_after_dot(test)))
# input()

start_state = Rule(S, dot + NP + VP, 0, 0, None)
previous_step = Step()
previous_step.complete = [start_state]

import copy

log = []
string = "They can fish in rivers".split(" ")

for i in range(len(string)):

    print("STEP", i)
    new_step = Step()
    # new_step.complete = []
    new_step.predict = predict(previous_step)
    print(new_step.predict)

    scan(new_step, string[i].lower())
    print(new_step.scan)

    derived = []
    complete(previous_step, new_step)
    print(new_step.complete)
    print(len(new_step.complete))

    new_step.predict = []
    new_step.scan = []
    log.append(copy.deepcopy(new_step.complete))
    previous_step = new_step

    print()

print(derived)
print(len([x for x in derived if x.name == S]))

# 1 for the first.

```
            - a.) 3
            - b.) 4
    - 
    - Comparing Grammar Formalisms 
        - Question 1 
            - The grammar is:```python
 S = NP VP
 VP = V N | V NP | PP V | VP NP
 NP = Det N | N
 PP = P NP
```
            - Question 2 
                - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/34VkdDHE7iHWyNB1FEC78owFxGCaY8e5HXcSvvvQGGpvSHVJQSkiHp32o1FjSIhyX0IFZgc-Au7gtM8ZYrynB5J2S8zarV-n9OaG6E_BpdmYjiON7eqM7g0y8M4quHsD.jpeg) 
                - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/ZfJkwr8R9HW7zK3WgZjyWbm7jpeCy_ysBQ7rrCz7ol3ZHc-79Fs0KLXGJB1kcsmPnNh0WcKZGgx4gLVp9a-zPoeZCMCtCYT85K0R3mTucvYudSry1p3VUcXWzeKssQfO.png) 
        - Question 3 
            - Don't understand tree-adjoining grammars at all, can we go through. 
    - Pumping Lemma: 
        - 
    - 
    - 
- **Supervision 2**
    - Natural Language 
    - Question 1 
        - Is frequency information or structural information more important when considering processing difficulty?
        - This question boils down to deciding if a rare/complex word is more difficult to understand than a rare or complex sentence structure. If a rare/complex word that a reader has never seen before is included in a sentence, the reader can only look towards the context to understand the sentence. Similarly, if a confusing sentence structure that the reader doesn't understand appears, they have to look at the individual words and build an ordering that would make sense. I would argue that a reader would have more chance with the latter, if I simply don't understand a word in a sentence and I cannot reconstruct it from context - I have no chance of processing the sentence (given any amount of time). Contrarily, if I understand the meaning of the words in the sentence but not the structure - I can ponder for long enough (building parse trees etc.) until I happen upon an ordering that makes sense.
    - Question 2 
        - Give examples and counter-examples of sentences in English (or any other language) that would support theories of constant information rate:  
        - Swear words are often added for emphasis, but you could hypothesize that a sentence with a swear word is want to be understood (and is often said quickly) and the swear words add redundant information to inhibit word loss:
What the `****` are you talking about? - Where the redundant section would be the `****` 
        - Many songs provide counter-examples to this, where words that could be implied from a sentence are stressed:
♪ I'm in **looooove **with you  ♪   
In this case, the singer is providing emphasis and not slowing down the sentence for listener comprehension.
        - Often when giving instructions, an information dense section will be followed by a period of slowing down the information rate: (Assume X, Y and Z are information dense):
"I want you to do X, Y, Z. And I think that's all there is too it."
This is perhaps used as a time for the speaker to catch their breath.
    - Formal Languages and Learnability
    - Question 1 
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/e1jeV9p6yknoMriBArBZU2G75l9F7bHH5ElweSTAgfl3z33qTK4AaR2PLtQY96Mnf_-W969f8XmapC6zb4ny8fXOJ5Wui3aIZyFqw8G3DhuCQJlbiz0bn8-dXUXAhoen.jpeg) 
    - Question 2 
        - ^^For a finite language, in the limit the learner would have seen (and memorized) every combination of strings in that language. For two languages to be different the must have at least a single string that can be generated in one language, but not in another. 
So even if the learner does not understand the structure of a language, in the limit they will observe every string in the language and will either continue predicting their learned language until a string that doesn't match is found ^^^^**or **^^^^they'll continue predicting the given language forever as no incorrect string can be generated. 
Either way, in the limit they will predict the correct language. ^^ Makes no sense
        - For a finite language, it must have a finite number of rules and a finite number of characters. Thus at each step, the learner can build the shortest regular expression that matches every previously seen word in the language and will eventually reach the correct language.
- Not really sure on this question.
    - Question 3 
        - Every string could also identify the language it belongs to (language 1 or language 2) and the learner would then learn each in the normal ways, with different areas of "memory" devoted to the learning of each language. 
The string the learner would be faced with, would then be strings from a random choice of language 1 or language 2 annotated with the appropriate language.
    - Information Theory  
    - Question 1 
        - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/9yCTGkcPm7TAjEjHsYxZbr2t0JG1R4L_DHUt-13kbtazERHwVa3Hi_2SIPiTdjgnHtJObVK5zA3EioKcjfK1UoAGPz_iWLL99_1bE7ZjHGWmoxMuaKYXOW-jFPNqIOR-.jpeg) 
    - Question 2 
        - I generated:  ['theren', 'theere', 'itheel', 'thitou', 'ineath', 'dothed', 'heneth', 'shitha', 'thenll', 'tlinth']  
        - ```java
pair_freq = {}

for word in alice:
    for i in range(len(word) - 1):
        pair = word[i] + word[i + 1]
        pair_freq[pair] = pair_freq.get(pair, 0) + 1


for key, value in pair_freq.items():
    pair_freq[key] = (value / len(alice)) ** 0.3


def generate_nonsense(length):
    word = "".join(
        choices(list(pair_freq.keys()), weights=pair_freq.values(), k=(length + 1) // 2)
    )
    return word


nonsense = [generate_nonsense(randint(2, 6)) for x in range(100000)]
ten_mins = [-100000 for x in range(10)]
words = ["" for x in range(10)]


for word in nonsense:
    probability = 0
    for i in range(len(word) - 1):
        pair = word[i] + word[i + 1]
        probability += pair_freq.get(pair, -100000)
    for i, x in enumerate(ten_mins):
        if x < probability:
            ten_mins[i] = probability
            words[i] = word
            break

print(words)
```
    - Question 3 
        - Framing "automatically answering questions" 
            - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/ICQwWOsuaKl2BdpVKky0zXiCzlv88qcw1bXnXAff0AAFYBq3MJXtAJxt6LYnwY1UMotDF8v5NbU0sf0F90qk21dT2QU0DcB_YiOyG0EjYPiPiRyEhjmDvgoiS7c6nHum.jpeg) 
            - P(answer) is the probability of a given answer to a question, this could be estimated from a large corpora of question-answer pairs. 
P(question | answer) could be found from the same corpora, by measuring the number of times a given answer was given to a certain question. To improve this system we could extract meaning from the question rather than literal words, allowing for alternate phrasings of questions to register as the same question.
        - Framing "disambiguating multiple senses of a word" 
            - ![](local:///home/mali/remnote/remnote-614c8a3b6997e6001643dfce/files/_PSnzxvE94woSMrR6v1CcaryVJhyCIzDuGPw9z_DO9sg3nQAfNFoghOg5SW2dTGEJgN1JhL_4QbRP7bwDpWmxIkts6slpaXz8zJbcui9Wh92gplIhKowYRdkMzMuy-19.jpeg) 
            - P(meaning) would be a measure of how often a given meaning of a word is used in some large corpora, that corpora would have to be annotated by humans to specify the meaning of each word in each sentence. 
            - P(sentence | meaning) would be the probability of the given sentence, given the words meaning in that sentence. However, as matching an entire sentence would result in very few results in the corpora (and thus a unsound probability metric) we should only look at the n-words surrounding the word who's meaning we're trying to assign. E.g. If we were trying to extract meaning from: "You killed it on the mic" we might examine just "you killed it on".
    - Distributional models 
        - We could examine the frequency certain characters appear together, for higher frequencies this means the characters are used as a unit very commonly and thus have a similar function (e.g. 'qu', 'th' or 'an').
        - We could also examine the frequency that characters appear in certain positions in words, if a given word pair often appear in the same positions then they may have similar functions in the structure of words.
        - We could examine the characters that appear most 'n' number of times in words, they may also play similar roles in the structure of words. And for the special case when the length of the word is 1, who are words in their own right, we could draw that they also function as common structural elements in sentences. This is because of their very low length, and resulting low surprisal, they are encoded to contain little information - instead used structurally (I, a, o (in more Shakespearean texts)).
    - 
- 
- **Assorted Flashcards**
    - Are regular languages context-free?―Yes, you go out from the Chomsky hierarchy. 
    - 
