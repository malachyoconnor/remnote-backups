Data Science
    Supervision 1
        
            import numpy as np
import matplotlib.pyplot as plt

def circle_func(x):
	y = ((1 - x**2)**0.5) * np.random.choice([-1, 1], p=[.5, .5]) + np.random.uniform(-0.03, 0.03)
	return x,y

def smile_func(x):
	x = x/2
	y = x**2-0.8+ np.random.uniform(-0.03, 0.03)
	return x,y

def eye_func(x, left):
	x = x/20
	if left:
		x -= 0.4
	else:
		x += 0.4
	y = x*0+0.3 + np.random.uniform(-0.03, 0.03)
	return x,y


def rxy():
	functions = [circle_func, smile_func, lambda x: eye_func(x, True), lambda x: eye_func(x, False)]
	function = functions[np.random.choice([0,1,2,3])]
	return function(np.random.uniform(-1, 1))

results = np.concatenate([rxy() for x in range(1000)]).reshape([1000,2])
print(results)

fig,((ax_x,dummy),(ax_xy,ax_y)) = plt.subplots(2,2, figsize=(4,4), sharex='col', sharey='row', gridspec_kw=dict(height_ratios=[1,2], width_ratios=[2,1]))
dummy.remove()
ax_xy.scatter(results[:,0], results[:,1], s=3, alpha=.1)
ax_x.hist(results[:, 0], density=True, bins=60) # fill in the ???
ax_y.hist(results[:, 1], density=True, bins=60, orientation='horizontal') # fill in the ???
plt.show() 
        
            density func:
                \int_{0}^{u}{u\cdot(1-u)du} = u^2/2 - u^3/3 
            cumulative distribution function:
                %LOCAL_FILE%5RCG5z7hsBmrF-QFWJFCBf4JNmoCJWWx1qb5NumaUsGp05deuEiDNdkSuJLW2PQmSACOmx8S-k1pBEO1jmKnZGBPbRQ-EKVs2a7Oaabi-_-Z4AFpPJW26YzZLt1ZzFnt.png 
        
            pdf = \frac{d}{d\theta}(\text{cdf}) = 1_{\theta \ge b_{0}} \cdot (\alpha \cdot \frac{1}{\theta}\cdot (\frac{b_{0}}{\theta})^\alpha ) 
            b.) 
                Pr_\Theta(\theta) = 1_{\theta \ge b_{0}} \cdot (\alpha \cdot \frac{1}{\theta}\cdot (\frac{b_{0}}{\theta})^\alpha ) 
                Pr(x_1, x_2,...,x_n | \Theta=\theta) = \left( \frac{1}{\theta} \right)^n 
                Pr(\theta | x_1, x_2, ..., x_n) = 1_{\theta \ge b_{0}} \text{Pareto}(k, \alpha+n) \text{let} \ c = (\alpha + n) k^{\alpha + n}  Then Pr(\theta | x_1, x_2, ..., x_n) = 1_{\theta \ge b_{0}} \ \  c \ \ \frac{1}{\theta} \cdot (\frac{1}{\theta} ^{\alpha + n}) 
            c.)
                P(\Theta \le \theta) = 0.95  ⇒ 0.95 = 1 - (\frac{k}{hi})^{(\alpha+n)} ⇒ hi = (0.05)^{-\frac{1}{\alpha + n}} \cdot k 
        
            
        
        
        
    
Programming in C & C++
    Supervision 2
        Arenas are a useful tool for amortizing allocated memory discovery, and have the side effect of potentially more efficient cache usage through the introduction of an array for item storage. I would use one when I have a data structure that may contain cyclic sections or sections where a given item is pointed to twice from another item (DAG) - in which case traversing the structure myself at the end becomes complex & error prone, and can result in memory leaks if not carefully thought through. Arenas trade-off complexity with memory usage, as we must allocate a large array of items (the size being the maximum number of items the program could use) to eventually deallocate. An example control flow would be creating a graph, every time I create a node I add it to the arena (wherein we already specified the max possible number of items) incrementing the current value in the arena - then when finished with my data structure I free the arenas elements array and free the arena itself.
        In mark and sweep garbage collection we store an list of "root" nodes (items that are alive, and we'd like to keep alive) and an list all nodes (these may be the lists). In the mark phase we iterate through all the root nodes, and mark all nodes reachable from each root node (i.e. depth first exploration, stopping when we hit a marked node). Then in the sweep phase, we step through our list of all the nodes and free any unmarked nodes and unmark any marked ones. When we next run the garbage collector the nodes still alive will form our roots.
            a.) A conservative garbage collector is one who scans the execution stack, and upon finding a value that looks like a pointer to the heap - if there is an object at that pointer, the collector must mark it. This conservative estimation is that this is a pointer to the heap and not (say) an integer value. This is necessary many reasons, including:
                C lets you easily convert from values to pointers, so we can't know if in future the programmer will use that int as a pointer to an item in heap memory.
                The garbage collector should never alter the execution of the code, and storing extra blocks of useless memory is better than deleting a useful block. Especially considering that if the value on the call stack is not a pointer, it is likely to change at which point the memory can be freed. 
            b.) Garbage collection solves the issue of cyclic data structures that reference counting cannot - under reference counting, two cyclic items pointing to each other can maintain a pointer count of 1 each even if there is no way of accessing either item (all pointers to those items has been removed) - thus these marooned items cant be freed resulting in a memory leak. Garbage collection stores all the nodes, and will find these items to be unmarked in the sweep phase - thus they will be freed.
        Show how a struct of arrays performs better than an array of structs when traversing the elements of the form struct entry { int id; double val; };
typedef struct entry Entry;

struct struct_of_arrays {
	int *id_array;
	double *val_array;
};

Entry *array_of_struct = malloc(sizeof(Entry) * NUM_ITEMS);

struct struct_of_arrays Struct_of_Arrays;
Struct_of_Arrays -> id_array = malloc(sizeof(int) * NUM_ITEMS);
Struct_of_Arrays -> val_array = malloc(sizeof(double) * NUM_ITEMS);
            For simply iterating through the array of struct, we have an int of size 4 bytes and a double of size 8 bytes - this will then be padded to size 16 bytes. So we can fit 4 entries into a single cache line before getting a miss (whether we're storing an int & a double or only an int/double) - alternatively if we iterate through the id_array and then the val_array, we first have 4 byte ints to iterate through - resulting in 16 items before getting a cache miss - and secondly 8 byte items, resulting in 8 items before a cache miss. 
            \text{array of structs}: \frac{n}{4} \text{ cache misses} 
            \text{struct of arrays}: \frac{n}{8} + \frac{n}{16} = \frac{3n}{16} \text{ cache misses} 
            So as \frac{3n}{16} < \frac{n}{4} = \frac{4n}{16} we'll have less cache misses in our second solution, and as cache misses are hundreds of times more costly than arithmetic operations - our struct of arrays will perform better.
        Describe loop blocking and when it's useful:
            Loop blocking is a technique for deconstructing accesses of large contiguous data structures into smaller chunks which can be stored in cache lines. The first access of each chunk will result in a cache miss, but subsequent accesses wont. In short it attempts to introduce spatial locality of data, for fast accesses.
            The technique is useful when a given array access scheme jumps around a lot, and does not return to a chunk of memory for a long time (at which point the cache line can have been cleared). Alternatively, your method of jumping could cause conflicts within the cache and force eviction of prior cached lines. In general, loop blocking is useful for items with poor spatial locality.
        1998 Paper 6 Question 6
            a.)
                Memory corruption 
                Hard to read programs 
                Security liability
            
                Low level control
                
            b.)
                Forgetting to deallocate sensitive data?
                Memory leaks
            
        2010 Paper 3 Question 6 
            a) 
                typedef struct node* Node;
struct node {
    int value;
    Node xor_pointer;
};
Node head;
Node xor_addresses(Node a, Node b) {
    return (Node) ((long long)a ^ (long long)b);
}
void add_value(int value) {
    Node new_node = malloc(sizeof(Node));
    new_node->value = value;
    new_node->xor_pointer = head;
    head->xor_pointer = xor_addresses(head->xor_pointer, new_node);
    head = new_node;
}
Node get_next(Node prev, Node current) {
    return xor_addresses(prev, current->xor_pointer);
}
void traverse() {
    if (head==NULL) return;
    Node current = head->xor_pointer;
    Node prev = head;
    printf("%i  ", head->value);

    while (current != NULL) {
        printf("%i  ", current->value);
        if (current->xor_pointer == 0)
            break;
        Node temp_prev = current;
        current = get_next(prev, current);
        prev = temp_prev;
    } 
    printf("\n");
}

void delete(int value_to_del) {
    if (head==NULL) return;
    Node current = head->xor_pointer;
    Node prev = head;
    
    if (head->value == value_to_del) {
        if (current->xor_pointer != 0) {
            current->xor_pointer = get_next(head, current);
        }
        free(head);
        return;
    }

    while (current != NULL) {
        if (current->xor_pointer == 0) {
            if (current -> value == value_to_del) {
                prev->xor_pointer = 0;
                free(current);
                return;
            }
        }
        if (current -> value == value_to_del) {

            Node next = get_next(prev, current);
            Node prev_prev = xor_addresses(prev->xor_pointer, current);
            prev->xor_pointer = xor_addresses(prev_prev, next);
            Node next_next = xor_addresses(current, next->xor_pointer);
            next->xor_pointer = xor_addresses(next_next, prev);
            free(current);
            return;
        }
        Node temp_prev = current;
        current = get_next(prev, current);
        prev = temp_prev;
    } 
    printf("Node not found to delete\n");
}
 
                b.)    Comment on this form of linked list. Consider the comparative speed, memory overheads, maintenance and other advantages or disadvantages of the XOR doubly-linked list approach when compared with an approach that stores both previous and next pointers  
                
                In this form of doubly-linked list, each node takes up less space - while before they store two pointers and a payload now they store only one , resulting in less overall memory usage of the new data structure. (16 bytes vs 24) For large lists, one list being two thirds the size of another while accomplishing the same thing could be important.
                However, if you want to hold specific nodes to edit later - you need to hold that node and the previous node if you want to be able to traverse from your node and edit preceding nodes - this makes it harder to maintain, as management of individual nodes is more complex. Additionally, traversing the list is more expensive in the XOR doubly-linked list as we must compute an XOR every time (which is admittedly a relatively cheap operation). 
                You could make the argument the XOR doubly-linked list is more secure, as you can pass your node value to another function safe in the knowledge it cannot destroy any other of your nodes as it will have no way of traversing to them. 
        1996 Paper 5 Question 5 :
            a.) Pre-processor macros and conditional compilation:
                #define BIT64 1

#if BIT64==0
#define BIG_INT (64*1024*1024)
printf("%i", BIG_INT);
#else
#define BIG_INT (1024*1024)
#endif

// This defines a macro BIT64 set to 1, and chooses different definitions of BIG_INT depending on the value of said macro 
            b.)
                char t = 'a';
char *p2 = &t;
int *res = (int*) p2;

// We created t, which holds the a character. Then we instantiated a char pointer to t. Then we created an int pointer res, and cast the char pointer to an int pointer. 
            c.)
                // This is a single line comment
/*
	This is a multi-line comment
	!!!
*/ 
            f.)
                jmp_buf current_env;    
int sam = 0;
int val = setjmp(current_env);
printf("%i\n", val);
longjmp(current_env, sam++);

// This code infinitely loops printing 0, 1,2,3...
// setjmp copies the PC and stack pointer into the jmp_buf current_env, then we print sam and finally we longjmp back up to int val = setjmp(current_env); val will then be overwritten with the value of sam+!
            g.)
                unsigned int sam = 3;
switch (sam) {
    
    case 0:
        printf("Sam less than 2");
        break;
    case 1:
        printf("Sam less than 2");
        break;
    default:
        printf("Sam more than 2 ;( ");
        break;
}
// if the unsigned int sam is 0 or 1 we print "Sam less than 2", otherwise we print "Sam more than 2 ;( ".
// In this case we'll print the latter, as sam is 3. Each constant case is checked against the value in the switch()  braces and a case is selected. 
// The breaks ensure the default case doesn't always run. 
        2014 Paper 3 Question 3   :
            %LOCAL_FILE%zB0WVJroIQK0RkHQzho8zvjIfCkmXtlmIL3jLz1ZOLw0MoLHOFDLFaEUh1PaKLG-bc64iVhJIajoLCoCy2b996jK4MoffBw6nDR-nU2P_UYxEe8qrzNow4mU46s9QqRk.png
            char revbits(char to_reverse) {
    int working = (int) to_reverse;
	int result;
    for (int exp=7; exp>-1; exp--) {
        if (working > 1<<exp) {
            working -= 1<<exp;
            result += 1<<(7-exp);
        }
    }
    return (char) result;
} %LOCAL_FILE%4EncOulylcP3ibS2i2YvFWOnzfLIxKEKnPaaiubqq7GDBEapF3kAEAU-uD7XBSla7UjDwumB_HkKu4FrEu1z6Zt9XdJ1EG6EnsNg07bjh9XbeYN6QInE3JQbCpm6AdPo.png
            void revbytes(char *ls, int num_bytes) {
    int left = 0, right = num_bytes-1;
    while (left <= right) {
        if (left == right) {
            ls[left] = revbits(ls[left]);
            break;
        } else {
            char temp = ls[left];
            ls[left] = revbits(ls[right]);
            ls[right] = revbits(temp);
            left++, right--;
        }
    }
} %LOCAL_FILE%5CvOkCE8ymJEeAFOsAjUVCQ44LCbCUtbJYWKdAgIN0OS8u2Wsy8ILFqk3zp-DPTbnJzLqlTcGVHd5dTnEuRc-4ql_tRA0umkx1Z9W4pm4vhHkDiAUYf7cgRiqiUI5SfJ.png
                The problem here is the file is left open after the result is returned, we should change the block to:
                if (...) fclose(p); return ERR_MALFORMED; 
        1995 Paper 5 Question 5  
            This is an attempt at an implementation of quicksort.
            typedef unsigned long int thing;
// Bad unilluminating variable name #define swap(p,q) v = p; p = q; q = v;
 // the type of v is not defined, this will result in an error.
            		/**********************
int i, j; * Declare variobles! *
thing v; **********************/
// int i,j and thing v will be commented out here. Also incorrect spelling of variables 
            i = left, j = write;
// i & j are not given types 
             while (a[++i] < v};
 // Wrong ending bracket, should be  while (a[++i] < v) 
            while (a[++i] < v};	
while (a[--j] >= v);
if (i < j) swap(a[i], a[j]);
else break;

// I believe what they intended was:
while (a[++i] < v} {
	while (a[--j] >= v) {
		if (i < j) swap(a[i], a[j]);
		else break;
	}
}

// What they achieved simply repeatedly increments i until a[i] >= v, and repeatedly increments j until a[j]<v and then swaps a[i] & a[j]

// ALSO these should be a[i++]... and a[j--]... . Otherwise we dont test the initial values of i & j.  write would more sensibly be named right
        
        Questions from lectures:
            In the lecture on reference counting, the lecturer builds a binary tree using:
            %LOCAL_FILE%crzic95YRyiryiMHGIrmxmqkDggUe_2KbJnaftk-5YUTWrhbDsP0rUPfufcy8XmrHUAHD9C9DPDpkqALR3EPC4UDY5wVP-WUnHm3WYEhxJX-Ea59BIvdWIrXP6XKJJO9.png
            But wouldn't this mean, If I edit a value on the tree (other than the root) I'm actually editing two? Given that both left & right point to the same node. Sure the tree is build, but it can't be used.
        
        
        \text{speedup}(n) = n(1-B)+B 
    Supervision 1
        %LOCAL_FILE%5x-kpFtSmurzNUmXPlR8GMTKt44S-pGI_uXQn-loC5FTlBbwH2KnTOZb_8vHNtz5Vc-B7ENcAojGyI83lmpHfPa1uVKKFQiiso5afgZZSAndShPRGx_1n1jctwLtBL13.png 
            'a' is a char while "a" forms a string literal - e.g. an array of chars terminated with a null terminator \0.
        %LOCAL_FILE%Y7TUSDPht8rMMmjwyibGk-4zzDDtqFry6U0sB0wSGHN80olxqx8IhSL4w7q1B7AwdfL5s1F1G8LzJ8q7-K3oTPmuVAZMovFSPG6aKrPGL5mZ2dLldl2lZEglvvYQf7oj.png 
            Yes, it will terminate when j equals 5 (e.g. the 5th ASCII character) as long as the two chars were initialized to 0 (which is not guaranteed). This is because you can store integer values in chars, ranging from -128 to 127 for unsigned chars.
        %LOCAL_FILE%dpZ64aPO1RH5e4EsS1UpUmsF6S1j8sYlwLx6vDShfHV1sW0kweO6ow9QUln0VP0NK6dzaTv2HQGT_siYVBxZQugXD5dBNPaOHMVYXAvK5T_8rvmGEmV0otOOn9oYFKxz.png
            #define SWAP(t,x,y) {\
    t temp = x;\
    x = y;\
    y = temp;\
}
        %LOCAL_FILE%wnEQh3sjj-ggBtFu8O9dNmiI2bL6F-ycmM394KL46Q4W-zJ1YH1L8zb5AuhRQgO0yH7tmlkZoS4jhEaKhz0CZSQcIHphGvCgeGCD5LGCuKrC4Cx7S9oxORSzpGNZE18s.png 
            No, the i++ will be evaluated twice - meaning the x in the second line of the macro will differ from the x in the third line of the macro (unless the values of v[i+1] and v[i+2] are the same). Similarly, if f(x) changes when it's called twice we will be getting different indices of w.
        %LOCAL_FILE%29dC0HQ66WGsn2vo69u4gNIJZt9N4FnfT5RwFJfM_G3BW7d5EhdsZOHsXWNjWacNu5zL96DZhIyb0C6NbtTEcqJ-TlSKnzxtwO9zn0fVU-ErJJFrKkkTJH3OY9HJb7no.png 
            #define SWAP(t,x,y) {\
    x = x - y;\
    y = y + x;\
    x = -x + y;\
} 
        %LOCAL_FILE%VjH7bsEoeEXOb-ZAO4st3yXVKrJqqTgta_w8kMyod9Oe22Y2SY3BN4_UwFKmCzzzfdD_XCnzJG4-JQsuy9qVOjbjp3QFCvNPX9JwYgL0iIb2bJSPIhuJLUjEDjsW6djw.png
            p[-2] means the value two items before the pointer, this is legal if p is a pointer inside an array - this works because arrays allocate a contiguous block of memory which we can simply step along (pointer-wise) to reach earlier array-items.
        %LOCAL_FILE%6xCzQuVGM16QjzvbI4VY4CytzsorrFvjZX1148C4R1ovPU0H4YfSVTLPf1cagxhM-sD31wpTXg6cKaq74aQ609n_mIrRaPIb0Yvmv8tjBgN0IUPrbDjxPksgy8rh1VM-.png  
            char *strfind(const char *to_find, const char *to_search) {

    for (int i=0; i <= strlen(to_search) - strlen(to_find); i++ ) {
        int bookmark = 0;
        while (to_search[i+bookmark] == to_find[bookmark]) {
            if (bookmark == strlen(to_find)-1) {
                return to_search[i];
            }
            bookmark++;
        }
    }

    return NULL;
} 
        %LOCAL_FILE%GtHkEKbpOG6jIjweOng_l6W05H7Yu9StIWUwwxs9dLB2QzAVtBTakVMQh-pHkARbLFo5t3z56QsppajKkhummkk_929JjfaKqROSQEgcZnq0Z_E6z2HzmF_wMr1mqUGD.png 
            A const pointer to an int is a pointer that cannot be changed, e.g. it will always point to that integer. However, the integer value itself can be changed. A pointer to a const int means the pointer can be changed to point at something else but the integer value cannot be changed. For a const pointer to a const int, neither the pointer nor the integer value can change.
        %LOCAL_FILE%ux0pai4I1dl1vDxBhD-cTy9Pz5Ne3eDRwJRRZ6_oW7PPjGjn5gxRXK5Yci7-xFVq4OFPnbgim5O5YHtj-Atp0MF37r5LXuplWnJQLl4xKYwVvp2RaesqucxNl1jgu8zW.png
            You could pass the values as const:
            int adds_wont_change(const int *x, const int *y) {
    return *x+*y;
} However, this is only an indication not a promise - the function still has it within its power to alter the arguments using casts.
        %LOCAL_FILE%q7tzntYJoJxSzhSjxKKL5xo7tQqMM2aUb8ggmcPB4hgSru1O59WOwTIIP62lL9UZ0YPkGiR0WsTifUx0OSi5me7cNjbveENoAqi5HehC-Vkt1tGajaBGdTSkVhekzIGE.png
            char* read_a_file(char input_filename[]) {
    FILE *file;
    file = fopen(input_filename, "r");
    char *string = malloc(1001);
    int memory_len = 1000;
    int currently_used = 0;
    if (file) {

        char c;
        while ((c = getc(file)) != EOF) {
            if (currently_used >= memory_len) {
                memory_len += 1000;
                string = realloc(string, memory_len);
            }
            string[currently_used] = c;
            currently_used++;
        }
        string[currently_used] = '\0';

    }

    fclose(file);
    return string;
} 
        %LOCAL_FILE%0P3QCnLO7m_QReMJWnZiOuxdyR3x8rUtd4bSxrgQch_t8UEhDy3_iGE_cLCbN7043bbQXt9NcOorpBdzmfdQnEzNhkW0RsMTM0Ltx9cSEdIhb1jvO9vaDtXgTKXoCGka.png
            %LOCAL_FILE%gcBE6wQaCcIKAs4I7o-7BUREOglWeWiNw8BVM06fB5iiiNPfsSRYKNJn0vki2qk-RDt2nyizBGi96ckZmwg_rQFMd10Tci3VUG2wwnunH84ZH-_MHy6VY6QWxUhE4njR.png
            Func1 can pass the pointer to the array to bar as the array still exists while bar runs, however when Func1 attempts to return the pointer to the array it's memory on the stack will be deallocated and thus any local variables will be lost and the values the pointer points to will be undefined - that is it's very possible they will be overwritten.
        %LOCAL_FILE%KmKDW7D5NUYnv6HW7bY0Ut9L2F_PSp9sxF8ykkzky4_dUrWqcETQTVvt38Ziv905iqB3iSvV7SQEiPUOhrVvgJ79DOcg_jc1y26c4jvSACgavfpB3Sdz3ZP4AHY3UU-C.png
            Declaration simply tells the compiler a variable with the given name is going to be defined at some point, while definition allocates an area of memory for the variable. 
        
            a.) %LOCAL_FILE%BlbZtBHf-NMCqx0Ni45FvW9jsJN0z5SnOn0pjG_FusfpndxTwr2s-jl7bfzlokLtRs-Q0rR3NqUjvoXWEm5YZkOonm8F1twD90e2am_nDGl_2sn6oatalYCCy3Zahj_l.png 
            **i is the integer value 78 0c 00 00 - which is  c78 which is 8 + 7*16 + 12 * 256 = 3192
            p⇒c[2] is 62 in hex = 6*16 + 2 - slightly confused by this one, how does the compiler know to move 2 bytes along (as if its working with a char) rather than 9 bytes along (as if it was working with a struct i2c)
            &(*pps)[1] is 0x18 + 2 = 16+8+2 = 26 (we move 2 forwards because we're dealingiwth a short)
            ++p⇒i is 3192 + 1 = 3193 
            
            %LOCAL_FILE%01AJtT87kPJuaTX5C5XZt2fkDMlszM_KcOj76VYkPP3spld9TxWz8igbTte2mVcT__CsI1CFlFlr5dQEdpgf-iLHbBo6NlIEVf1o_opvopjYa6F9XEK7M8kOF47i33A5.png
                First we cast our manager pointer to an employee pointer, however when we call wage(e) it sill uses the wage calculating function wage_man as the address not wage_em (we never changed the function being pointed to in the cast) - this then casts our pointer back to a manager pointer and calculates 40*10 + 20 = 420.
        
            %LOCAL_FILE%8zVLbpTXRupf0NULVKpgctjp9VkjS4ZcHNDQLuTRUyCGgd-edbhLPclwFXnuh2uXkjhKEDkIJdy5lWAaQEGJWZsfd_psK0fXIA_-BxK-hqwe3y9Ljza6RhcTgKYN287h.png
                #define CONCAT(a, b) (a b)
            %LOCAL_FILE%tic3LFyhReYVg9QlQVNVKS7tIwEUZBzjUdDYyiQSwuZ9_n2oEAijrte48htdQ-JcRmPa7VhPCys-1hnYCqOysIGcx4ATPZnLFITqbksPDrIto3T51OM49qsvJWNfTpJj.png
                Line 3 would not work, as CONCAT only works by leveraging the fact that string literals next to each other get concatenated in c - e.g. "a" "b" ⇒ "ab". All CONCAT does is paste the values next to each other and let the language do the rest - however CONCAT(b, a) will result in CONCAT("UoCCL", a) and the compiler cannot simply infer the value of a to combine the strings (not a string literal). 
                Line 4 would not work, as j is not a pointer and you are thus taking the integer value of the pointer that concat(a,b) returns. 
                concat(a,b) allocates memory on the heap and creates a list of chars, while CONCAT simply places two string literals next to each other (uses stack memory). concat(a,b) returns a pointer to the char array while CONCAT(a,b) will evaluate to the combined string literals. 
        
            %LOCAL_FILE%Yc7pDYou5sDFsk8G-vXRMbYZJZkvIvoPnxYzVJzVgAFYuEB1gHaD_VDxY8yvyAernjZfJxRStzCKWnlxhKLebEBiAt38UsEs5HBe0iDRIrJeeAJE7wnMXcHI_ZDczi3L.png
                500 bytes: (4 + 1 bytes) * 100. sizeof(struct Elem) =  5 bytes. long is 32 bits - compiler supports unaligned access, unlike all below.
                800 bytes: 32 bit machine, the long is 4 bytes, char is 1 byte. sizeof(struct Elem) =  8 bytes - but as 32 bit machine have to use two 4 byte words. Resulting in 100 * (4 + 4) = 800. 
                1200 bytes:  48 bit machine, the long is 6 bytes, char is 1 byte, sizeof(struct Elem) =  12 bytes - but as 48 bit machine have to use two 4 byte words. Resulting in 100 * (6 + 6) = 1200.  - possible answer but more likely: long 64 bits but aligned to 32 bits - 4+4+4 = 12
                1600 bytes: 64 bit machine, the long is 8 bytes, char is 1 byte sizeof(struct Elem) =  16 bytes- but as 64 bit machine have to use two 8 byte words. Resulting in 100 * (8 + 8) = 1600. 
            %LOCAL_FILE%Ep4ntJAbb4n94rCUys5-sCXlO1A_qHTJLLNhp1VaSOwwGa6LU1mKBgnyYGPDlAocg7j39QT7M7vUoPh_zTSCQKRoMdoaB5ne8E74h4aC1Mc6_P7-HdZsak0E_be6VYwQ.png 
                The fundamental changed is converting the file that was created by a compiler using unaligned access to one that does not support unaligned access (no need for the added complexity when memory so abundant).  Also assumed little-endian.
                struct Elem { signed long val; char flags; } v[NITEMS];
char temp[5*NITEMS]; // Each byte in the 5 byte line is going to become it's own item
fread(file, 1, size_of(temp), temp); // First we read the file and get every byte as a char value in our temp array
for (int i; i<size_of(result); i++) {
	
	v[i].val = temp[5*i] | temp[5*i+1]<<8 | temp[5*i+2]<<16 | temp[5*i+3]<<24; // This line combines them using bitwise ors - consider 00001010 | 1010000, combines the values as the zeroes are ignored
	v[i].flags = temp[5*i+4]; // last value is an actual char as we want it
}
 We could simply have a flag to compute this for or not depending on if we're using this outdated format or not.
        
    Lecture 1
        Primitive Types in C
            What are the three primitive types?↔Numbers, Characters and pointers
            Are there primitives of composite types in C?↔NO
        What types of locals are supported, and what is implemented by a library
            Stack and static locals built in
            Heap implemented by library 
        Sizing of various types
            Char↔≥8 bits
            Int↔≥16 bits
            Float↔Single precision
            Double↔Double precision
        Type operators meaning
            unsigned↔Makes the value unsigned
            short↔Synonymous to short int, varies but is at least 16 bits
            long↔8 bytes
            const↔Informs the compiler that this value will not be changed, can still be changed using pointers though
            volatile↔Means the storage can change at anytime, compiler must check it every time it's used
        How do you specify different types of number in C?
            long↔123L
            float↔123e10F OR 12.5F
            double↔123e10
            octal↔053
            hex↔0x54
            
        Enumeration
            enum boolean {TRUE, FALSE};
enum override {TRUE=1, FALSE=5, T=5} One can define Booleans, they are indexed from 0 by default - but this can be overridden. 
            What items in a enum need to be distinct and what don't↔Names must be distinct but values need not be.
        Declaration
            Variables must be declared before they are definition, where definition means giving them storage. They can be declared, defined and initiated inline. int i = 5;
 char a, b, c;
            What's the different between Declaring, defining and initiating a variable?
                Declaring↔Telling the compiler this variable name will be used
                Defining↔Setting aside storage for a variable
                Initiating↔Putting a value in that variable
            What does the extern keyword do?↔It declares but doesn't define a variable, means the variable will be defined elsewhere - i.e. we allocate it no storage.
            What does the static keyword do in regards to
                Static variables in functions↔The variables keep their values between invocations
                Static global variables↔Static global variables are only seen in the file they're defined in
        ALL OPERATORS RETURN A RESULT INCLUDING ASSIGNMENT! int y;
 int x = (y = 3);
 // x = 3
        Type Conversion:
            Type conversion occurs when a binary operator takes in two elements with varying sizes. The operator will return a value with the same size as the larger of the two elements - e.g. short + int ⇒ int.
            However, narrowing is possible.int x = 1234;
char y;
y = x + 1;
// y is overflowed and is the last 8 bits of x+1, in this case -45 in denary.

// CASTING:
y = (char) 1234; 
        Expressions and statements
            What is an expression?↔An expression is a formula in which two or more operands are combined with an operator.
            What is a statement↔A statement is an Expression ended by a semicolon
            What type and value will this expression take on?int x;
short y;
x = 5, y = 3↔The expression has type short and value 3. (The rightmost expression)
        
        Arrays and strings
            Strings or arrays are created using this syntax: int x[10];
            Give 3 facts about string arrays
                They must end in "/o"
                No bounds checking is performed
                The compiler places them on a contiguous chunk of memory
            What does the following result in and why does it work? char x[] = "abc" "def";
 printf("%s", x);↔This results in abcdef, it works because string concatenation is done by default. "abc" and "def" are set to strings using double quotes.
        goto statement
            Works but shouldn't be used some_label:
 	printf("We used a goto")
 goto some_lable;
Intro to Computer Architecture
    Supervision 3
        %LOCAL_FILE%jf-UuofY17s1TjPGvIF8ZS9eJEGHmrfT6Dtk0fNmdAlw730xbsBUpcv3s28ChgFS2TM2cTESgdTonLNfsyKo82RUWXrUehVixXxz1GrdTSRgtEiPUrhlTh0PBrIs_iYQ.png
            Interrupts are caused be factors external to the programmers, and cause a context switch in which the kernel takes over and performs certain commands - this could be scheduling different operations to take place, in which case the operations stack space and registers are saved and a different operations starts work. These occur occasionally and we have no control over them, and cannot predict when they will occur.
            Environment calls are made by the programmer, and are used when the program needs to make use of an OS feature - such as allocating more memory, changing access level (generally decrease) or make use of IO. The kernel will take over, execute the relevant command and then return control back to the user space process. We can predict when these will occur and have control over them.
            Exceptions are also caused by factors instigated by the instruction stream, usually errors like division by zero. These cause the same sequence where we jump to some privileged piece of code & in this case we detect whether this can be recovered from or we simply have to core dump and abandon the program.
        2. 2008 Paper 6 Question 2 a, c
            %LOCAL_FILE%rSCemIkz1ebYPPCOr7SiSWifHvnyMGhn6yrZ9to0JpDF1YhzqazpR0aBclMH_lj0DloczVqRw5a47AgXdpLRkVsQ2IYDWR1NA60tCiDN30tXtTWFlVRF_z3ZGBMGkceV.png
                RISC stands for Reduced Instruction set Computer while CISC stands for Complex Instruction Set Computer. In practise RISC commands are fixed length, while CISC commands are variable length (1 to 15 bytes) this means RISC commands are faster to decode as we don't have to wait for all 15 bytes to arrive - additionally, when CISC commands are decoded they are broken down into many short RISC like commands that go into the pipeline. However, given the greater length of instructions, you can often write complicated instructions faster (by hand) in CISC than RISC.
                RISC tends towards making the common case fast (based on Amhdahl's law of diminishing returns of speedup) while CISC often pushes for specific domains of commands being made as fast as possible - e.g. adding a specific integer division command.
                RISC has less pre-packaged instructions than CISC, but more can be emulated using subroutines that are attached to many of the Instruction types - CISC attempts to handle a wide range of instructions out of the box while RISC prefers the architecture to be built upon for specific usages.
                CISC attempts to minimize the number of instructions per program, the logic being fetching instructions takes time - however the result is long instructions that take time to decode. RISC attempts instead to minimize the number of instructions available by default and thus the cycles per instruction (while still offering a good amount of functionality). 
            %LOCAL_FILE%WZL6Sadbb6pgxZVdsiw95Y6XfhAd9nu9lmRaLHS618gTWt-VuLb_R7g1HiKWz7yOgUknIgOPRjJLlg7Vwxiy8TUcGgV774gb4Jrk8dEUdGp5aOCrhIEphZe3_cRJUzTQ.png
                For an accumulator to run an instruction on two operands, it needs one to already be in it's accumulator and the other needs to be fetched from memory - so the operation is run on the value in the accumulator & the value fetched from memory, finally the result is placed on the accumulator. For stack operations we need both operations on the stack, and then the two are popped from the stack the operation is applied and the result is pushed back onto the stack. 
                In stack based we need to fetch the two values (if they're not already on the stack) and then apply the operation (which is a dense command, causing two pops, some arithmetic and a store) while for an accumulator we would need to fetch one value, apply the operation (which is less dense just one fetch, an operation and a store). Thus the stack is narrowly more dense.
        3. 2006 Paper 6 Question 2- part b:
            %LOCAL_FILE%ae32BTrZ-dV7nLJGOy84Vjb38Zcr5CHMvCizurI6cEumjHdLdYWRWpseLfZoW_MaM56457SlgWeI7KxpeVYNir2vq5LEvUElvflyby7X80EKYYOCUe7UrmzeTn-tZTHt.png
                One reason is that while transistor speed & density continues to increase, the speed of electrons in a wire cannot be increased through engineering effort - as CPU speed increases, the clock cycles taken for electrons to reach different cache levels continues to increase to our detriment (no useful work done in that time). 
                Another reason is the problems caused due to ever increasing parallelism, an increasing number of cores that need to share data between them requires the creation of shared caches - we then have the overhead of ensuring the values stored by different caches is coherent and we also have the increased time of accessing caches further away from us than the L1 cache. 
                We also continue to live in a world governed by the Von Neumann bottleneck where Instructions and Data flow along the same buses, and although we have separate L1 caches for Instructions and data this separation causes a slow-down when fetching from caches higher up the hierarchy. 
            %LOCAL_FILE%N2z2HfJ2xfd2fMm_8q2AY3g1spuGcmNGdYAqgGKe2nVAmUHSlooSzcnxnw8TonaSGIieAKfswunhEfudtuSIKlINozST-DoJR5sPNSWjVzu-MSWvPEqJNf2C2qgNQD2h.png
                Spatial locality: 
                We adapt our caches to take advantage of Spatial locality through architecture to improve cache hit rate, we do this by reading items around our desired value into a cache line in the event of a compulsory miss. This decrease in hit rate makes memory accesses (appear to be ) faster as we can often fetch the data we need from cache. Similarly, for temporal locality in that we make the choice to evict the Least Recently Used item from the cache - even employing a victim buffer to stop pathological misses. 
                We also employ a hierarchy of caches, where if one cache further down the hierarchy misses another is likely to hit - this decrease in misses results in faster reads. These SRAM caches are faster than DRAM, and are positioned closer to the core (L1 cache often being part of the core) resulting in FAST read speeds.
        %LOCAL_FILE%p03-uxf6e0ERGAz1XawaYCw7btJszLgaAdOl_9LioV5SIYV9M_mSybKikb1jwSu-tf5rClW2ZW2npscjSUTC6xHOmS0OB6fD3L2aYHFFG7HSDSV_X7KwBxpPJT9EcRJJ.png
            The TLB improves performance as it can decrease the number of RAM accesses made - TLB is close to the CPU and can typically be accessed faster than level 1 cache (because we need to determine the page to fetch very quickly, before destination register flushed), if we do get a TLB 'hit' then we don't need to consult the page table in main memory, instead the TLB serves up the address of the PTE directly which can then be fetched. 
        %LOCAL_FILE%DWS_DbhGbc1gfIswk9CSqZ45WLZrf9UPdQHZRIsWfvwQNEa4LUS-7LN3ZTONqD4l32WulBKQ6vEeh61WDP8bjVaW9FwQW7_R6km3bdSFypp-552Dfv-HHSbZvoiZpiAf.png
            When a program attempts to access memory outside of it's virtual address space, this will be detected as the PTE will not be in the application specific page table (or on disk) and a page fault will be thrown. Thus, an application cannot access memory managed by different applications.
            Additionally, virtual memory provides permissions on who can read, write & execute a file - whether or not a process is allowed to access a file is checked dynamically and if the process does not have the required permissions it is denied access. This is possible because the actual address is hidden from the process, and hardware can decide whether or not to supply the real address.
        %LOCAL_FILE%TsQ1TzHmnIZh5ASUJrgWmNdalT2Zw2lQ1gChLh3aBI1DCJ2z37OJ3-onlMT0sr1ZQdr9V4wufI_vCt25EgM4-6292xa0NpHZpbPp83Gp_7CcxJF5Um7CO1zm-6IVqKap.png  
            Processors, memories and interconnects.
        %LOCAL_FILE%F_LCdAJw_NvkBinFS1Gf3VgNHiTkHy8SR9hyIqFeWs7ZDgDljRLP2Yr9aHash8P-S1J4EgtjKeVXKMQdI-h1IghV1NsyUmNzs08g_0n9Cydxxupmz8HK-7obw5I7Mz0G.png
            If all run in parallel, the bottleneck is the slowest member - and as D is now doing 60% of the work in the time it takes to do 20% of the work, the speedup is 3x.
            %LOCAL_FILE%7iZx4ZweGdvOvzu5z_l70StyotqciorBogTQ6HIlyEXejW8oGySnXQQfZCUZhw2tU6rTH-01veBC5M7vS31eFkBzdRBLQ3ICvBuolW6SbLwQDIkIruoq87qCMzXkPvy5.png
                \frac{1}{0.15 + 0.2 * 0.5 + 0.05 + 0.6 * 0.\overline{3}} = 2 times speedup 
            %LOCAL_FILE%cY6QT7bsC9lo6AwrvC6bshytYsneI0Hw01nqLGFF74XMVHNoFR1aiMWmRvU9zDkm96sQ1QKHnzZ7l_0qY-sGa6ZgT3K9iWmc_KtliJetZvTqQNpWLw4OKchJdbPsoa7m.png
            Sequentially: If B & D are instant we have \frac{1}{0.15+0.05} = 5 times speedup
            Parallel: If B&D are instant, the bottleneck becomes A so 1/0.15 = 6.\overline{666} times speedup
        %LOCAL_FILE%KwpkuAQjVok9cAmJfeDWPSi6oD4n6EUgpULx4L_MeBp9qelxjRMHG8cqi8mnKxJhZpR7xXflxIOMJxLE9p6JdL5UdFgHo2D2rxvpU7_rh3ekNroZr1WgKZ1jJ9D46sr7.png
            %LOCAL_FILE%GROvBCPnzUMS58cVrT3Cj0AK5Lml2PHMrLGzq7qHqdvCFxTqpSpzlcU9qywrd8F_wkJMNTLhCzEn1T8NNIcKIouGy01kJyUJoGh3GEL2su4bnhr8puZW-1W_Dl_AQNr9.png Drawn by hand.
            The DRAM cell is a method of storing data in a mediumly fast, volatile and cheap manner. There are two methods of operation
                Reading From Dram - In which the Word line is set to high, and the Bit line is set to some 'floating' value between low & high (say 1.5v) then the capacitor either outputs to the bitline if it was holding charge (a logical one) causing a voltage of 1.5V + \delta V  to be read at the sensor below OR if the capacitor contained no charge (a logical zero) and gets charged by the voltage causing a voltage of 1.5V - \delta V to be read at the sensor.
                Writing to DRAM - In which the word line is set to high for 1 and low for zero depending on what we wish to input, if zero then the capacitor will expunge it's value to the BL - if 1 then the capacitor will either charge up or remain the same. 
            The capacitor ambiently leaks the charge stored within it over time, thus we need to periodically (every ~64 ms) read the values held in the DRAM cells and write those same values back to them - recharging the capacitors who held 1s.
        %LOCAL_FILE%fq2nVqfEASe2rnCEpIIB2dL4wn9JaIZo1qx0FAHCetvWKlRNXoDp8gYk2ux0o77RDN4Ro3VigSt_yze77Jh-2UXANVxx-u99AV8ctH75RxR8mAR8fAQ7hfMj9-E7VnsX.png
            Because a bank that's just been accessed may be in the middle of refreshing the values stored in the arrays (since reading from the arrays is a destructive process), we can mitigate the time taken waiting for the refresh to complete by accessing a different bank first.
            %LOCAL_FILE%Mmrjr3lJSwf2HGb3SEYjiUKgGoC_g_F0kimsPV6k_2xFRkZ0A6igeyMEd_tqeBSKrmR3DI-gBBzMiFEN-YNgHIqIGHyhLEn7QBHcYdOAJUdv3cPHOrQxvhGYKxH_Tc3Y.png
                If we're in a Open-Page scheme, wherein the sense amplifiers are not reset by automatically after reading a bank, we can simply read the data stored in the amplifiers without having to fetch the data again - this saves time, and thus is beneficial.
        %LOCAL_FILE%dS9yzQZbCPSxlszPvk9vMGJBUhD26m5AgQr5_qloo3ztSp3LGRDaNwWZmAu5rWuUwGxgHB6Wz2wVfmoQ_klivUNkYoj3yVhVS2ntTuQ56YwIIXr_d_NN5nYWoa2Ygde1.png
            The row access enables the transistor, allowing the transistor to emit or charge and the column access then allows us to read the value detected by the voltage sensor.
        %LOCAL_FILE%hFCKZctPGzZ4iaV3m296g8bI9xVITUwrowz8mkSRk3vgg9FevcflRk0umaT_oB4dNRsSCktLVAOykLSWqm8VGBeRn6WOfTASMY-cjZIW_GH3-s6e_sg6EzAJNStZnoZk.png
            Single Instruction Single Data: A simple single core, sequential processor who reads instructions one at a time to process data serially.
            Multiple Instruction Single Data: used in systems designed to cater for robustness, i.e. when we have multiple CPUs voting over results from data inputs such as rockets. This voting process makes correctness more likely.
            Single Instruction Multiple Data: Vector operations where a single operation is applied to a host of datapoints, this is efficient and low energy.
            Multiple Instruction Multiple Data: The commonly used multi-core parallel processors of today, wherein multiple cores operate different instructions on different data at once.
            A multicore process with a short-vector instruction set would fit within MIMD, this highlights the breakdown of Flynn's taxonomy for use today - as almost all use cases are multi-core.
        %LOCAL_FILE%bkqqQkjhlzBDEmrYDKbLncwWBjl4nfafP6NZ5sIS9AZ11TZe8Uff_d-RhB6y5uHGDwUSadY6o3Q8BnforMTm7i-C8f93mPgyHG0tr96jZocEoD4J4RuIboucDnA7H84A.png 
            Amdahl's laws concerns computation of fixed instruction size, evaluating the speedup as number of cores increases. Namely: \text{speedup}(n) = \frac{1}{B+\frac{1-B}{n}}  Where B is the serial proportion of the code, and n is the number of cores. It makes clear that for a large speedup, one requires a significant process of the code to be parallel.  As we increase the number of cores, the speedup becomes more significant.
            Gustafson's law concerns computation of a fixed time, wherein the volume of the problem varies. For such a problem: \text{speedup}(n)= n - (n-1)B. This law explains that even when problem volume is scaled up, the parallel part still gains increasing speedup but the relationship becomes linear rather than sigmoidal.
        %LOCAL_FILE%PCW_aIZZjTdu7vAWNNYXs1SCIspDtj-FJfICVMl0rMpOguJqmOjQAEti0fEDQYBLgNVutmTfoEN_9h16fi3c04prxuHmgEvO2WYtsVu_GYawovf9i1GV5UnOeXZPrjF6.png  
            https://remnote-user-data.s3.amazonaws.com/vvC6eIv17JNOfYPFCIjZjixAGSJU0u7t0rYbRD6Rjln0rhC2Oil3z1gmJl4Jhe0BVV6yG4UwTq14ZcXWFePWclF7RrTBlfNYZlYz7ynWsW1exuY4nJPZ2JKwbvmzQqvw.png 
            DRAM is organised in a hierarchical structure, Devices are combined into ranks who act together (their results are concatenated) where ranks are independent. In devices within ranks, the data is gathered from a bank (only one at a time), the bank gets its output from individual DRAM cells concatenated into arrays. In this case we have 2 devices where each of their banks has 12 arrays,  resulting in a possible 24 bits.
            I don't understand the below excerpt from a slide, if we have 512 columns why only 16 bits? 
            %LOCAL_FILE%Ha_YYFQAifCByvjCuocyUgozmMrufyZe2HmoWyQEwRCgg7NHMoS_ALKGvZj-DJDxn1TX0BwBVqXxTaHSps4j4sMu91wuKIfxHaKz4Qls6sDc6V3l_zkfO5d2bMIAM2NV.png 
        %LOCAL_FILE%s5cRytkd_TEPs8P-ywDIKKBH2Wgremx2bFpd2YLXnDvfbbtZqoanRSfNgVaHxKxPnr0tTxE7gLemr26vugi0EUihzKgW1ZIFwSYsSZL6tQQGsPcLNK3bPuZbtziODcBQ.png
            Open Page: Results stored in the Voltage-Sensors are not flushed by default when results from a bank is read. This benefits processes which are likely to access the same locality repeatedly, as we don't need to re-fetch the values.
            Closed Page: Stored results are flushed after reading. This benefits processes which are likely to access differing localities, as we flush the buffers before the new locality fetch is required.
        %LOCAL_FILE%QGN4oxkVDnrIU52JdgYHePNfoYhX_cMCGGZx5x4ocIjBwWGWPZczL9Hu_fx7unVcBnOZKJ2ZaVHfMWoaES-nb_R6rNO5qRRfngwCWRdkyE-hr_G3tCaw6zkzv9Y7aKXM.png
            Shared memory is a useful concept as it gives an easy to conceptualize method for threads to act in unison. It's disadvantage is in the effort that must be done to ensure the threads are reading the same values, in ensuring this uniformity of values we have to implement some serial sections - e.g. the MSI Protocol which forces cores to wait for each other in order to communicate updated results. If this necessary aspect of data synchronisation is not upheld we can have data overwritten, incorrect calculations (based on incorrect input) and general undefined behaviour.
        %LOCAL_FILE%bFXtw7UfA25j4KUr2DJJUcsjAxscfdX-35LYuYejGdzHRyY4kdhrOC5d503-rnVmjiQ4u-YYuIXP2PyuvFRetOJZyw3b5HXa-7V6b-dvPmqK7tbhGDfigV1Ik-pNNSgu.png
            %LOCAL_FILE%3JlfPD9eb-Go6IkvwXCQJQwV-gJPBgRjyHUFwSqajFROt9wz7C5lEwuI9s6PhgKxsJN2TbmSp8UmX3Q2ipWLea8CoO8j_MBJioz6Qxi5oLFuz6gVNfD3fNCw9HV1TrDA.png %LOCAL_FILE%TS1Nt5YEigxRgtW21hBSS8kqzIL47DstMDt2jtVksbpi4U7ScTIEqG8YB900s-M9LoSntSCNOiL76knagGsjSa7g6yiDvxz6CXDHKJwVDWsiSS2aE5s8uyS26E6bBvvy.png %LOCAL_FILE%OEtTI8My8XSioVO0Fju6T0ioO0N3yT_0PgAM0j28ogNebOiJNjON2n_zZxQ_TKlC_LeRDwFwuPpFVxvaaZcdUOLvq_kQ_i8hdoKStO-ZXyuie8nEygfTeaPu5w8JrpNQ.png
            Method 1 has the benefit of quick communication of values between all cores in the fast L2 cache, each core has it's own fast private cache and not a huge amount of data is repeated as only one cache holds repeated data. However, the central bus could have a lot of traffic slowing down processing and a lot of effort would have to be done to ensure consistency between caches.
            Method 2 has the benefit of reducing traffic on individual buses by spreading it across two levels, additionally every pair of cores has a fast method of communicating. It has the downside that for the two pairs to exchange values they have to use very slow memory. Additionally this method makes bus sniffing more complex as the L2 cache would have to communicate the sniffed results to the L1 caches.
            Method 3 has the benefit of expanded private use space for the cores, reducing the chance of having to resort to memory. Additionally, it is more straight forward for cores to communicate via bus sniffing than in method 2 as the private caches & the public cache are clearly separated. However, this results in more repeated values being stored needlessly (in L2 cache) and a lot of processing has to be done before it's decided a value is missing (search L1 cache, search L2 cache, send a message to search L3 at which point we have to wait for the other caches to run a check after sniffing the bus then finally we check L3 and at last we have to resort to memory).
        %LOCAL_FILE%ALKU8hSSW9mdiBjiPYuDGLsx0Yl4N1tAyPEOjyemUzf0V5Qnp09h7t0pF8FsR4kWqM3fsbMiT7BPGslH8kAKeBK4w4aQG8BduX9nUZ36TEHQYmUwr466QGue4LicVYun.png  
            Inclusive: All values stored in smaller caches, will be stored in larger caches.
            Exclusive: Values will only be stored in a single cache.
            NINE: Values in smaller caches may be stored in larger caches, though there is no guarantee. 
            By smaller caches I mean caches lower in the hierarchy & larger means the opposite.
        %LOCAL_FILE%gnGeaQmtcoNy5ZhjROBvVzAwXMfdjSG3C5kxJSC1R9dsEAKRSGo7La2Kh__bK0IqcThPgZFWL6EVgJ40BaDlG0R15A5Z3ygZm4ecv5AXzMAgqGhykj_kpllr7QCOI1-i.png
            Under the MSI cache coherence protocol, caches can be in 3 separate modes:
                \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {I}}} mode, where the cache-line is marked as invalid - i.e. we act as if we don't have it stored (but don't actually take the action of removing it, it will be replaced). If we wish to read the data we must issue a BusWrite call, read the value from memory and upgrade to \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {S}}}.  If we wish to read & write values we issue a readWriteX call, read the value from memory and upgrade to \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {W}}}. 
                \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {S}}} mode, where we store the value as a reader NOT as a writer. If we sniff a BusWriteX call, we must become invalid - If we decide to want to write to the value, we issue a BusWriteX call and update our value from memory before upgrading to \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {W}}} mode.
                \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {W}}} mode, where we are the exclusive holder of the value and can read or write to it. If we detect a BusRead call, we must write our value to memory and downgrade to \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {S}}}, because a cache in \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {W}}} mode must be the only holder of the value. If we detect a BusReadX call, we do the same but the down \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {I}}}.
            This ensures all caches hold the same values, as only one cache can hold a value that's being written to - after which any reads to that value are synchronized from memory.
        %LOCAL_FILE%zvEuBYjnX_eiFcabqGcgI7WQeE3cwisMx6wIcGbV_ZNC8yuXiSQ_QbKt9YoH9AGmPE3nAK5Zxw984u6aeLkgmviAX7qWDeEpq_fkZf5I3060QNzC3RfrUYed19J4R8Vn.png
            You first write your data to memory (Wasn't really explained how the precedence works here, how does the other cache know to wait? And for how long?) then transition to S state.
            %LOCAL_FILE%FLBiBIzm4AwpDjF5bcxLv5v2J-QpDrRQiIZ3-U4ywuFOuLlp5wtq5620H_U4i83aERdqZOCaCjqbyp3O9n6rhEtsONWsDM8eFJF06RDYT_sUUjj-IBRcLj9OkqSubU13.png
                You do nothing. One could extend MSI to send your value along the bus as that would be faster than fetching from memory.
            %LOCAL_FILE%esM97s0emvnAHg5JhST0ezKtEWoXS0lquJCV3hWl-OV8mfsa04RPXrXaPXm8stTTTnWe9l71sxFURzzUkrG8nCc1oFmH-FUIcBCmdgm5S33WwNah0wEdFDIEViBeOWb7.png
                You do nothing.
        21. 2018 Paper 5 Question 3, part b.  
            %LOCAL_FILE%7bgda-H3OLiUSJ063JyXJ8hE_s3S8ODxBRThEtdRTcAvpZBsXTcvLesI5jMCnQHKsNqaZMBGHWC-sO4GO7NUTejpenn2Fduf-I0DaX3OMVlRSIlDYf-x0G3Pi95ccwxd.png
                Cache coherence ensures values written in one cache are seen by others, while memory consistency attempts to ensure the ordering between reads & writes doesn't cause improper behaviours.
            %LOCAL_FILE%5sIceteoEqGQoCKvfF8ey7aWgLHquJ87-RSGWsI1jBVqxzH51ZjWAVVN0bXAsIYoA_Ag7Br1dFR-YusLhlkFbHuah8oaUtQEJCJiubLqCynCqx00PrnKEH8ooq3Xj1nr.png
                https://remnote-user-data.s3.amazonaws.com/_ZQbCuIDZz0b7M3HkNsf6nTPaV3HqYI82Jpr810QCnTppzPYXgokn6juAkBg-3vUXrYgXirk_6iXKppd_Mhb9RyCETtt1mvUpix-lqgAyYxW7b7yi1XLPBs6BlwQ5ojh.png 
            %LOCAL_FILE%PDJPOQqzMhDtFwmkSxYgzwPaNArVL2To06Gd2Vj1BVTOFgrKjfF4Gu5z-ubiAH-T6NP3JpuUd5xvQMyEbTBpmWlK51W_u4WPtiEYpMMvuG31e8qJCvjsgykieZJspy2l.png
                %LOCAL_FILE%8a68vLHMj-2N5qVbfqbGSlhp4lOGjEmuoQ2L7vprEQu9SHKgVq2P3EcR6RU2WcKkQ0eLJ6EFSaQ03JFc7mEAqSXjaDufcacKCBFWAH9hQBPCX0PmB-Uwqi3unoKivYn1.png 
            %LOCAL_FILE%ZjGTjLTqjWMKO282PNdtrnKWSu7g-SzttsWD3jlULbBncXDZd2j-ap8qG2JDUwTpU4FeoSyCizFVhpO1ex3ayr4ZgjgLSTh7c66SzzkqfzNG0UNBqRSgxTp89T4bS22J.png  
                Passing from the cache to other caches is faster than fetching the values from memory.
                If we're in the M state and another thread issues a read, we transition to the O state - after which if we wish to transition back to M we no longer have to process an extra memory transaction - we know we have the most up to date version.
        22. Summarize the main message from Lecture 9 in 1-3 sentences?  
            There exist lots of methods of hardware support for operating systems. Different processors have different implementations. 
        24. Summarize the main message from Lecture 10 in 1-3 sentences? 
            CHERI protects from improper memory accesses by explicitly linking bounds measurements to processors. Stack based ISAs can have very compact instructions.
        25. Summarize the main message from Lecture 11 in 1-3 sentences? 
            SoCs come in all shapes and sizes, designed for specific domains (whatever that may be). Lots of opportunities for customization within an SoC.
        26. Summarize the main message from Lecture 12 in 1-3 sentences? 
            Multicore processors provide MIMD parallelism. Using shared memory paradigm intuitive for programmers, but propagating writes to data can be hard.
        
    Supervision 2
        %LOCAL_FILE%ujcL6v4TPBhwrAcC9U-DaheF_SZq6DZo4qsSSIJ_y5ZWZw2da0KoPhAc-IC7-96muSFQa3iDtOcZFGvVILKMMqKF8thbd7P8VPDM5HDH-kPnflEeHP0Iu8ue1pvtHVLD.png 
            An immediate is a value that does not need to be fetched from a register or memory, that is it represents a value directly rather than an address of that value. The only computation that needs to be computed is extracting the immediate, involving sign extending.
            %LOCAL_FILE%Bp04bTw83jaJ9i3BiMwlJkcbV2zGungrBbTZkiONosl6IIUNJIY44FN8t0FxPPXLej7ilSAiBcFsEAvBObuzn9CT5dsZGLDwxryh2ouy-blZW25weMm4z1xKCbkLAbPx.png 
            AUIPC rd -1000; // rf[rd] = PC - 1000
BEQI x5 x4 100; // if (x5==x4) PC = PC+100;
ADDI x5, x0, -5; // This stores -5 in x5.
            %LOCAL_FILE%TeoYGLDUjRa7z5q-5Sn9A1w67YUAcm-CNL2WnXCz09VzCz1Y1eAmoGrncZiSqTrcIBx5hleeoQ8KLbe3CDOo3AHBFzl_korJ4nzpcqvhA0TUp8CJZVWmEYJoi97vraU9.png 
                Because immediates take up space that can otherwise be devoted to funct3 & funct7 fields, which extend the opcode space and allow for thousands of more instructions. R-type instructions contain no immediate and thus have a funct3 & funct7 field, while I-type uses an immediate and thus only have a funct3 field - 6 bits less opcode control results in 2^6 times less possible instructions you can perform. The reason in general is that register-to-register instructions require only 5-bit register addresses, while immediates often need to be longer than 5 bits.
        %LOCAL_FILE%TYJ8_gFTsVYCGehDl-wZLT7Wk-FpYNQne44DtqUNd_0Py1Jv5aJylbRJ4IaeaoUsJeyOuexaycxFPcKAOd--yodGGGFhyijXf2dy-_R2Ffu7kqihZ0lBTMsOzn79_bnA.png 
            ISA level simulations are programs that interpret machine code and run said machine code in a higher level language, they are often "bare metal" (they don't run an OS) and also ignore factors like varying clock speed & interrupts from other programs. An example would be SPIKE which is the ISA simulator for RISC-V.
        %LOCAL_FILE%J7L7np4Dby_9cLgtwDrxVP_rCQvCdpja652qu3tqA_9_zf00zFxmlz5Y1Y91Q67pxNMadtl_1T3b4jNQSVQnK436sCLkhjdhDGUyd30sLe-CNjCeLVA7cAOgRoHtkRP5.png
            The data path is the actual flow of information (be that immediates, addresses or register values) while the control path is the signal that controls the flow of information. They tend to flow together because certain control decision can only be made once the instruction has been decoded, and the data values can be examined - for example, the values of the registers in a BEQ command control the control-flag the ALU emits. 
        
            %LOCAL_FILE%hUXO8mnviRXtruya41BN9kijy81S75f1t4yhCe2D_F-K5Xcfy00i2Whwvtj99jatVA46g22xuuDOv_P9KBet5IdEGrYBfqtoWorSMisBVdFzpaJ7X8lrnVRKxaSQIBiq.png 
                Performance can mean many things, in terms of CPU we have Clock speed, Cycles per instruction, Cache fetch speed, Number of cores, Individual core speed etc. A benchmark could be developed to measure any one of those metrics, but there is no overall metric of performance.
                Performance varies as outside conditions vary, a benchmark could be run when the CPU or other components are already hot or when the room is abnormally cold, which can lead to incorrect predictions when the system operates outside those conditions.
                Performance can change due to conditions inside the system. for a CPU we could have lots of OS interrupts during the benchmark or lots of other processes being used that hog resources - making the benchmark innacurate for normal operation.
                    Performance can vary for the use-case, for example a CPU can favour high core frequency vs more parallel cores - where the higher core frequency system could do better in a non-parallel use-case the system with more cores would do better on tasks like video-decode that are suited for parallelism.
            %LOCAL_FILE%DHrxwCXTlVqaXyaqr4YJu0IbL7nGyZFwnlKjLGPDeZMf46R9iyOaHoTqqmMn5aIdtOCMtTu4BuB1UhY87LdrVnJM-IsjP_GRWLZyIzpINdk3GP7_nPBvnq869JBRisYq.png
                Instructions per second varies wildly depending on the program used, some instructions take more cycles than others - thus, a computer running software with more lengthy instructions would inherently be considered slower by this metric.
                Two computers using identical hardware but with different ISAs would result in different measurements for performance, consider CISC vs RISC - a given CISC instruction could encode multiple RISC instructions into 1, and even for the computations would thus compute less instructions. 
                Again, this measurement would vary with the conditions of the two computers - a higher temperature computer would likely throttle and result in a lower clock speed, leading to less computations being computed per second.
                Under Von-Neumann architecture, much of the time spent is data fetching & setting - and while we wait we can simply perform polling instructions. Thus, the most critical factor for performance is not captured by this metric as we can compute a load and then repeatedly poll to see if the data has arrived - this will result in a high instructions per second although nothing useful is calculated.
            %LOCAL_FILE%oJoZsvB6MsmJwvU8JTK0mCpOZwXsO74aGqk67D3Kj0BtNaS0ZZ3ub3GzMCUEzROzFokhWOllo9mU1Ht2Rdzvp0omRBRQ756XZfOpMBsc7tz53YwTOfD_naTefMAxWsxm.png
                Peak performance is only useful as an upper bound on performance, can't tell you anything about the common case.
                Common case can vary wildy from the peak performance.
                The difference between peak performance and common case performance will vary wildly from program to program.
                A computer with much higher peak performance may complete a task slower than a computer with a higher average case performance as the peak performance cannot be continuously upheld (temperature throttling, data starvation etc.). 
            %LOCAL_FILE%jazhr05nvPg_iVt0LtIBeOZTQuMCyOcdNOviCvlVf8W-KmER0h5cIj1Gy4QVqy27RU2Mf6u2qwK0glP-aOVH4hOEofmgDjSUeOpeV9vqStLxou_cxkEC8iLi0bDwCb7X.png 
                Some applications are orthogonal, e.g. best performance and lowest price, meaning you can't have both. 
                General purpose CPUs are usually slower than FPGAs computing the same algorithm, due to the allowance for greater parallelism. Thus, serving multiple purposes results in slower speeds. This similarly goes against making the common case fast.
                The design is always a trade of between performance, energy use and area - these trade offs cannot be thrown aside, as some applications want small area & energy use with a lower requirement for performance (phones) while some require high energy & performance resulting in larger areas (compute servers). So no ideal processor can be developed for all applications. 
            %LOCAL_FILE%Dpic-8nsa3ExxGcMZkvAmhuAwT4wbYgoy6GScYpcugSf2Dc3x02taF2oTvhMwuRzlhJIHprRr0jXl8fTvSyvfLNlzpcchdPBLJhtQxoF8eFn_67EheC2FgQxVtZv5h56.png 
                While longer pipelines can allow for higher clock speeds (less work per stage, allows for higher clock speed), there are associated trade offs. 
                Diminishing returns from performance increases, Amdahl's law - if an operation time is barely used, adding a pipeline stage can decrease performance as time needs to be spent waiting for that stage to complete while a fast operation may be waiting.
                Additionally, resolution of control and data hazards increases time taken (we need to fetch the correct instructions and flush state causing bubbles in the pipeline). Mis predicted paths have a greater effect on longer pipelines - Got this from notes, but don't really understand why it's the case wouldn't it be the same? 
        %LOCAL_FILE%HrcWNFR_E2HESaJNxYOc7XrcKLX7vDuEmKx5F6q0Ap4tlz7gLbVz46xUZcZFdvFK8tcDtriByoZ1znxvUBVKmXSTWuBOSG3FdtkBC2qTP4jjZxoijgI4vGt2mfKPaIf8.png 
            No, more complex pipelines can have more stages - the 5 stages is what RISC-V uses and can be modelled as an abstraction for more complex pipelines. For example, CISC pipelines can have a second decode stage where longer instructions need to be further examined but this would be put under the umbrella of DI.
        
            %LOCAL_FILE%3Uy5_FV4LoCFD69iUHoN_NLwWYbbpIANN4-7dl7wqsadCZTdSr4xD-zcEHF0qNv23_lY9jodR7L4rdJ2zgKOFGKC_LU5288COxmEU7NaLzdlxbKOIQJjtXzlziqGeyJz.png 
                From a birds eye view, data & control hazards are a necessary feature of parallelism - if we are to run sequential programs concurrently we are going to have instructions which rely on the results of a currently executing instruction. More specifically, data hazards are when an instruction needs one or more of the items that are to be "written-back" by an executing instruction as it's operand/s. This hazard is caused by operations wanting to use transformed data. Control hazards are caused when conditional branch instructions need to wait for there condition to be evaluated before we can update the program counter and fetch the next instruction. This can be mitigated by trying to predict the outcome of the branch, and rolling back if the guess was incorrect.
            %LOCAL_FILE%p7VHTCavrC_gDoAR3T0SQE-x2lZ_lO0XFya-WFeB8v6O2zvBO6s58odYJIEVdVIHLepDOIV3hatj8h8JZdGnpm9wZjd81eZD7Yeh9vuEx_fffZQ2FUQWL9_RZI45vFYL.png 
                Can we go through this question - don't know what it's asking 
            %LOCAL_FILE%fzxrPBHAxm4NkUXqKIIy3xlM1J658qmhBqCZWn75WRdHYpWMNfBPKWKdz_GVO_imF2vt0XpRlyQLUNFiCsbEkHOa2w2Cam9dfqed3WxBz8dZ6CMFjiYVFaTwaSLC4WdA.png 
                Pipeline A:
                    Data hazards cannot happen in pipeline A, because once we've decoded the instruction the previous instruction has concluded and the register has been updated so when we fetch the register the results will be correct.
                Pipeline B:
                    We can introduce forwarding from end of the 3rd stage of the 1st instruction to the beginning of the 3rd stage of the 2nd instruction:
                        %LOCAL_FILE%I_OZbM7nhza2JNMJh9i2jbqw_hqkjESg5n2cqMqBOB5aHntnRGYE-eWc_xiuCoxd86If-izdy5APWDfFTaUwprErMbypg7j-UDXOSqc8QYz1J78Ht_rHpij5uhmKKGSh.png 
                    This forwarding would overwrite the values in the registers if they were changed, for example:  ADD x1, x5, x6;
 SUB x2, x1, x6;
                    We would have to feed the updated value of x1 from the 1st instruction to the second.
                Pipeline C:
                    This can be resolved using standard forwarding if this is an R-type instruction, where the result of the execution of the first instruction is fed into the operands for the second execution stage:
                    %LOCAL_FILE%2X9HLoPP_olxS3qMxdKywBN3qrUQgBv4Kf1gqfUKzKNf0TriTV0CpKUE2BvwaQrnlAVIwM7SmYywV8nKXZauyItrAKNbnAdUzyMExe8igbYJHystbNYxCkDFFvGOvzwU.png 
                    HOWEVER, if our first instruction is a load that the second instruction relies upon we will have to have a bubble and use forwarding from Memory to execution.
            %LOCAL_FILE%1iYR9GJRtRBen6sgG7ruEOkFMcgFH1yGk9-keOL-hwq0rO_NbQwSkqMITqdFblngsP9H1TSsdhYrqlPyp7_pYQ3nO1B8moL3fvPWS0_tSq2TYDbf0Sl4P_D2up8CNA64.png 
                Exceptions do introduce control hazards, as they result in the pipeline being flushed. For a precise exception, the exception is recoverable and this will simply result in bubbles in the pipeline according to the number of clock cycles the exception took to resolve. However, for an imprecise exception we cannot recover and must simply exit.
            %LOCAL_FILE%rj3My9YwE1oxjg_TS7tLeqXizhP0fVSeBZvGTt2ve4bmbMWPQx7skKXtO1nEQyuIwAIIMTOB6YrXpcvSAXWF2iVxhwW8_c-mpR3DUg3SjHZGkZiofmio1q8VIZBtSeVB.png 
                Interrupts do not introduce a control hazard, as the entire pipeline would be pushed onto the stack in the event of an interrupt and resumed after the interrupt was handled. We never add incorrect instructions to the pipeline while the interrupt is running.
        
            %LOCAL_FILE%E7o0ebzVnw8np-AE1bfqtq4SmA7zbpTWTm-KLVnaK_u4T80CWJvAXgZpbBOgPh4J56uQ4qBtn1GzFJSpo6w6QmNmH-NJvJhJvQJ09P40jKqdpBMgVJjx2UR11VtZah4J.png
                Branch prediction is when the processor does not wait for the arithmetic operation section of a conditional branch to be executed, instead we make a prediction about what leaf of the branch we'll follow and roll back if we are found to be incorrect. This can avoid control hazards (if it predicts correctly more often than incorrectly) as we don't begin fetching items while we wait for the branch to execute, instead we instantly make our prediction and update the PC resulting in less instructions needing to be flushed.
        
            %LOCAL_FILE%E3_cpU7FhIMFMosx0KRu0eo_OaU7N78GMgNKn6Z4mK05y9ZFcbgrbb_xNb5vFRQhmDFCxO9uEGx9vqsbmy8p0InfYnVHSLIG032U0pjFECdo8YqrRwhM5RB2uJCC_tJB.png 
                This is contents of lecture 9 which has not occurred yet.
        
            %LOCAL_FILE%XqoBtTygBlIN6uHJnGQFh8I3qcG5IFvD87VmbkDnkztk5tUxtfQOmQ40ltDXkkhsoRbKPujZEQQmJ1r7D0-xwc8AfWKfENyGLKqnF8dNeNZPx3Hzb5rcMjgZsBNdVUPU.png 
                Memory latency is the amount of time it takes for memory to arrive after being fetched, while memory bandwidth is the amount of memory that can be fetched within a certain amount of time (generally a second but sometimes 1 clock cycle).
            %LOCAL_FILE%4MbwB78Gg0rz54Ex-HTTS495wkzxAFFrId7boZPkYTW-NPvTw84Qy-53SoNYxxl69wJs1_1j2r8vQFsyMWfBPaiv3mkT0CgYqPKrqdf5aOTBKXc-hPD3KD_pC3UhqCiU.png
                Exceptions are caused by processes in the CPU, whether that be a division by zero error or a syscall, while interrupts are caused by an outside I/O controller like the OS. Interrupts require the registers to be put on the stack until the handler is completed, while exceptions can cause irrecoverable flushes of register files.
            %LOCAL_FILE%64IDOmAG3--v5FSz06UBv19TM9Ten3RMit5FpFJrCZQedy_Pgck5FP-1510pdgGPPtl5qPy-9vTc09QwuPVdvnZrrzZiJBbbQO3Ao__BU6FBh4ye4wZ_zhLPMo43xFM7.png 
                The programming model of a flat linear address space is the most intuitive and easiest to reason with model, we assume the programmer doesn't need to know the fine details and abstract them away. The CPU is then tasked with making this intuitive process fast. A hierarchy of memories is used because the speed of light is slow (and because very fast memory is expensive) , calls to fetch data from DRAM can take hundreds of cycles so we position fast caches nearby the CPU that can be polled very quickly. Only if we miss our poll do we need to step up the hierarchy and fetch data from DRAM or potentially even HDDs. This is also a result of the Turing tax, as most of the time is spent fetching and manipulating data rather than performing computation.   
            %LOCAL_FILE%CjzrJJe0a3jCVrKSznbuZBlZa6b775Anpio3P0U3NAmDXWuekow4eGuAqN7z0olkCfSAxcq0RR6HeaMfRf00bPhCXCBf0EgKikgvbJN88Rgdaj0FmQZeFT_L1NBYBygS.png 
                A direct mapped cache consists of many cache lines that contains data that was spatially-bunched on memory when it was fetched - each line is referred to by a unique hash which is usually some combination of it's index, it's tag and the word to be fetched. A set associative cache is a large collection of direct caches, where all direct caches are polled simultaneously to find data. In direct-mapped we have no choice of which cache-line is replaced, as it is built into the memory address. In Set-associative we have certain options, such as least-recently used, not last used and random. 
        
            %LOCAL_FILE%Ayul6TLMRCoxjIc4cAeVdla03F32FFZBHs2WOkxOm2JoUTWhDDDPHb8vr82sLaKnJBgFSvWweZFPG13haPBzu_i3HkOIMibSi3Dl2xp5QEaUlmmq0CIc6xsAEWPCUWJZ.png 
                In control-flow machines, values need to be accessed and consumed quickly to allow for the progression of the program - as later instructions will require the results garnered from the consumption of the aforementioned value. Thus, if the memory access latency is high the program cannot proceed - this lack of progress while waiting explains the sensitivity.
            %LOCAL_FILE%6XbZtfmhU9YoHSaiiAAH83hNSuVF-1TFe30ZdN-KZp1KnKiYdasyjpow9irpDz__J0zagsIWnV0Khhi2GcNVi-DMhcImhRCB06PwBVLuNLgsTvXFz3zIms49CxjvSPPJ.png
                Temporal and spatial properties of data access patterns are exploited. Spatial: if a certain piece of data is accessed, it is likely that nearby data is going to be accessed soon. Thus, we cache requested data and data spatially close to it (in the linear memory) on a miss. Temporal: If a piece of data has recently been accessed, it is likely it will be accessed again soon. Thus, don't just flush data after it's been delivered to the CPU - cache it for reuse.  
            %LOCAL_FILE%BubVqInmQH73h3cTr6eb56u3QrOMDUy5kFkJzxcyS4I_rHKgLZAQl5X7WiVh5fbXXb0d-BcUAOC62OoAj4WKHekVCLk6dpp5CjkD76PMcil9bcVpM3nVLQdjz19D5dBH.png  
                Direct-mapped caches have no choice as it is governed by the address of the item. However, set associative can use least recently used (hard to manage for large sets), not most recently used (easier to manage) or random (easy to implement and around same performance as LRU). They might also implement a victim buffer, to avoid "pathological misses" 
            %LOCAL_FILE%x42S6Ceps_2HqEL8DCX2MNjn47hCcBIv9BnoibcAplrSNuL_RqflsWNePHiY5N5eqrUFDV2Wm8SpJfcq6qwtGCjYEhto6W3BFUAt7d3m4rdWi9Mim0FhrzSw8TvhPmWe.png
                Write back & Write through. Write through is where you keep a write buffer, when a data-write hits update the item in the cache and add it to the write buffer. Only stalls if the write buffer is full, in which case memory needs to be synced. Write back is where we mark updated items as dirty on data-write hits, then when we attempt to remove the item and it's dirty then we must sync memory. Both of these are methods of keeping memory and caches synchronised while keeping the CPU occupied with computation rather than memory management (as much as possible).  
        %LOCAL_FILE%tSOQjmd979Wel0vDmCZMaa4dj7gHcSLmRnHZGNq-uA1Gb4DFVOomMrCrDVhBVXvV9Cxt50j0SrgvZUOg3hsJqdi3K1aOOmmrVFslMiHiQztfSseQ-kh8ZOlfQIHFQgdf.png
            Cache miss rate is the rate at which we poll the cache for a certain value, and it's not there - meaning we have to fetch the data from higher up in the memory hierarchy, if that's DRAM then hundreds of clock cycles can be spent waiting for memory to arrive rather than the 1 or 2 for the data to come from cache. If our cache miss rate is very high then we are constantly accessing from DRAM, resulting in huge slowdown of our code as we wait for memory to be shipped over to operate on.
        %LOCAL_FILE%neVpREEPM9QEQh0eE3YLO6iHU2MOTJ8UiE2T7JCPv6ylofruCHwLnLNZxJUANvvPIw5kajdAq7yl4xxj7T7OJgd_Pjuc4SmLGKzDApXwbEKWsi1JFjIsiPUVq-HFBqUE.png
            Single sections of the pipeline can take less time than others:
            %LOCAL_FILE%QgDYiw-84BUNbSgRI6NeWKTsYHXqJqKIWtTbVWCEnfVMsvlGWNfH5W5BeOhrrU5w0y9YksdAxhGCdVecxuyuDUb4iJjyRvl6EzU7QIohyaNRMn-pQ631fgHoWeTquEbn.png
            Thus, if a given instruction wasn't waiting for the proceeding instruction to finish it could complete the execute stage quickly and advance to the memory stage - however in the pipeline model, every stage takes as long as the slowest stage - resulting in: 
            %LOCAL_FILE%Mu_iWQF_bZD1YvTf0XGvMi35rlo8rUysvGO3fuqEI3NB9t-5Gn3tTOVaLJEIMSSn0JHGqE_6up5Rok71mWSeQJahF-fzbRXKp5m9o7AZUGIw25izDnqQ1xJ4OkzYnv1n.png 
            While individual stages can be slower, we can compute up to 5 stages simultaneously using the pipeline model, resulting in the overall execution being faster.
        %LOCAL_FILE%xBcTxIj0lgc1qrOs4Tz9MmVw0eVOqdOgEF_xlwQ3nmMJXJNAb2DWCYVTLvC9tinnMJxqkmtRZYWBUOLadV3iTFoi4cPNaDDMkYTXclrfMWae96U6qY0svUk7ndFljEBZ.png 
            Compulsory misses occur on the first access to the block and will always occur.
            Capacity misses occur because the cache is a finite size.
            Collision misses occur (only) in non-fully associative caches due to competition for entries in a set.  
        Discuss the main message from lecture 5 in 1-3 sentences:  
            The ISA provides the interface between hardware and software, however that interface can be misunderstood as its often explained in English prose rather than a formal language. We desire a formal language of an ISA to allow for easy ISA simulation of a processor. We can then compare the models of the processor using tandem verification.
        Discuss the main message from lecture 7 in 1-3 sentences:  
            ISA influences the design of the data & control paths. Multiple issue and dynamic scheduling can be used to reduce hazards and increase parallelism.
        Discuss the main message from lecture 8 in 1-3 sentences:  
            Because physical constraints prevent the creation of a huge low-latency memory, we must rely on a memory hierarchy. Caches (widely set-associative caches) take advantage of the temporal and spatial statistical features of memory access to store data that is likely to be used close to the processor.
        
        
        Discuss the main message from lecture 6 in 1-3 sentences:  
            Pipelining improves performance through parallelisation, each individual instruction has the same  (or higher) latency but the overall computation time is decreased. However, it is subject to hazards that must be handled with care. Instruction set design affects the complexity and feasibility of a pipeline.
    Supervision 1
        
        %LOCAL_FILE%2UQdpnQ1FkPkFUL4c5t1kWXX8y3CADrCFDn91MYJHDc14YPQZRJNraIwCbbIp89K4lKp_c4rP_kIcGIvHYGgzPMxlPie8s-mfFv3OwXYC4kPYjfGX5OrK1qSldMXblyW.png 
            Moore's law states that the density of transistors on integrated circuits, doubles (about) once every two years. This continues to apply for the time being, however we are approaching the limit in size of transistors - given that the smallest transistor we could produce lithographically would consist of a single atom in size, it stands to reason that Moore's law cannot continue forever (although we cannot predict future discoveries). However, for the time being the doubling of transistor density every two years is still being upheld. 
            %LOCAL_FILE%xYXHCdBxVEZ6SpLlP-ayz2XFjwCy7rNKi_33h8Ft17-nd85USY418aMCx8u1LRpktHyZtvLU8yMgvokFrZkmRMFDkL0vk1EJmqRIId_2caDaXUokQUVpOl0smC6Vz0Ut.png 
                While the density of transistors continues to rise, the computing power you can leverage from said transistors is not increasing at the rate it used to. This is due to the breakdown of Dennard Scaling, Dennard scaling is the idea that the decreasing transistor size results in higher clock frequencies at the same power. However, this has broken down as transistors require more static power to run and the working voltage of the system cannot be scaled down at the rate Dennard scaling requires - this means relying more on specialised chips (accelerators) and improving processor architecture rather than continuing to miniaturize it.
            %LOCAL_FILE%Q4ZZGTuw-9knew4SIejsy_O4NY5MIENMPj9WtraKgVUfPcSKFU4XeS2GoYszaS6keQ62ydV4udorTU2XdJ9BwbPsD6FykSnC-dIDlRpqDT5no4GOWHQdqm53hiHH5EYz.png 
                The doubling in density of transistors every two years would result in a pattern of Dennard Scaling, namely
                    Chip area halves every two years
                    Frequency can be increased (by about 40%) as the length of the critical path would have decreased dramatically
                    Voltage must be decreased to maintain the same magnetic field over the chip
                    Power & energy decrease in sympathy with the voltage
                    Thus, frequency has increased 40% but power draw is unchanged
                Before 2006 this relationship of doubling transistor density resulting in increasing frequency at  the same power draw was feasible, however after 2006 this relationship broke down for a few reasons:
                    Transistors leaked more power, static power use rose and transistors required relatively high voltage even when not switching
                    Semiconductor technology no longer could improve at Moore's law like rate
                    Wires don't scale with same chip area - Writing this from notes, but don't really understand what it means - would be helpful to go through this.
            %LOCAL_FILE%Fw_FBajQEF-Q1GHz6GAARODE3VXBhH3R0LRSkqdRZBXYmxqRgG7vn8gy-EfmFUvKbVD4PG8R6E4mtm2ATTTyn4E3hlZyrRlAn8ZcsIpWvWI84T60DpyNFqGCSomlzvtp.png 
                The breakdown in Dennard scaling requires a changing in approach in the design of high performance general purpose chips, the 1.4x frequency increase every generation can no longer be relied upon to bring hardware improvements. Alternative strategies to improving performance must be employed including:
                    Domain specific FPGA chips known as accelerators
                    Limiting the number of transistors active at any one time - in that vein is race-to-dark, turning of cores when not in use
                    Improvements to chip architecture & ISA
                    Employment of different chips for different workflows, e.g. a more powerful CPU (or a GPU) being employed when doing rendering work
            %LOCAL_FILE%rEOnDKh22cKdx73dq-PcN_vOV8Cxn89SGxibA9BzJ7xQI8S8uQLf6rULEmAM-eaeTDB4Ug_Uer2MECtP2NkXhQ72OTzE_UTYrbJTWJwoOjKvDbK3WuAUaRsHuiK6WyxQ.png 
                Dark silicon is the amount of chip area in a integrated circuit that cannot be powered on while maintaining suitable working temperatures. As described above, the efficiency of transistors has not increased as Dennard scaling would have it - so if all transistors were supplied with power in increasingly dense chips, the resulting increase in temperature could damage the chip or cause a fire.  
        
            %LOCAL_FILE%IT2dql__fLGqf4GVNEhdRVjR6-BnrpjXNiugp6PTQvw_I9j5k69Tv1Ks--Ykd6G55c_eFnSdr0NXF0Bnfq2S51OWqpezgQTjtNyZgIA6LYsNC6ur2O8y0tNYxfYTKSUa.png 
                %LOCAL_FILE%p6oneUwYrLCGTrHJs9vTxCUgjN_tN_NPdlHLiLcZ2R31Mlpdau-G6wEKpokuFnXG7JYy8TBo0PPh_tqb-dvSq4DmiVNA-T1lLxeiegE0vgor63iFn6gkmX20JkzBeyQZ.png 
                module seven_bit_nor(
    input [6:0] a,
  
    output [0:0] b);
  
  assign b = !((a[0])||(a[1])||(a[2])||(a[3])||(a[4])||(a[5])||(a[6]));
endmodule 
            %LOCAL_FILE%hvLJ8_w-jNwTuRhid60ORFMQI6QKJZXkC8LH76ZnP4iZ2IE4w0g0eVxf3OW6r5RkQ5ioCwFNi-he9qQbhhPmCi0M9x7sdqXKWkrrkduxhT2DJdtzGg8MmwysRT1TuKr3.png 
                module four_bit_adder(
  input [3:0] d,
  input [3:0] e,
  
  output [0:0] carry,
  output [3:0] result);
  
  wire [4:0] temp = d+e;
  assign carry = temp[4];
  assign result = temp[3:0];
endmodule 
            %LOCAL_FILE%cIXgZzNJHK2WR6WlPb5Xa5O2WJKh45GL9qCrPopl3SFPcaCG-s-u1w9WGvjhgCbyIFpB6r2kDBj2h2Wab-OzxI5hjcdWEk1dr9jAQXL6Hcg3EMaZL8Whr8K88mR2V78B.png 
                module multiplexer(
	input [3:0] a,
	input [3:0] b,
	input load,
	output result);

 	assign result = (load == 1'b0) ? a : b;
endmodule


module thing(
	
	input clk,
	input load,
	input r,
	input [3:0] g,

	output h);

	reg [3:0] ff_out;
	wire [3:0] adder_out;
	wire [3:0] mux_out;
    wire who_cares;
    four_bit_adder adder(4'd1, ff_out, who_cares, adder_out);
	multiplexer mux(adder_out, g, load, mux_out);

	always_ff @(posedge clk or posedge r)
		if(r)
			ff_out <= 0;
		else
			ff_out <= mux_out;
		
	assign h = ff_out;

endmodule 
            %LOCAL_FILE%6K7crCiuD-NaTWO5HDkgVM6FkAe0Of2_sqJQvWVP3upsUajztlz2m-KGH70D7eVV_cRR_H2gri3BqPGzELahOpgmJNKlqGC3AkLtPehk75w1wo19H5XEy5BjYc71zyOa.png 
                %LOCAL_FILE%BUjnDkUsxRvCapdfbOZPBKiFsYzgeUIhACSbI38k3SkiAJHDSdGse8ICaWWLaDhY1QPEzayhsBQazHEWL8mEiPFuKe6xKYh2ACD0Ay86olZl4U2_cY3jZ_Q8vlc3HAPC.png 
            %LOCAL_FILE%rOhjWwznStNn4uT7WgYJKTEYQY4qRLbaELNgyfAg_NM3wpopMfnPgCTJQjnNMs_sWeE2SDwzHA8sfu83f4BRt2tp23FdJE-wgyPDlw-hZDFfbhuxHmg4VccfcEJn3ofm.png 
                Whenever Ain is 0 & Bin is non-zero, 
        %LOCAL_FILE%62jYcXcvp5-K-KT9jGUFLq0G8ymM7zsMMkrns5IvjLMBDOFIj_hYmDl0Uu-FPw3unnSyFYHrkKInj3tYN7P7WcJIENoO7_UWgxJx9CieJwrKeJ72aOLVsCwi4GZLAmri.png 
            The mystery module behaves like a stack. 'head' is the index of the head of the stack and mem is the equivalent of an array (that head points into) that stores that items in the stack. The opIn command puts the value in dataIn onto the top of the stack and opOut simply moves the head of the stack back one (allowing that value to be overwritten in the future). When the module is full, if we try and add an item error will be set to 1, meaning we will not be able to add another item. Similarly, when we are empty and we try and take an item out error will be set to true and we will be stopped from doing so. When we are empty we have the additional side effect of outputting -1 in dataOut. op decides whether we are inputting data or outputting data or neither.
            %LOCAL_FILE%hUkjVm7UBOSGw-ABIHcSOtftvD9mLbfGtF8VIgib2ETzsgFn_3vwsXGxUFPyBesxk70KJMVN-DGsja5FjlBTelQFspG3eQMSnM_LpRYyv_H9nyR77FhX4LWjKMsxofik.png 
                Production tests are used when the chip comes off the production line, it tests whether the correctness of the board is maintained throughout the manufacture process. Functional tests are done before production, and ensure the correctness of your HDL code.
            %LOCAL_FILE%ufaK7X_Tg3kA8QzEtneISxNOefoDhJE0V6Bwh8EMdc0lNl24lQDjdz45KY_jSQnZ3BoMOUZaiGkFQh7PHbsB_cERPZb1pGLlXTkJ7_H-wzYumQeY3U19dJK9j7kMQG_s.png
                The mystery module is parameterized by the width & depth variables, thus there are a huge amount of permutations of those values that would need to be tested. The opIn, opOut & opNeutral values would all need to be tested - and varying ordering of the commands would  to be tested, resulting in thousands of possible permutations. Additionally the correctness of the outputs of the stack would need to be tested according to varying inputs.
            %LOCAL_FILE%61sohsSD65S5uRDO7M5mmOgV9Cl-ttxt3bT7agOqJCBQKBffyOlE7wriUFv-K7DgJEA2Ypjvj8YJOmEqUlqVRJrugPn9eJvIEfwbDq9tse1AiMDeUsoDqHjZyftziDRh.png 
                Similarly, the correctness of opIn, opOut & opNeutral and permutations thereof would have to be examined as well as the effects of varying width and depth - however less permutations need to be tested, as we have concluded the logic is correct and are now testing the physical board where any problems would appear with less nuance than a small logic error. However, this process is made more difficult as we are using a physical board - it becomes more difficult for testing in parallel, and physical factors like temperature and humidity need to be considered. So individual tests may take longer, and we must ensure the board is kept at the conditions it was designed to be used in by users.
            %LOCAL_FILE%L9w2S79jAzPJe-B5MEgJrZ03BGSBkt6ryzB0yvzSWy6YJnEalCGKflbVuEEnCT8EnxOGZGmphXuvmJHHIgbQbo5e1g2J3nlZ3sSZhNL8s3thrK64Ai1YlI9irxlIGCQq.png 
                SystemVerilog is a hardware description & verification language used to build logical models of hardware that can then be simulated & tested before finally being implemented in silicon.
            %LOCAL_FILE%s_dGIjzu1_bYbflvHG1GqIQhDr3wBoOryD0wRVWVCyiN6pAc-21l-DmB-dHReBb1K4qy7jBN09IW3LXdZ_Er7L0PsQNeynZaSeNrZK5awAt4wP1JLQnjySt3ubgF1bHM.png
                Meta-stability is when the output of the flip-flop takes a value with voltage between that of a high & low signal. It occurs when the input to the flip flop changes during it's setup or hold time, and the flip flop captures the value of the input as it's changing.
            %LOCAL_FILE%rljtTxKhNSKzEPi1Q5loesdd1fpx_6hXt-vg42OuntEKsxFmYR8YCUiuM_cGTjeV4PMnY7az-5OR-AB6gUor50g7dvs4gHpRrUzmsOADWhjf35daaVoLOl9YYuVEt_5U.png
                FPGAs comprise a sea of interconnected logic blocks & IO units. These logic blocks can contain complex features like flip flops and lookup tables, and can be programmed to implement a variety of functions. They benefit from being directly programmable by the user, that is one needn't invest in a chip negative for millions and produce your design in silicon - this means programming one is much cheaper than the alternative. Because of the vast array of logic blocks, FPGAs can take advantage of massive parallelism resulting in quick computations. However, FPGAs are much slower per gate so this parallelism is necessary to reap a performance benefit.  
        %LOCAL_FILE%9JfvfQNG88PPatL-LGthmAf1ekA6VqJBTBChY92IumRxKw24x0ntjkLvUzM3ZV_UObXKGqhdW8oZCKDv9HvetY8ptV4R6SvHMBd1ReiQGfLE3VqcKcrEsKWXtP0KCHIi.png 
            In C's case first pre-processing is done where the plaintext elements of the code is changed. Then the compiler compiles the C code to assembly code, this is ISA dependent as different instructions will be deployed in different architectures. The compiler will employ tricks to improve performance while keeping the semantics of the code the same. The assembler then converts this assembly code to object code and finally the linker combines various .obj files into a single executable file containing machine code.
        %LOCAL_FILE%Sdh8cPzeyLI-XzDa6wMfkiZYUB6H_Zki2pKhO_YN3Ns3c_Cd1JHwxAtAhDZ1pu1Ami8Tl67xRH55V6FFll45UCaBIp_jWUQ7AO4j4wtkLEoRCqk4knF7CI_GcEe-lMhw.png
            Clock Speed - The number of times per second the CPU clock flips, equal to: f \leq \frac{1}{\text{Time taken to traverse critical path}} . Operations can be segmented based on the number of clock cycles they take to complete, and all are sped up by increasing the clock speed.
            Number of cores - more cores allows for greater parallelism and thus a potential speedup, this does require software to be written to take advantage of parallelism.
            Cache size - as fetching from memory is such a slow task, having a cache large enough to minimize those expensive calls is very important when trying to improve CPU performance.
        %LOCAL_FILE%g0pdH_L0Ima0pUXs9OKScuB2D6KMgpQBOK8bRQXVNuGGd4YUOCHTjIuTOuDn4mbe9YjsJu96F3uCt8T5d5g8-3koJVb3dsY5T-P1_ZtOeXm0cm2F_khcU2PV50yInyY_.png
            An ISA is the set of primitive instructions the CPU can perform, ISA matters for a number of reasons:
                Ease of programming - One can design ISA instructions that couple multiple commonly combined instructions together to make the life of assembly programmers easier. In RISC such instructions could be left as subroutines.
                Making the general case fast - Including certain operations into the ISA can result in speed ups in specific operations that will rarely be used (e.g. supporting integer operations) due to pipelining. Thus choosing which instructions to leave out, or how to use non-special-purpose components for a given instruction requires careful thought.
                Some parts of the modern software stack is ISA dependent, meaning porting those sections to modern systems would be incredibly expensive. 
        
            %LOCAL_FILE%hSiQu5PlT8ACwIkSvrP7QR59BevxaMb9YVZOVrd72gcmD7ONRE87Qq0XY_iUHb-sZke0qV9Mo3060_YtFMjL3kjOJu2x8zQoyUG6we5DDRM8kYSGECzgwqA61IXsyqee.png
                Moore's law makes predictions about the density of transistors on the CMOS chip (namely that it will double once every ~2 years) while Dennard scaling makes the prediction that clock frequency will increase by 40% every 2 years while keeping power the same. While Dennard scaling is based on the assumption that Moore's law is true, it goes further and assumes increasing transistor efficiency. Dennard scaling broke down in 2006 as transistors required too much energy even when not switching, however Moore's law still holds today.
            %LOCAL_FILE%UY0yfJpN6phy-eB9hvgQ8zb-XyGz1uy9gljT2bTMYvVfdOY_EehHgKuHVUtdm-aK4iLXdRS_gJN8kk9kNvtfh5QBu_Ks21NuUw_QuEqCXalCgUT-Su4MqvY2lq6OPKZp.png 
                The critical path is the longest path that data can travel in the circuit - an exhaustive search could be carried out by testing every input for every state & measuring the time taken to finish computation. Alternatively, we could model the circuit as a type of graph and compute a breadth first search - recording the largest number of steps in the search. The critical path directly impacts the maximum clock frequency, in that the clock frequency is:
                f \leq \frac{1}{\text{Time taken to traverse critical path}}
                Thus, decreasing the length of the critical path would allow you to increase the clock frequency.
            %LOCAL_FILE%sZU5STJY-guUdE3BQKXz05R38ufvi96_JuO8__Gyvy-s_Q1IZ9jQ2FMe83wU57ZB1AyOmj6Q6vhwYopP9mIrqF_gwdK52wpZVyQFbII2GpcxT6E7JRL7sJiwee9hg3wq.png 
                A calling convention is a low level scheme for how functions receive parameters from their caller and where they do with results, for example MV r0 r1 moves the value stored in r0 into r1. It can impact the design in a big way, calling conventions imply certain big decisions such as:
                    Max instruction length, decides the total number of instructions possible
                    Max number of immediate parameters to a function, e.g. parameters that don't need to be fetched
                    Where return addresses are saved
                %LOCAL_FILE%BlFcoIXChRxj-7sJ7kp-w1qtgRgmJVcwHgNY9niWjD3TvdTtUvetXCboQ-cPtmM99T17rV38pCr_s6_zcGGjV1gQjSxxBDzzPhZrcLgC062aMOSSTZ4l6eEFqpLRvye4.png 
                    Section A: This section checks if we need to do another round of the function or if we're finished. If a1 ≠ 0 then jump to .L7 otherwise jump to ra (this will either be to the initial function caller or to another instance of gcd)
                    Section B: This section allocates space in the stack, and saves registers so we can make our way back up the stack frame to the initial caller. Allocate 4 spaces on the stack, store the current return address in the first segment of the stack.
                    Section C: This section computes the modulus and recalls the function with these new values. Set temp = n2, n2 = n1 % temp and finally n1 = temp. Then jump and link to gcd (storing the current instruction location into ra).
                    Section D: We'll reach here if we've previously completed section C and section A finds a1 = 0. Here we retrieve a previously store return address from the stack, free up the stack and jump to the return address. This starts the process of unravelling the stack frame.
        %LOCAL_FILE%sBPoSMEcGwfeclHCQG5umDhvhCzuEURc3ZFKjKlKxZWrAqVUb05SDX2eaWMJOC8QpkfIjBRGoNxhcRdC4XDdaIiJtDvfcRC-5S02rkInpNg2NzzkCnvWZBbX9yoCyJl0.png 
            Von Neumann style processors are not the be all and end all of general purpose processing, the incurred Turing tax results in slowdown & massive power use. As the golden age of silicon is over, we need to consider accelerators & other methods of improving performance that our outside the Von Neumann box. 
        %LOCAL_FILE%xzHKmOBmsp5VTUoRYJqnaeoGLIMjg6wfYAlFayAT7WDYyE3UH1rDsBABqCNr8M6_ZfUR7SyD0-tUrPiXdfiXuO3t1uuT3vqOiMDug6FC-Z0X_z9Z_q2JxwWc_W0nGfvb.png
            Clocked digital designs are used everywhere, propagating the clock signal is very hard and requires careful thought & design. Designers need to ensure that their system meets timing requirements and handles asynchronous inputs safely.
        %LOCAL_FILE%KnuI9ow0ir_IgodEqmPCY7IHvLAA5uGz_W4OqIwyGOIrbGVgtUaub1gRxD2OMrQg2JLYTmNUH5OKJLuUMoJLFzyHknqz358TnMcq-rMQVSx21tmKPfKUcYzwonBnDHlT.png
            There are many possible elements to optimize in processor design, and while one optimization could help a given workflow it may hinder another - thus there's no perfect CPU architecture only compromises. The Von Neumann model doesn't help with improvements, most transistors are used moving & storing data rather than performing computation. 
        %LOCAL_FILE%M6Mcdr232g4ok7gXv5Jnt0aBIXo-zv-LY1wfcfH3O-o8qTWJ_IbgZSIJJN-ma2PeMKIeiC8JlpD4XwHqS3srsVvlCk8lnU8OBJjeMph6pjP8HqEP4LLLL_GZ1rw1InkS.png
            RISC processors are often simpler and lower power, but CISC still dominates the PC industry (ostensibly due to backwards compatibility issues). RISC is used in the phone & microprocessor space and also in the research & teaching space as it is open-source and (relatively) easy to understand. 
    Lecture 1:
        Different Processors are created to fulfill certain requirements, what are the 3 main requirements?↔Power, Area and speed
        Give two factors that constrain the production of CPUs with higher speed↔Instruction set parallelism and Power
        What is the main benefit of devices having general purpose CPUs when a preprogrammed gate array would work?↔General purpose CPUs allow for firmware upgrades in the field
        Von Neumann Architecture
            Almost all processors use this architecture 
            What are the two main features of Von Neumann architecture
                A linear memory storing data and commands
                Control flow processors that execute sequentially 
            https://www.cs.mcgill.ca/~cs573/fall2002/notes/lec273/lecture8/vnb.jpg 
        Memory Hierarchy
            Various different memory storage types with increasing size and decreasing speed. 
            Processor Core, L1 Cache, L2 Cache, DRAM
            What feature of processors poses a problem for fast memory fetching↔Sequential execution, means the processor cannot asynchronously fetch data
        Manufacturing cost
            What is the most crucial factor for determining a chip production profit margin, and what is that item proportional to?↔Yield, which is proportional to Chip size
            One of chip production cost is going up, and semiconductor fabrication foundries are getting  more expensive exponentially
        Wire Scaling 
            %LOCAL_FILE%1T9JycMAAFENrnfZUO0ti3ABwW9gPPx7tlIvpaxhOCk1D53dJNi65eBCRM6mtfzcDtHO6cPcfvmz_juAgfvUAO6oZ3_swOANtSJ3F89qkLy0QdS604_CmJIUBd_MtVwp.png 
            R \propto \frac{L}{W+H}
            C \propto w \cdot L
            T \propto RC \propto \frac{L^2}{H} 
            Where T is the "charge time", e.g. the time between applying a voltage at one end of a wire and reading it at the other. So performance doesn't increase with the same chip area
            Halving L makes the wire go 4 times faster, so can add buffers
        Dennard Scaling
            Sequence of Dennard Scaling
                Transistor dimensions reduce by 70% each year
                Area decreases by 50%
                Frequency increases due to more densely packed chip
                Voltage must be decreased to keep electric field the same
                Voltage decreases 70% reducing energy by 65% and power by 50%
                So every generation, transistor density doubles, frequency gets 40% faster and power stays the same
            Causes of the 2006+ Breakdown of Dennard Scaling
                Cannot further decrease supply voltage as transistors leak too much, static power demand increases vs dynamic power (when transistors actually switching)
                Improvement of semiconductor technology slows - no longer delivers on Moore's law like performance
                Wires don't scale for the same chip area
            Is Moore's law dead?→Moore's law is still (just about) delivering on the doubling of transistor density every 2 years, however due to this leakage and static power issue we need to constrain the number of transistors active at a time. Move to accelerators.
        Age of Dark Silicon
            What are three methods to improve processor power efficiency in the age of dark silicon?
                Use domain specific processors (accelerators) which are more efficient and often faster
                Race to dark - Keep transistors depowered when not needed
                Use a range of processors for different work loads
        Voltage and frequency trade-off
            Active Power = C \cdot V^2 \cdot f \cdot A 
            Where A is the proportion of transistors actively switching (Activity Factor)
            For a limited range of V, f is proportional to V - so why not just crank V?↔Heat could pose a fire hazard, and CMOS circuits get slower as they heat up
            What is DVFS?↔Dynamic Voltage Frequency Switching, adjust voltage with frequency to trade off performance and power use. Similar to overclocking
        Low power at idle
            Would like power use to be proportional to load, however the i7 uses 258W at 100% and 121W at 10%
            Static power makes this difficult, and having a lower static power results in slower transistors
        Moore's law for storage
            Moore's law for density applies to DRAM and flash memory, because they are made of transistors
            What is Kryder's law?↔Kryder's law is the equivelant of Moore's law of increasing density for magnetic HD. However, the size of HD increased much faster than Moore's law
        Design and verification gaps
            As computers get larger and more complex we get problems with managing designing them
            Solutions to the design and verification gaps caused by large and complex systems
                Hierarchical Decomposition
                Replication of parts
                Libraries
                Abstract interfaces
                Higher level hardware description languages
        General Purpose Chips
            Not power efficient and slower than the equivalent FPGA which have 15x slower gates - this is due to parallelism being exploited. 
    
Further Graphics
    Supervision 1
        %LOCAL_FILE%gTK4JW5ryukfyXXqdHoX4VyrjOWKeMBIZR0MErPyy_mHWLsDjfYI4zrgxLoRfwEXncPqwwfL72zuQswpGmhrs_N-qTprQFaoxHUWpdUPJ19TiARF_nwV6yK5JSa4Afkv.png 
            Parametric surfaces are created using a function f which operates on sets X \subseteq R^m and Y \subseteq R^nmapping set X onto set Y: f : X \rightarrow Y. Whereas Implicit surfaces are created using a (distinct) function f, where a point x \in R^k lies on the surface if f(x)=0. 
            While both use a function f to decide the resulting surface, the parametric function gives us a way of creating points while the implicit function gives us a way of testing points.
            Advantageous components:
                Implicit functions give us an easy way to check if a point is inside, outside or resting on a surface (f(x) < 0, f(x) > 0, f(x)=0), the parametric function gives us no such method.
                Parametric functions give us a method of generating points on the surface (any point on our domain X can be mapped to our range Y), Implicit functions require us to check points to find one resting on the curve.
                Parametric surfaces allow for easy computing analytic point wise differentials, while Implicit functions do not.
            Implicit surfaces would be suited more for ray tracing applications, when checking if a ray impacts a surface is the norm - in particular, implicit surfaces can be useful for visualizing mathematical formulas and the conjunction of multiple formulae (computing f_1(x) + f_2(x)=0for two such formulae).
            Parametric surfaces (most commonly Bezier surfaces) are used for 3d modelling and sculpting, however the surface would undergo a second process of being converted from a parametric surface to a normal mesh that can be rendered using rasterization.
            Point set surfaces differ from parametric and implicit surfaces in a number of ways
                They are generated from real world data.
                They can be used for direct rendering using rasterization with dishes rather than triangles.
                They are not analytically differentiable like parametric surfaces.
                Formed by taking points and examining other points in a region and forming a proxy surface and then checking if the given point lies on that surface.
        %LOCAL_FILE%7GDDM58Km_D6S_B7eTNJRuJuvQZ2lcJOW9WMCy-vyKsvkWTCPlVnMA6ULHFbsRn96NOYlogLBgaD02NrPdWYGVujgdOh2Tqld1J27kRYT0EL28IH24B7GaUblTqK91q-.png 
            x^2+y^2=e^{2t}
            R = e^t
            This describes a spiral centred at (0,0) where the distance from the origin is e^t
            
        %LOCAL_FILE%ZmkyinQzH326IJwFAzKQAtsa4DjUaZkSKgdGN2ILCYoWxNMSz5LfKPKDRhxI_gCOzrpjNX9XzgBFrSJiE0U-q9q4g_AeCGpt8esliS1FG8NREOTPRZmrnrXCzhnFudMq.png 
            a.) First you must compute S_u = \frac {\partial (s(u,v))}{\partial u} and S_v = \frac {\partial (s(u,v))}{\partial v} , one can then compute the normal via \frac{S_u \times S_v}{||S_u \times S_v||}.
            b.) https://remnote-user-data.s3.amazonaws.com/jWzZl4lN7321QOkwuLl-rQjLyrCTQHHloFpCA7i31xJ_aiViYTlSBWgQyh430XaZCzVQv66zT4atPR-jzFKdo7K03icYTVX3q89WQ2nMKXqS7V0XdJ91BT71WGRyLY7v.pnghttps://remnote-user-data.s3.amazonaws.com/VRgybaadKrIWnFcRLL4KuG_nZMVq3eWkG6IL2aQutOuyYrgkyEvqiFJU3oT9xTxH0x2R38Xxel-F-ld7kyFgLW0TZZU2xa_AmUc8RwNGFJxE3sQEZETk4hW9PkO_sv3H.png 
        
            %LOCAL_FILE%8UIPYfQEqMSbTmQI5-FBIbJ3mtNHi6yluVkNxBaGrWKOV4ToxB-e_ludB-kOIn7TCdZatDLxcfuDn0nKUG8cba9JGVYW53vTW4zqYDOWv0gesXcwuPRq0u60MoGIUHuf.png 
            https://remnote-user-data.s3.amazonaws.com/4sG-KtYTLnDXew39MLnWTljXM61KRrHhOyT_C5lSh3n9Md2ceefb3bBHHYowjiMzHZBdc5k3UTxMvcUreaIc7IonOk7PBVFy3yzNxMWlExbjp4LdhqwUbhPb4S7n5a8i.png 
            
        %LOCAL_FILE%XOvvZeCJvrhpTcf7nXIMF8k55A6mrKyG3U296WFcv6gSIkPOHkH2Rc602pUZH_Whj4Lj79-n9FjqHOAxdkqtHkHJVl9ZxW8L5qDGP6d1HBFnar1KZ11BscqFF1tl2WdS.png 
            This implicit function will comprise all (x,y) coordinates where either x, y or both x&y are zero, meaning the function will be a plus sign on the xy plane. The result will be at (0,0) there are infinite possible surface normals, while at every other point there are two equally plausible surface norms.
        %LOCAL_FILE%_Avg0u7PgJxITQwfaoK-ZxNT52GGyof6YJk5QVE2BQfM5c-Yblz20LZ2GAdY4Ataq4mGQTFU-a_MaFFfvf9iVgHvOmqToxKhKC7QR_ayMi_JHBQoteu2v9_IpU64ziYx.png 
            Triangle meshes store two separate lists, a list of vertices (containing the x, y and z of each vertex) and a list of faces containing indices of sets of three vertices. E.g:
            v = [(2,1,4), (2,4,6), (1,4,6), (2,4,5)]
            f = [0, 1, 2, 0, 1, 3]
            From these lists surface normals and other factors are calculated later.
            b.) The geometry would be the list of vertices, while the topology is the faces - while the topology deals with the more abstract concept of the connections between vertices, the geometry is the actual coordinates in R^3.
        %LOCAL_FILE%5b-NrE_NjGIA3t1Pie8OKt4e2vKrQb0aGEiVYC18feWaJh1s4lBqocECCpVNjNl_kwKR7TgVZvI4ohGBSyx6JDtNIiWWCQAL3keyYAyPjwOiSm0iBdf--8-p2QTxT7wV.png 
            For normal triangular meshes this is vital for a number of reasons
                The order of the neighbouring vertices decides the direction of the surface normal, without that ordering the resulting vertex can be facing the wrong way.
                The choice of neighbouring vertices decides the size and direction of each face.
                The neighbouring vertices of a given vertex cannot simply be calculated by finding the 2 closest vertices, using this method can result in areas of the mesh being devoid of triangles (a see through part of the mesh) or alternatively an area where triangles overlap. 
            Point set surfaces instead do away with triangular meshes and use dishes oriented in the direction of the normal. With a large enough number of dishes, we can cover the whole surface and remove the need for storing neighbouring vertices.#
        %LOCAL_FILE%-VeCFokgTj5THyvZ9IIAPBfs_rDbc2vueDR3P-iZKsbBo85CCQRpSbltC7kceuyif3ffjF3SdUKohuqzmg2NaaFNWw6MMhX426EWBs3ckHEEeNzNbU5r5Wk_zIcKXFe7.png  
            A manifold is a topological space that is locally Euclidean, that is for every point there's a neighbourhood around that point that is topologically the same as a closed ball in R^n. This differs from a manifold with boundaries, where every point is either topologically the same as the closed ball or a half ball in R^n.
        %LOCAL_FILE%PEM-naJb74Gs19Jh3tbbGf2pnLbzQiIt9rIYBLAV4Bm8XlmA5QrjfRs-XJIO8331tIU_0KcwatDz0JKNb5bsf_SFNwup9jx5DvQMkV1MxrUlo281wdNat3crFfkZt3pL.png 
            https://remnote-user-data.s3.amazonaws.com/nbpdkz3g0QTGfwXYz_2uyTwCzjPTci0hfo3j38PyEvqyPA088yUhPZ5S8DYrtRSILq_vxGpcjNg3QkMwnv9-36llrfMzauxSAe6suBcpPtRlTBnX8wVFVC2XpyvvoG00.pnghttps://remnote-user-data.s3.amazonaws.com/owCO8hXdTgMd1Uu6nRgpsMYjlfdPbQauA2SNjhsFi1qT1KChWZiOPWQ2wc5lGVQVMQfdRKSeiCi2lVIamb2ajAhP8GWy0CXh2qxqp4VqS5VpasWB9bJvmHrx62VmNRU4.pnghttps://remnote-user-data.s3.amazonaws.com/HWlUsLlt6unQe7NVpUwo4q0Gc5-_adujByQFIgxpjRBtUIkqtYtoSZD4PsLTo83IXYPrZJz6VV1TXFel9HN352HXyVBeMcMXFmX7ZLz2DNAjQHozIymb-KxK7D4UyiOL.png 
            
        https://remnote-user-data.s3.amazonaws.com/uIcH7WGA54QIVhJ0azN-JgKybCDazS3nb53Q8fSnOopK2AxZLIDjzNsZ8VpCt9qxZ7HhT50ZzvhMiKJ2Q7rWt_E34F9u_1Rrt8J9_pg8SHGHp-QrlBvN1sC46gpxcP5G.pnghttps://remnote-user-data.s3.amazonaws.com/_MYCKA-U6uo18yOOH0vg0MJCh-VbzUD6JEaBhkdeMkUAuIFH7rWS059UxTsrSIjNBY3opRO67MSX7fzCVwzptVrt_8P5eDMYc_Bqa4NNvjThGIKDg6RhF692uBzCV3iV.png 
        https://remnote-user-data.s3.amazonaws.com/XXtkOh1iTv2VMWTab58JYFEVXFb0HN5cExyDfPZfweVMW7f-nE2JzM6UBNkxSJXBkftd9ZKZl27snfG1dDvMMdfygheG4SRalotI6yU5B46hgRzMJAUQGI6Geyy3yzsi.png 
        
        
        
        
        
        
        
        
    Lecture 1 Geometry Representations
        What are the two main sources of 3D geometry?
            Camera sampling (Point clouds) 
            Digital 3D modelling 
        Considerations for 3D geometry
            Storage
            Acquisition of shapes
            Rendering of shapes 
            Editing shapes
            Creation of shapes
        Parametric Curves and surfaces
            %LOCAL_FILE%jij8LnlRdQqRvRBAWGGdAQMBmBAFtnPIX84u1QnPQlrCKIOQJnD1KbTRNX_glroP69oKsNKhXOxi2qyMepdHfb5THuZ-07L1y9AX7YuqIzediVK3zAVFe-NmwQ3Zo6m9.png 
            What are the three fundamental equations for parametric curves
                f : X \rightarrow Y
                X \subseteq R^m 
                Y \subseteq R^n 
            Where usually m < n and infinite samples of the function result in the whole geometry.
            Planar Curve
                What are the m and n values for a planar curve, and give an examples function
                    m = 1, n = 2
                    s(t) = (x(t), y(t))
                    r(\cos(t), \sin(t)) \space\space\space t \in [0, 2\pi) 
                %LOCAL_FILE%f2zAZKGcOtm5cobCTsjFhv6Fdm2jafuCJtEazRN8u1fwJ-e9yv5kPMD2VnxBSWfibY_v5A0z5U25qKn968rSEQSoLLgcS-WlC503hNYuG9Jh5b6R0Bw6lWPaEtU2jpaU.png 
                
            Bezier Curve
                %LOCAL_FILE%G_qXB6PERNCbBzvO_5ofEPkcZqZFNcDgss5N4PEJVEToDvw1xg3rlBXbOhEpkB5WGi2vVuIbiEib8fR3WkzywVfW-yDihEDZLk6dR69RHs9kkcb7UV_a6svTZ0dsUKFV.png 
                s(t) = \sum_{i=0}^{n}{P_i B_n^i(t)} 
                B_n^i(t)={n \choose i}t^i(1-t)^{n-1} 
                Allows for moving smoothly from point to point
            Bezier Surface
                %LOCAL_FILE%ZE9Z9UBBbLbpiGj7UYd_gZSWf4HHEub6Iy-TWItU3QlrIOG8lGMYJ88XNuqV1tkLO5g7oZjcl2yQ3-PG_SQIaGAeSRLNXd9Dm_mUloD7Yyxb2l5pPC57N7sGLddAKmaz.png 
                s(u,v)=\sum^m_{i=0} {\sum^n_{j=0}} 
 \space P_{i,j} \space B^i_m(u) 
\space B^j_n(v)
                Where B is shown above and P is the height of the control point of each vertex
                A Bezier surface will always lie within the convex hull formed by its control points
            Normal and Tangent Planes:
                Give the equations used to calculate the basis vectors and normal vector for a parametric surface
                    S_u = \frac {\partial (s(u,v))}{\partial u}, S_v = \frac {\partial (s(u,v))}{\partial v}
                    
                    \frac{S_u \times \ S_v}{||S_u \times \ S_v||} %LOCAL_FILE%W0TDCq4d8uKp1i4ptIwB4wG8Py776569voOeqwE7HOAHqgZZaUPDZP1UraYIbiGfYAnrbnfVn7igdp5jIkBk3bUNu7BPxu1MVWxn8UKeFuK_QjoVzgOrqCm4rk3Klk-x.png 
            Volumetric Representations
                %LOCAL_FILE%aFvD3btV7B1EOl8_jCMjS4vvfbnDY2LWxNnfD92VeckX4W_4Ot5HcSN6IKF_zCESGAGAJNJsngr2EWqySoex2RoWPyZ4F0LQIhelvG00OOGt56pDBpqC-xwoAZeLfFD9.png 
                X\subseteq R^3 , Y \subseteq R^1
                Forms "density fields", useful for medical Data
        Benefits and downsides of Parametric Curves and Surface 
            Benefits
                Easy to generate points on the surface
                Nice point wise differentiation
                Easy to control
            Downsides
                Hard to reverse-engineer surface
                Hard to decide inside-outside surface
                Hard to decide if on the surface
        Polygonal Meshes
            Connected points that form a piecewise linear approximation to the surface
            %LOCAL_FILE%pVRbFTJiQ_QCs6i4Y2_KVqFIlF1n5sOjiJUwECTh8Q0OvPRz104EIuSfphvZzmsxnem5MnmDMlP8p2bPcy8ryFn6wYD8cGll_AEfCN99ozHuK-kN_QyLeLqXuckNysxI.png 
        Implicit Curves and surfaces
            %LOCAL_FILE%OSyZL75cz9VCR7NvHHYyt6cgxA0xiF5C_u2ZT_qyb3PRulP0uzR6tO_6ftdws0WHs_aeJf3hrDffa-uPV9Z2CgFcu8lQT0v4XW1cqKxz_U7HJB-6MSxcbYvDCffYIl7n.png 
            What are implicit Curves?↔Curves that satisfy a given property f.
            What are the two Fundamental equations of implicit curves and surfaces
                f: R^m \rightarrow R
                S = \{x \in R^2 \space| \space f(x) = 0 \} 
            What three things can our implicit function tell us? And what is this useful for?
                f(x) > 0  Then we're outside our surface
                f(x) = 0  Then we're on our surface
                f(x) < 0  Then we're inside our surface 
                Useful for collision detection
            Example usage:
                Give the implicit function for a circle↔f(x) = x^2 + y^2 - r^2
                How do you find the surface normal of an implicit function?
                    First calculate grad of f, for example \nabla f(x,y,z) = (\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z})^T 
                    Then normalize the resulting vector
                How would we find the surface normal of the implicit function of a sphere?
                    First note that the implicit function would be f(x,y,z) = x^2 + y^2 + z^2 - r^2 
                    Then take the grad of f, \nabla f(x,y,z) = (2x, 2y, 2z)^T
                    Finally normalize the resulting vector
            What are the pros and cons of implicit surfaces?
                Pros
                    Easy to Know inside / outside / on Surface
                    Easy to combine surfaces
                Cons
                    Not suited for real time rendering
                    Hard to generate points on the surface
                    Limited set of surfaces (for finite functions)
        Point Set Surfaces
            %LOCAL_FILE%ezsxBHc9t5yBc3GfDoiccHryfozIxDjLE0beSaveN-VUt20FKhBkAs5eExIE-WxU_eiVIDWLodZpPCx_36B9SR0uV72ZJHl2OuTGf8zNaLc86vYCDi3F_FVQV2RFOMKl.png 
            Point cloud ⇒ a surface, end with the analytic form
            What type of data can you use for point-set surfaces?↔Only point wise data (position, colour and sometimes normals).
            Approximate measures ⇒ Smooth surfaces
            What is the process of determining if a point is on a point set surface?
                First examine a region around the query point and map a simple proxy surface (line, plane, hyperplane) to that region%LOCAL_FILE%LVdEODyX7e5rr0D7IF1K5ENp7bN0Kyw8pclpQI6MfUzQYeFxTzxaXwRJcISTG2WXVc0263NC9bHnLz1I_s_PS6HIc1Hn3R8T5qUr-WgaHwTbkLGBqw7fjI_z9muuF2uH.png 
                Then if the point lies on that line it is on the surface
            How can we project onto our point set surface?↔Using our proxy surface, we can get the line between said surface and our point and then project it onto our shape. Using this we are guaranteed to end up on the surface.
            What is the resulting function after generating a point set surface?↔The result is an implicit function %LOCAL_FILE%b-PQYvFpJ5nOnDjLgeClezXc7nPGedb19FXDjcDJWKB4kWYIbghJQpzk80BlZ0thAfuLJ-knQPv3kSJzpA-hFdAhF9i7FT4vne5YUlO7olHcjeb5mrTHb7p_K5A_j4Dr.png
            Give three important features of Point Set surfaces
                They are robust to noise.
                You can do direct rendering with them - can do rasterization using disks and surface normals around the points, rather than triangle meshes disks are used.%LOCAL_FILE%hyf4-cGTIQB6M2_d20tHFIOEzJvtBcr7Zgfan8pnO_GBVSlOHPq8BNUSCxImeIDVsfhtz7od8HgcDY_Y7wBmPPV0wiUEh3y6gBNHfUaFvHzOESlZ1W9_yauMI6OsPCa_.png .
                You can convert them to meshes.
            What are the Pros of Point set surfaces?
                Easy to determine if inside/outside the surface
                Easy to generate points on the surface
                Easy to determine if on the surface
                Can use real world data - robust to noise
                Can render in real time
            What are the cons of point set surfaces?→Not efficient to use in some modelling tasks
    Lecture 2 Discrete Differential Geometry 
        Manifolds
            What is a manifold?
                A manifold is a topological space that is locally Euclidean (around every point there's a neighbourhood that's topologically the same as a closed ball in R^n) 
            What is a closed 2-manifold?↔A surface is a closed 2-manifold if it is locally homeomorphic to a plane everywhere
            %LOCAL_FILE%dKte5l_PI2Zm2NUj4i-Z275X5hPPeV15rfocqZmS4pNznAKURVu-dJQ2rAU7xMNx2qlXRw_CLYmWsIrF8KmhCQPr1XGGEMbbHj9Pmt7-7trIAjl1wahOzEvawX__l_4u.png 
            What does homeomorphic mean?↔One surface being homeomorphic to another surface, means it can be deformed (without piercing or pinching the surface) to become that second surface.
            Math definition: 
                %LOCAL_FILE%51gnM6JUfXL6wRogOH_bI-CTnPszmLiWWgdsmrRPopAai2XtfQNxsRA7iOgBdkiCa-0VrIV5i9ShHb5oXojk7G2DE0WZQmX3xXW0L1TUE7ZjZvsWmpBS-RkFGHi9YF2s.png 
                I.e. we create a volume sphere and get the intersection of the sphere and the surface and we can map it to a circle.
            Manifolds with Boundaries:
                %LOCAL_FILE%f2sNy5DHFQ7BmN09_acYPmqzWsLvEGCdqgS9iV3wZNBBAQQUVMT4ME-vUoYFStvFbVnmHlfLXVX6HIAdol28njMdslDyhepZIAIxn2geKpKkvIp2aITuRpK37fU_s1Nu.png 
                Each boundary point is homeomorphic to a half disk
            We treat local areas on the shape as mappings to a 2d plane, we can then compute differentials. We get a continuous 1-1 mapping:
                %LOCAL_FILE%3Tz91ygPntWjdga7fwAFjx81uL3qA3c1t13TsPymjw-mfTnaWO3xYQ4xO0KXuR5NhuNVGSVfS2d8GLjz0NrFSMI2j2ZGapybMTT-pwzw5oMVF-cj-Y8ijaJNyf6q0w6p.png 
            
        Local Coordinates
            After mapping from an area of the curve to a 2d plane, we can define local coordinate systems and thus a surface normal.
            p(u,v) = \begin{bmatrix} x(u,v) \\ y(u,v) \\ z(u,v)  \end{bmatrix}
            p_u = \frac{\partial p(u,v)}{\partial u} 
            p_v = \frac{\partial p(u,v)}{\partial v}
            Having created this orthonormal basis, which is a linear approximation to the surface - we can then find the surface normal n = {\hat{p_u}\times \hat{p_v}} where  \hat{p_u} = \frac{p_u}{||p_u||} 
            %LOCAL_FILE%JaLVopTxOR2dzfUFnUOAOtId0SwnHmuJmT-7lhnt8yN2DJAjWt_wtUdElqImTuZj0gOI5yp8Cia9yXs5ZRUbQCU0TXOOfD6FsEH0aISXFn9pb41wauxQFsdRp4r4L-Ms.png 
            We can then define a vector t rotated within the local coordinate system \hat{t} = \cos(\phi) \space \hat{p_u} + \sin(\phi) \space \hat{p_v}
            Finally we can use the plane defined by t and n which will cut the surface:%LOCAL_FILE%-8Brlr1ugBaqXwEtdburKdWwts5IXK0KGgKbn3JTXIkCuBsuALdoLB-HJtlPx9gVII5CYpRLMP2RKb_8N5sgKdCvDA71542HKTaTYdAjrYe-BZSKmbS3JbhLBINtZCEZ.png 
            The curve \gamma is the intersection between the curve and the surface, %LOCAL_FILE%B-hT4tHdYddzp1HPoRVNY0Sk0zJvW7pxz1QU3aHbpGz0ZUxT8cBJXYttuWf-vVuY4E69oB21phun1HRg6SuThqqaMbL9iA_q3Ol15hFj7gFww_hI7bKujdxHLz8HqBTi.png 
            What is this curvature we use for minimum and maximum curvature calculations?
                The curvature is the change in the tangent lines of the function as we move along it: ||\frac{dT}{ds}||
                Which is the second derivative.
                %LOCAL_FILE%8EL3m2BKkI4-UtrtZGo2GDfHjMyNC-d7qJMKBCBS5_yz-HVrEPAsVnFXHwhlUwkZHANreEdZVGid5-uIEwaBcvFnqR78IsvKridxLnHBXt8pNc2tFi3ey4ZblmatvC8b.png 
            What are the principal Curvatures?
                %LOCAL_FILE%zZFXEjt9KTY2MuWviuyzubq7nOxf_dddcS-iYeMJSJIX_O7bA5kcs8V-LEruq0dxKkcO6VNAKQec0ydo5f9huOYiLjoYKW5m34irU67gDY3WITlo-vq0LMAuy_5HsUmX.png 
            What is the mean curvature?
                %LOCAL_FILE%uI9S-7wP_kjtF1oI7_N8nF77BGS9FclFmtA2voIl9QU5qvMIAj0bnLKTLLcQMTEBZ6KTAU2yGCHuesvHm6qa6uNOUAksFQfKS2Boor5cLkUwycNlhjCaD-M12JwSrOy5.png 
            What is the Gaussian Curvature?
                %LOCAL_FILE%o_Of_U-l_rzIc_3dsrGGLIYZl-jFKCSXe7CMOq7Tv-5QbSl2F7eEzU2_emGPVC541PXjfc5DbNAzonYQD8Xzo3B-CAKN1ojRsnADJiW9zCHamXlLx1wdc29bgBDg0AIk.png 
            %LOCAL_FILE%qFKiqndLatwB-2qb8juCneDmGFXkBxx3RHtzZMeLIjm6KPKlTVtSheQJ_SPB0dJNuGuwfm8RTHTxe0hdHLuFUUXsV8RV9sW4u1eIQd2h5qEntL2riDd9c-nF6QUU35Xv.png 
            What is Euler's Theorem?
                %LOCAL_FILE%znH-YE5uWUdRs5GdzTaPML-Buo2lQr7pdPjhaGS5PodRGR0Zhtw9KtKbQC9xYuiULnQqIaBjXcNzCul6Mb7anjnxG_JwSHmavEqZRW7BLhWzVcH3aLpspPerOjC8Bhkj.png 
            %LOCAL_FILE%bkQksDnoG9sHaytQuyOCfQ9BG44Nw1-It0AFWYaLUHGiwPHrILob9f5rTwc-UwZfvXeKovInsNag_oD4ZnyS_K1MKHJczu4tLotflOfABWSrRfcCmGOMYXQwTxonm-QP.png 
        Local Shape by Curvature:
            Isotropic : Same in all directions
                Facts about Spherical
                    k1, k2 > 0, k1=k2
                    %LOCAL_FILE%FXocmZ9XYMUx3J4DnvFroQRfK_jpMQESHrLrGV7r3UX11vX4nr7kWRtEX8ff9gHWRPjoCXtxNUTUVqXYmp21lu63eW7Dc5zRD80mTldzbdEExc9QJCMfmb2LGvHgnZpN.png 
                Facts about Planar
                    k1, k2 = 0
                    %LOCAL_FILE%UXK5d_EUdwEUhlAAV5jaJ0ZzN3QfBKikVGFUiAIbXKvm5fNZ9urNR3Zbe3zl3TdDF0bQ154zHoCVL9l6c_sRK-UP8eR7z3UJQEUfJxRZeQP2jyRgdL1RrD-oP_aeiBZm.png 
            Anistropic : 2 distinct principal directions
                Facts about elliptic
                    k1 > 0, k2 > 0
                    %LOCAL_FILE%01C4IoMzLHL7XbEetHy9hOFKX-09xE6GDIBQabk1hQRnznxdnX3c2xpTOrU8f4vnur9IdlUjkPzbLaUjP6tQzGjm85BfknmvwIpVlSRKqY3i2XLqiYzByNWCBsUrHat7.png 
                Facts about parabolic
                    k1 = 0, k2 > 0
                    %LOCAL_FILE%l_M41qvVCPcHecuIqpu2eSHL2K18NgRPTJqGk3pjVyNrH80jKdNtp8rbqobl4KlLhJwd9N9Y-UmQteMGKX9RjH6DDLLABaJvh2qcwwIs4aYpl5pvNx_-W4FONFZ501tP.png 
                Facts about hyperbolic
                    k1 < 0, k2 > 0
                    %LOCAL_FILE%TLMhicO-U_9gxvd_t7-n6su7GwssPb5gPXqF6D8jBSqafojBsoo2_1go-LkNay2dgbUyLhMuI_I35Df-nj9eTFMLFawOaqpofYT13UXhpzrXxClFU0BqvFpasOvVtpgu.png 
        Discrete Differential Geometry
            What two main approaches are there for approximating surface normal & curvature?
                Local surface approximation
                Global surface approximation : discrete Laplace-Beltrami
            What is the Laplace operator? Describe using its composite parts
                f : R\rightarrow R^3 \Delta f : R\rightarrow R^3 
                %LOCAL_FILE%AQHspVFkjdkY5T4ATL_BfZKdol8X_jMbzpuLIjbUuKxcf6Oo9tCuV4DF7a4coqV884PPL-ZuWo9K03tHeseRp3D4abrrdaZpPoIsHbXAg3Rv7Wap-4296ID9uncwVxHb.png 
                Grad f points towards the areas of most rapid increase, e.g. pointing towards a peak and away from a trough%LOCAL_FILE%Cp5sUp1Hglr13hoB9ygBDhUrilQ9bG29O20JunrwdmmCrY_Aj2QOWdAvWlFyabxHXB9DZggBIqabgNCrlk8_jRg9q7K22hk0MFnR6ktGcegzlnmRQIcK1cEeXGQvBT-r.png 
                divergence can be imagine to be if we dropped particles and imagine how they would behave under the vector field, positive divergence meaning they would move out of the area and negative meaning they would move into the area.
                So the Laplace operator can be thought of as a 3d differential, higher if the curve is increasing and lower if it's decreasing in an area.
            What is the equation for divergence?
                %LOCAL_FILE%LnvQlR7K5uT3b4I8jx-vuUE1HxoOwDx2SJLvpXCeQhWo1hXyngmN_KeybHpBtPNkxJRDb0Nt30GCYnKoRKPZCdrFS91NUZeLl3Ksp8VXUoGtnUu6b8MVvmy-5IFpePFF.png 
            What is the Laplace-Beltrami operator?
                The laplace beltrami is the extension of the laplace operator to manifold surfaces.
                %LOCAL_FILE%YdVLZ1lEJZ9Op8zdiQyq3hw1JdjASQ6LSmwPLXbzTQ7XRgKOp8VxaaRqD8bo7s6xU3SYZBLfszxVo4HeFqiJxhGJb-OgNE-9irDsr7gFBrfMbWqDc94rLP4IbMx__P_P.png 
            %LOCAL_FILE%sK3lbCjN0RoQpKxLtelEvmg76R0hYeT7VN_UzuYMNrmmRSO_GCT_cGk3RpnM90cX7oNC7_Nnb2EKkKubwdazvWRabafL4GTizZ8T0Pe0KVL0dER8s_8NAYNIEqwz8qpn.png 
            %LOCAL_FILE%icIUVjGxKotBfFMTmkn5KlfQAOSbeYJhF0q-L382TejUVBsRX1D9EtJbUIlSjwfQsFzZjxoSRUiBZ1vp9dAV_RXzGU8Z1TPwLfO3zq-ik-auC30qpDIEoFwqLjmtRS9F.png
            The Laplace-Beltrami for coordinate functions gives us -2 times the mean curvature times the surface normal. This is useful as we can calculate the Laplace-Beltrami discreetly and then use that to calculate the surface normal.
            Discrete Laplace-Beltrami:
                Method 1:
                    %LOCAL_FILE%mJlIrYRU2cGwqnA_93w1-6enQHprSmwvnFO-sCxo-pjtU6nmRuatH_cXcgkJle9fFdeFQvtRpcx7P7GXBAQW459Nv1-9Zqosop9eiJsGk6MYHgipTlbrbOS3AorXAEJR.png %LOCAL_FILE%JiZp0sH3O03p-BLspz-B9xdanb7G6o4ewv8lxHu_uvL_DLo0KmTu_yJZMGTJHatHeBVKoaV7wZhtUIP6pd6zO7uK4wDg4VAejEix7w8_mjeOD-H6d4C4D2PMgzo1QgLq.png 
                    The first method of calculating the Laplace-Beltrami is to calculate the average vector from a given point to all it's neighbours.
                    Resulting in:%LOCAL_FILE%IZd6ZDPxYXxMUwir1wc1ozsRmDAS0kj991y1VryOoxd9_YRZmUTc13bcNeV0eypOVHSbvTaRU_y_MGiNmlqyHnVpJS8FnTL41N20YEPVb8twbFEFrvl7yKuI5PpXNG32.png 
                Method 2:
                    %LOCAL_FILE%3JHJT7Qjw9p_4Cdb4ZCv6fIZJqNlR1Pkw96JrtIG2b6CxzFaGjYOs5jTcffMWnJbrQQiXBqbDkTH-JZfx0vY4QQmRnFHL5RKZUW2qYuXgwWa1XPV72pTqPoC9H3xmGHe.png 
                    %LOCAL_FILE%Kj_TKwMNKF7tIRV3pZWpJybJD9MNftNvHDilcnmXGJY37r1BLjukV5ABnUlyH6CCTKvIXQkYvPv9_pllWlAmiXzALGG-Km7LDaUKu9vlV0zDGxdVVBzn9W_syqU4t-CG.png 
                    First take your neighbourhood and flatten it to a plane - then find the circumcenters (or just place the point on the edge if the angle is more than \pi/2) 
                    %LOCAL_FILE%coHWS9QvP0MY3wzL4WUmItishrY7ZDgBwZG0UxMNPtCRuTyQRZAz1W_E-PYBqChf7bLXgXbag64QykG9dXk3jH303anW8DI_kBagt8xeL_UuVswtNWoCPmxEBxL7Lutj.png 
                    We then get the weights which is the area inside the triangles:
                    %LOCAL_FILE%rH9vm-MWTQv0PYxKOJIHC6wgrJTMN1HmHcmIxLAcLsSkOO9xbJpFnwhs2FOIvluPkJE_wMi7qOGSNIPt-MCWJCdQsyWtN7ekEwJJMsjpHbEbnheTe2dvx1_uim7kcSSI.png 
                    We can then compute:%LOCAL_FILE%t4VYeXmxmvcIvYdZB31dwCImYhbr3l99RF0EypNX_nlLza4n4M2PK_OtXoV34GMqokBvQhZE2dUkrD55b85kAvDCPjyDrRr_9uN-zVRnZnhoixvOUf5scICT_DMNd_6i.png 
                    %LOCAL_FILE%D1X9bGGmh6f7AaUXpVjB_zROuWvwBXu98EF92mptAPmYyAF51si5aYMAOz2okeODANFw5EbES_QZ8luji2C6A4nHSCkTtlbHAw4MpOmVnnjnz6TgpxaVH8WwakPufh0W.png 
    
Concurrent & Distributed Systems
    Supervision 2
        
            %LOCAL_FILE%Swiev7gzpJr99rrNiwQ5TgxsXFzzJnb9TDsw3XEAIPEcszEutqMEGgth5IkZjGmHMjrbrsUpMTebpMmwY6gia6b6fzs4e40gkgUjNsAa24VjL1d4CfQ3xROkgAYKh1rb.png  
                Atomicity: Not necessarily true that we do all of the transaction or none of it, as we could crash during the system and we have no specific roll-back procedure. More work needs to be done to ensure rollback should we crash - could copy shadows of the data.
                Consistency: Not necessarily, one could still for example remove money from a system - invariants not necessarily preserved.
                Isolation: More work is needed, as we must implement strict 2PL to guarantee this. This falls out with strict 2PL, as our work on the objects is not affected by any other threads as only we can access those objects because we hold our locks.
                Durability: The system can still crash while running commands, we now have the additional task of having to release locks after a crash. 
            b.) %LOCAL_FILE%3www75KrC9L4l9rLhFcV3IN-01D-iZpqMqQg3uMdruRyWkCnPbJNPuERRhQogTsi0qvReyD3RoKQC_EPUjkAp8XfhCp67_aBwKbHr5S6_w5A3z7S0n1FF8nscRIkJgRr.png 
                Serial possible results: 
	T1: A = A + A * interest
	T2: A = A * (1 + interest) + 100; B = B + 100;
	(A,B) = (A*(1 + interest)+100, B + 100)
	OR
	T2: A = A+100; B = B+100;
	T1: A = (A+100) * (1 + interest)
	(A,B) = ((A+100)*(1 + interest), B+100) 
                Interleavings:
1: a = A.getBalance();
2: A.credit(Interest*a);
3: A.debit(100);
4: B.credit(100);

1,2,3,4: Serial
1,3,2,4: Result for A: A = A(1+interest) + 100. Conflict Serial
1,3,4,2: Result for A: A = A(1+interest) + 100. Conflict Serial

3,4,1,2: Serial
Below will be Conflict Serial: Doesn't matter who we arrange change in B, since 1&2 dont alter B
3,1,4,2: Conflict Serial
3,1,2,4: Conflict Serial
                %LOCAL_FILE%tzUTAneZ0s9AsmnNR00hdVj71jlOLoBLAjVEsRjWBDQPY5L5USmZY5KjscqZIsDiqTD8jfVjRTYrvmwV1b8RUCJhAUkQTtrvbwM3XZjAplhqnONhEFVHbui1ZWRKevyf.png
                    Because with OCC works on local copies of the data, and only does its commit right at the end when all computations have completed. Thus, if a computation does not complete it is not committed. However, with 2PL commands that augment the dataset can occur before we commit - thus crashes can occur after augmenting commands have run. We would need to implement some form of a roll-back mechanism.  
                %LOCAL_FILE%ZxmkFIv7FVtFhWDGJNEmNntWkAPkBx0ji8GPHQCqzvHfPji44haQVbAwjVhOwWcShYuKjSG_iE6O-BQG920FrNyxo199WYCRcGgZHxwgH-GGmlqjXWM_oMQorNyEzdjV.png
                    T1: 
	lock(A)
	a = A.getBalance()
	A.credit(a*INTEREST)
	unlock(A)
T2:
	lock(A)
	A.debit(100)
	lock(B)
	unlock(A)
	B.credit(100) // If T2 aborts here, T1 could have acquired the lock of A and done some work, T1 would then have to abort in sympathy with T2.
	unlock(B) Strict 2PL would have avoided this problem because no work that could have caused an abort would occur while we're unlocking A & B - as we do our unlocks at the very end of the program. 
                %LOCAL_FILE%zeXb8Y8jfZNbfPh9thhE-nn5c8NptqY5l3TRNLFjBN-_NmXcNv-ZlkWqE89Y2A32wrZ-0fW1nu3tr3ILAENnmP1Ce3ZIQfr_2lTbjlgcvWlwCc5lHaYQqKbnIpnVbWLP.png  
                    If the program crashes after having written some object updates to disk, as then the transaction hasn't completed but has still changed the state of the data. We would need a roll-back mechanism.  
        
            %LOCAL_FILE%8Gmm-oC6vjMBJtvL4PA9fP7zCsvZyUlL-qzd3B5wtClDSZDbUl5q09Pws4pc_mrRL3mN7NYIIt4d7PPbQQDrCecLxizAZhJgkym_3PnZWsJ3OqZv6xe6wz3Xm0-eaq9G.png
                T1.L1 // T1 now holds the lock for A
T2.L1 // T2 now holds the lock for B
T3.L1 // T3 now holds the lock for C
// A deadlock is now in effect, no further work can be done
            %LOCAL_FILE%A2Sp068a8HqNXoBYDmO99qWukv3ScfW2dHTrDsZ-xOUwZupjRty6_bn1htMfxrytwMjq1Bd2bc-rqRxZ3bZLMwdO5UgpqBvhTH5LqtcHKAdLzNZfx0_TrvbYcO0pBqox.png
                T1.L1 // T1 now holds the lock for A
T2.L1 // T2 now holds the lock for B
T3.L1 // T3 now holds the lock for C
A Deadlock will be detected at T1 all processes will abort.
T1.L1 // T1 now holds the lock for A
T2.L1 // T2 now holds the lock for B
T3.L1 // T3 now holds the lock for C

This process of all processes aborting and restarting will continue.
            %LOCAL_FILE%FE29KtXxq3a2RPGt-6SQkq_ZmPfUGq4obesyhGD56B1hBGbiGG4E2FMBGdDaxaencwVoGQSKI0QUGaUewM1vpjHwiFFLuMjJKYsDaiHF1uXhuXkMGjBmefVzFzv66yXG.png 
            When deadlock is detected, we abort the next thread we operate on and continue. 
            T1.L1 // T1 now holds the lock for A
T2.L1 // T2 now holds the lock for B
T3.L1 // T3 now holds the lock for C
A Deadlock will be detected at T1 and T1 will abort
// T2.L2 skipped, no lock for C
T2.L2 // T3 will acquire the lock for A, then release locks for C & A
T1.L1 // T1 acquires the lock for A
T2.L2 // T2 acquires the lock for C, then releases locks for B & C
T1.L2 // T1 acquires the lock for B and then releases locks for B & A   
        
            %LOCAL_FILE%vN_VbUS8amAxLCs-AOhqqxAur_W_tYyXlN2WybuBBo5j7uiEavHAl7gChERgXUax844uyMNik7Ls2WjWzgR_JehImsbRLJySqF3P5gUnieydjoYVyYrZJSH95wDSm42u.png
                T1:
	a = A.gb()
	b = B.gb()
	return a+b
T2:	
	A.debit(100)
	B.credit(100)

Interleaving:
A.debit(100)
a = A.gb()
B.credit(100)
b = B.gb()
return a+b

T2: timestamp 1. T1: timestamp 2.
 
A.debit(100)
a = A.gb() // Abort here, as V(T2) < W(A) but final result would have been correct
B.credit(100)
T1 -> Gets new timestamp 3
a = A.gb()
b = B.gb()
return a+b  
                %LOCAL_FILE%CPdhUdYYYwwg7iW6ACw3pMXPZmGXASdwc5aRJ4MdLmnuDbdz7WN9ACNex7r5uCM1tbFiRDOHXE4JoVgyk-DNkIAi0G-RnVexfzrWOclPFaUgPI8VyB1G8HHobO6nxjed.png 
                TSO can result in situations akin to an even worse spinlock - This occurs when a thread needs a resource a later thread has accessed before it, and thus the thread has to abort, like a spinlock the thread will be restarted but will be spinning over it's entire code contents rather than a single while loop - that is all computation before the abort needs to be recomputed (To to ACID requirements). So, if the mentioned resource is never freed the threads could continually 'spin' over their whole codebase - continually computing the same results and then throwing them away. 
                Also, for a multiple resource case - a live lock could occur when multiple threads are trying to hold a lock on all the resources:
                %LOCAL_FILE%gJrEJWDYlmX2Q9E3k9-OCYAzelh1401QHiC5_6LVvhEBgXMmKidk9IVDW15tVHKE1PDebbVxV85pL9ojHgKOXuy31beFjQWPscTcmuqqTOPlGdfMEqvQFX_NTn6yAgno.png 
                Case where T1 gets lock of R1, T2 gets lock of R2 - T1 aborts meanwhile T3 gets the lock of R1... This pattern would continue.
            %LOCAL_FILE%QEfpCL6My-m7761W-3ZbSyaqMfX5WKrbxSg48JYQR1NDZXX__BAGkf5jkASIGBZG2nS7G7XjCNXMmeRvWUhep_U31MN9Ox7KQZ_9L6paneWAQ4_U67n1fdftT672yiRQ.png
            T1: 10
	A.debit(100)
	C.debit(100)
T2: 11
	A.debit(100)
	B.debit(100)
	

T1: A.getBalance() (0,0) 

T2: A.debit(100) ()

T1: B.getBalance()

T2: B.debit(100)

// As OCC distinguishes between write VS reads, while by default TSO doesn't 
            d.)  OCC executes transactions against local copies of data (‘shadows’) rather than globally visible 3 original copies, which avoids the need to explicitly handle cascading aborts. However, copying all objects at the start of the transaction is problematic if the set of objects to be operated on is determined as part of the transaction itself. Why does on-demand copying of objects complicate transaction validation?  
            Because using our current system, the previous state that decided on demand the object should be loaded - could have been referring to an earlier version of the item that has since been changed (by another thread). And since we would be copying it on demand (and thus getting it's timestamp when we load it) we would have no way (within this system) of determining if this was the version the state referred to. So the transaction would be valid by our current system, even if the object had been changed to a different (unwanted) object just before we actually loaded it.
            %LOCAL_FILE%IHsATz-UZGEawcPlhHwr-fQBPo2GeTACbAb6eaeIyZlNZb2M1CPnhclXl_dtuQsY9L01l4AkOEF4_QID_hg0tOnjvjGX8jTB2cjuGwT0SbyXybhL1XMkkyK_ejpfoi_r.png
                Because TSO aborts simply when transactions don't follow it's one prescribed schedule, while OCC seeks only to abort conflicting commits. So as TSO aborts more, more live-lock situations will occur when perfectly correct schedules are disallowed.
            %LOCAL_FILE%gslF3mNNmDHVxaPjwqK3JmnsluwJTXFPvI39tdrA2MQg_Sp8cZXQCJVJw3CUryCqRKZ5IWDZml6eoKi6e5KVvT1q-qCptOaRxVa6gA7O_j7To72jCGQVF-Oy2sN6aBYE.png
                If a given transaction does a lot of computation in it's body, it may be slower than other threads who use the same resources - in such a case, the thread will continually find that once it's finished computation a faster thread has modified the used resources, forcing us to restart our slow computation. Consider a case where we're performing a statistical analysis of some database, this is a slow operation which will require shadows of multiple different fields - meanwhile one or many fast threads could change the values used in the statistical analysis, meaning once our slow thread has concluded it discovered it's used stale values and must restart. If a database is continually being added to, the slow thread could be starved forever.
        %LOCAL_FILE%tt68NLsWYZBlsWdYWq-ZfTPlVMtMZMMxIeiiadDE8gkDHP1rrUj30jF_zmobxPhdwmMZ9LYWpyJMFLTYJgpBUFZwBW8QFW9gfS3OShawlETDGMD_se7_T7r6TeG6hnW-.png 
            %LOCAL_FILE%Fh51IxHMCTYmMkjSu0vg7Zr9psKsZquDCBnOGirStWM76-uwrBGw3Moa17c4hOgwhMUMZOMA0CCOevAty9w_g7YMxf9LjNugU7xThtFkM3Vf8LCSTmKMMXnSbAjFqIcT.png 
                Edges represent a "is-before" relationship between nodes.
            %LOCAL_FILE%h-UuK42dzShBvMy6BZKKVy9nxj85tKJTgxX6fY000655jnaru8NGXJEZ94y2Oc_HSs75GiIB41jfWQcIrJ8M1ViUwOgwAsPRq-8zxfYAbFm13xzd7ivT_XLIPrjuX9B3.png 
                A Cycle will be present. I.e. both threads have operations that occur before operations in the other thread - a temporal impossibility.
            %LOCAL_FILE%k9dHfdy-m5Bm1Eh4nEabAx6LZQo4HfDy5d7XcTZc9ZLOc87gNUEhqyyianbsfSc-0y-QXgxdDKkJ2j7dYa0nlJaeQScECu4ifV3BcNMKri-JIlFhwagPLhzuvDerZvxQ.png 
                Isolation & Atomicity
            %LOCAL_FILE%lsFX9NuhzszQ6riLY5SF10nNuNddizzb2_KRElUbL73415zKh3CQen490K6g0gEZ4CQiU_un_kVQ4SUV86nbwqyTn4fm5gdJLjSqnvUsIpnjC6DVt5nohO5ezn8spbCN.png 
                Serial execution is when threads complete their whole execution before another thread can begin and complete it's whole execution. Serializable is when commands from threads are interleaved, but the result is the same as the serial execution. Serializable executions are a superset of serial executions, as serial executions clearly produce the same results as themselves. 
            %LOCAL_FILE%uXFO19F5I8DmY5Z7OaAZvp1OCZxkVIdYFBPvVU3CH7wsdY0yXABb27KDwQhcXAxFWV2jDo0GdQCURa77rgG7UBmwHX_dZ0c8pnZJL5izrJX03bYhPyPSZjJKqTDaqw3v.png
                A dirty read is when a thread reads a value that has been modified by another thread, hear we could read the value of A after it's been debited (a dirty read) and read b before it's credited resulting in a result 'v' less than it should be.
            %LOCAL_FILE%qvlb9mw6AyME0cDsNXhKYb5hPJHHP8jZCC2YSiNwg5GH8MTA4oIHZkKCLgs7iQ96f0__8rfLHJIt3-RbJyDVpn7OgwFWnWGaM2Q2v11sn02SX-n2KRZcGzsCVF4CdFPm.png 
                %LOCAL_FILE%Ah4OrMb9r7ElHo9fqdvI3fits0ZlHtYDXNv5C0LxsOnCMi1PEYfKS1j7Gxrpqo-ls1s3qoHqq-PmZrtfhIMZ6mGrOGESeSMf_yJsOOJKTdqEBziY9QtbWTdsHpa_Dadc.png 
            %LOCAL_FILE%Dq_JxVQqJdUaj6lqnlApNCeWstzIj_u0XzVkiOIVwyfUlxb8htFtTObOn340pTU-tVbtaVwprsSKa6GnLTKzquscQgSw3MDMfWTdDLQY5ql5am_9kJNZvcZ8XAfxDS6D.png
            %LOCAL_FILE%7rCJGSabjZmRl1LewnIgITcgke_b5wYjbn4wOGZI5HDIY7q-QNd6xKWmFnHA_yo6l9c3MpUV-stiEmAP05LuKZyyNJv6K0tPCmly-W2RnHqnq5noQABzyfvk_lr2qOia.png
                Not necessarily, after roll back the transactions could travel along the same execution path and cause the same commit error - resulting in livelock. However, there will be some cases which are solved by this addition, at the cost of large overhead of generating and testing this graph.
            %LOCAL_FILE%4KdOPCU5sQRPvkY0iFyRQMmkUlkcPEKxeLPdF1ldpMQ8VgM8VbJyiB3mSW_EUC4-b7hFhKs-EIeE1qpsdhuUYOW6H_Vvp-NE_8HZQhN_QyEdnDO-WFfeJGNuoCI_m9WJ.png
                It accepts more, because it will never reject a good schedule. While TSO accepts a single correct schedule as transactions come in.
            %LOCAL_FILE%43Q_Ib1cOqV0P8xcRYdZMwJmJt1eP3PSh7ie0ZEadmfnt0Vw7ljIT6_I1Z5gf-uOtiOQ2C6u5Q4fk-Gg1ZeKTLjRxm6Yt4v3JnFy0jE7cqgbrX_V-5frhNiPZP7zgCs3.png
                This method accepts more schedules, so could result in less cascading aborts resulting in less wasted time. However, this scheme has a huge performance overhead of generating a history graph and checking it for cycles at every step.
        
        
    Supervision 1
        %LOCAL_FILE%sd2R5eXh_2n9qz1rlMJFYdNLd0tG-bE0UaLbKmCsWpDMrlH-pHYl-ZYXdacYEQIOuqaugFW1YRGzMEU9GQ6JOBONxdF-ugR7vRcWjB1AKZSipTorsRhQrosKThDmkULR.png 
            0 - Condition synchronisation where one thread is producing items and another thread is consuming them. Initialising this to zero signifies that there are no items to consume and thus the producer must run. The consumer will wait for items to be produced (S > 0) and will then be signalled (whenever the producer produces an item it will call signal()).
            1 - If we have 1 resource available for use between multiple threads, in this case the first thread that arrives get's access to the resource - i.e. only one person can print from a printer at a time.
            n - If we have n resources available between multiple threads (more than n threads), in this case the first n threads get access to the resource immediately - i.e. only n people can use the n printers at any one time.
        chopsticks = new Semaphore(2);
int chopstick_1, chopstick_2 = 0, 0;
// Thread 1
while (1) {
	if (chopstick_1 == 2) {
		eat();
		chopstick_1=0;
		signal(chopsticks);
		signal(chopsticks);
	} else {
		wait(chopsticks);
		chopstick_1 ++;
	}
}

// Thread 2
while (1) {
	if (chopstick_2 == 2) {
		eat();
		chopstick_2=0;
		signal(chopsticks);
		signal(chopsticks);
	} else {
		wait(chopsticks);
		chopstick_2 ++;
	}
}

// This can result in both threads getting one chopstick and starving
        %LOCAL_FILE%hB68m6SGg36hFL_xPZrweB2Eqg7k_MVpuvzdel9hIGD09WTMEsLGbpsWbnxmyGvH09l_OM7wDs9nDkFSYcrrMckP0b-W4XChfeRFosc3SSgvINuuRlz1TegpO9zNM9qH.png 
            Yes, because if we have a context switch anywhere between the weights (including just after the wait command is run) the various producer/consumer threads have different conceptions of the state. For example, if a context-switch occurred (in the case of a producer) before we changed buffer[in] we could resume the thread when the list is full and cause an overflow. Alternatively, we could consume the wrong item by the same means.
        %LOCAL_FILE%D19kr_2Pf7_0ZafdDz9-dTMCRWKuzsTeC3BNISA9-vMa91p5xUIbPtkAcO-FvWKG-BOYddEfZNGpxymen8EKKA9v6OFpxc89bqDSu4Y6cYZwpsad_kCWmOu6P2Fstjta.png 
            In the implementation of wait & signal presented in the lectures, the problem would be making checking the queue and changing S atomic (for signal) or checking & changing S atomic (for wait) rather than the question posed above. This is because in the version presented in the lecture, only the first and last threads in the door change S.
            Wait would be unaffected - this is because we only perform a decrement if S > 0 - and we only block a thread when S = 0. This means the two can never be in contention.
            If signal's integer & scheduler operation were not atomic, we could have a race condition - in between the integer operation and the scheduler operation another thread could call wait on the semaphore and perform a computation. While that thread is running, we would complete our scheduler operation and our awoken thread would perform the same computation - this race condition could lead to anomalous behaviour. Waking up the thread first, and then incrementing the value would solve this.
            
        %LOCAL_FILE%33akS39qWNR2m71NzOgaRJBQFNkWyVBtTw06nG-8WkaHVY_RldW4gw2BUKle6mpGtR5l7Eizk96IOtfF0m8hj52Aznye1SsP6aGioBc16kDSvqDO1K60vUqc9_FwdWnM.png 
            Round-Robin - In a multi-core processor where each consumer thread is on a different core, we would spread the work to different cores to ensure temperatures of an individual core doesn't skyrocket which would result in a slowdown.
            Prioritized Work - In a multi-CPU architecture where faster CPUs are in more demand for work, faster CPUs would be given a higher priority as they would complete faster and we'd like that time-save when those CPUs are available, while slower CPUs would be given a lower priority. 
            MRU - If the CPU cores have caches that will have recent data stored in them, if a thread has been used recently it may happen that the required data to perform the computation is already in the cache (addresses of items etc.) meaning time will be saved which would have been spent fetching data from main memory.
        %LOCAL_FILE%RdrlWILK0E0bNwSv0LdF4PR9LSwMfofFUuoeUnN2zZ51HMYhG-COY7mcbZQY7g0NXtGGAgopnKwofrjQhkZ-RW5b53cxNVacz10Dj-fph_bN_LdPWE_SrBDgb1eLYKHN.png 
            int buffer[N]; int in = 0, out = 0;
spaces = new Semaphore(N);
items = new Semaphore(0);
producer_guard = new Semaphore(1);
consumer_guard = new Semaphore(1);
// Producer threads
while(true) {
	item = produce();
	wait(spaces);
	wait(producer_guard);
	buffer[in] = item;
	in = (in + 1) % N;
	signal(producer_guard);
	signal(items);
}
// Consumer threads
while(true) {
	wait(items);
	wait(consumer_guard);
	item = buffer[out];
	out =(out+1) % N;
	signal(consumer_guard);
	signal(spaces);
	consume(item);
} 
            %LOCAL_FILE%bPGP_gNXPzszZu0_q37-gatUw0H5dw8izVNFee9drU-pzJ-qJsaKuetdA0NFulnHZLxjr7CjpzZMpGvCJCxYpVZWCmMPIlAGMK-4SQrszI27PiIeOky3BPa_hlCHoaQN.png 
            We can consider that the consumer and the producer interact with different sections of the buffer in their "guarded" sections - the consumer deals with out and the producer deals with in. We can then consider that the whole design of the spaces & items was done to ensure the back and front of the list cannot affect each other, that is we can never add to a full list due to spaces and can never detract from a full one due to items. So as in & out are safely independent, we need only protect our consumers from other consumers and producers from other producers. 
        %LOCAL_FILE%y6BGMYo6C6AtmnKq8ltCZY67Be2lsy6CI2hVWujhK59XcjEncDNTVw9qCa3N4l3fOIbd_kzM-OmGGOfOdTIkNrFe_W9laL-85ipJVFLlCulSOooNdRFDJ1ZLkqcR-Mw-.png 
            Because generally there will be multiple (low priority) consumer threads in the guard queue ahead of the producer thread, they will necessarily be popped from the queue ahead of any producer threads. However, if you were using a mutex:
            while (! test-and-set(note)); The thread that gets access to the resource, is the thread that runs test-and-set(note) first after the resource is freed - if the producer threads are set to a higher priority by the OS, they are more likely to run the command first.
            
        %LOCAL_FILE%XtO2e4DqYfpL0B1gbYvMSEYqayEMkA50280I_JG5WD2FiCBcqtGMbZTY-fLOAmdiJLDuTOEe-alLzih8lCUPd9E7G_BCc0Dv_sxyKAGKcJd8IY8mq4XdPT6UXHgvTkhm.png 
            One could replace the queue structure with a priority-queue, rather than waking up the next thread in the queue in signal we would call popmin() (where a lower priority number signifies a higher priority) and the higher priority consumers would always go first.
        %LOCAL_FILE%Wy1sGRze451GrBUFtmUiDbaytnxBlC36EbdEbjjsWFZ2CaSDbQa78plAuoKq4hHoXQ5CslliK5YT_cJkEkuecGzqnKCKst6nEduav4Pibnv5blNuHzndlqfJqDSZCyOg.png 
            One solution would be to raise the priority of the producers (to high) whenever there's a high-priority consumer waiting - the wait() commands thus acts similarly to the signal() command, in that it allows another thread to run.
        %LOCAL_FILE%cjRL7x0rri8fzqS45tGSxJ_lTt-r8ZbvSkwSlM-jlu96XC0v3j76VAMtqfibLO0JJQMDupkzyO4in-M9lDVC3LpA_f5UZ8DUvR3fXWzaR68yH9UANntExc4So-IiYXSf.png 
            Each thread has its own stack, register file and scheduling state, thus there will be two instances of each of these.
            Threads share the executable program and virtual address space, so there will be only one instance of each of these.
        %LOCAL_FILE%gSeUH_wE-tW5DLaCuzxk0t7wair3fJw17i5FnYJAVLrFHH8d6hAQtJur1zWRVBjCKb5gsnuUcMNPlui8jIWfaKs-Wys6EhI_8toKLA9B3cFTitBM2p9WMPdN--zIbDxD.png 
            Consider the following case: 
            if (!beer) {
	// printf(note);
	if (!note) {
		note = 1;
		get_beer();
		note = 0;
	}
} A race condition can occur between checking for beer and checking the note - if another thread is just about to write to the note. However, as printf() takes a non-zero amount of time to execute that added time could result in the second thread changing note  in time, meaning the first thread will check the note once it's been changed (e.g. desired logic flow occurs).
        %LOCAL_FILE%r9xDNEUd21vdFWgQV_oBu_b-yT79qFcKluEGCeipA0baCbUXBf4g13mns3lEDnAmLdZwxbOyLpKM8587Ae9iw3K87JXbLdhpE_C6FkpF5JD65RNQAfDJqyiWzg03192u.png 
            n! As the OS is (modelled as) non-deterministic, we cannot know the order in which threads will be scheduled and execute thrprint.
        %LOCAL_FILE%rOmx9KUE6ei8U0fUQOOJT_dEeaW033n8VTJbWBYfwSH8oESnVDHu_5OpJl-KV63e0zBY8VNTCTXmF_RLMl1F0t5ujdZmkq99WLSKkC8Lk3_AKDNASEYiuUNcygfFIsWc.png 
            1 - As pthread uses signal-and-continue style conditions, the if should be a while loop - as the value of the condition could have changed while the thread was waiting on the condidtion.
            2 - We do our printf after unlocking the mutex, this defeats the whole purpose of the safety - another thread could legally access the state and complete it's printf before we've completed ours (this could be due to a context switch or simply another core that's faster).
            3 - We never signal ordering_cv to wake up other threads.
            int next_thread_id = 0; // Next ID to print
pthread_mtx_t ordering_mtx; // Lock protecting next ID
pthread_cond_t ordering_cv; // next_thread_id has changed

void ordered_thrprint(int thread_id, char *message) {
	pthread_mtx_lock(ordering_mtx);
	while(thread_id != next_thread_id) {
		pthread_cond_wait(ordering_cv, ordering_mtx);
	}
	next_thread_id = next_thread_id + 1;
	printf("Thread %d: %s\n", thread_id, message);
	pthread_mtx_unlock(ordering_mtx);
	pthread_cond_broadcast(ordering_cv);
} 
        %LOCAL_FILE%gcADy-5emy1KJOIoJd595ND9GnD5-ptl-P6YS97lwBJayyx_m-ziHUyQeKJsIrMoEv3TZJd2euOaVtZE15SHtQdEbABINv6eTJNsILmfI65csyku1bmNsxdHQBmVlNKl.png 
            We could solve this problem using two additional pieces of state (and associated mutexes), an array of size N (number of threads) and a protected integer that starts as N. Whenever a thread is executed, it would do the following
                First get control of the buffer and the protected integer
                Then it would place it's debug message into the buffer at the index of it's thread ID.  
                Then decrement the protected integer, if that integers value is now 0 print out all the debug messages and associated indexes and we're done.
                Release the protected integer and the buffer.
    Lecture 1
    
    



